


<HTML>
<HEAD>

<TITLE>September 1992/The Foundation of Neural Networks: The Adaline and Madaline</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocsep.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Features</FONT></H2>

<hr><h2 align="center"><font color="#800000">The Foundation of Neural Networks: The Adaline and Madaline<A name="01B9_00DF"><A name="01B9_00DF"></font></h2><P>
<h3 align="center"><font color="#800000"><A name="01B9_0000"><A name="01B9_0000">Dwayne Phillips</font></h3><hr><blockquote><P>
<P><i><A name="01B9_0000"><A name="01B9_0000">Dwayne Phillips works as a computer and electronics engineer with the United States Department of Defense. He has a Ph.D. in electrical and computer engineering at Louisiana State University. His interests include computer vision, artificial intelligence, software engineering, and programming languages.</i></P><P>
Neural networks are one of the most capable and least understood technologies today. The theory of neural networks is a bit esoteric; the implications sound like science fiction but the implementation is beginner's C.<P>
There are many problems that traditional computer programs have difficulty solving, but people routinely answer. Examples include predicting the weather or the stock market, interpreting images, and reading handwritten characters.<P>
Since the brain performs these tasks easily, researchers attempt to build computing systems using the same architecture. The result, shown in <A href="fig1.htm">Figure 1</a>,
 is a neural network. A neural network is a computing system containing many small, simple processors connected together and operating in parallel. The structure of the neural network resembles the human brain, so neural networks can perform many human-like tasks but are neither magical nor difficult to implement.<P>
<h4><FONT COLOR="#000080"><A name="01B9_00E0">How a Neural Network Learns<A name="01B9_00E0"></FONT></h4></P>
An important characteristic of a neural network is that it can "learn." Once you have a neural network program (such as those given later) you "train" it. You do not write a new program for every new problem. You use the same neural network program and train it to solve new problems.<P>
The basic building block of all neural networks is the <I>adaptive linear combiner</I> shown in <A href="fig2.htm">Figure 2</a>
and described by <A href="phillip1.htm#equat1">Equation 1</a>.
 The adaptive linear combiner combines inputs (the <I>x</I>'s) in a linear operation and adapts its weights (the <I>w</I>'s). Put another way, it "learns." The adaptive linear combiner is not a neural network &#151; it is only a building block.<P>
<b><a name="equat1">Equation 1</a></b><br>
<IMG SRC="equat1.gif"><P>
The adaptive linear combiner multiplies each input by each weight and adds up the results to reach the output. The software implementation uses a single <I>for</I> loop, as shown in <A href="list1.htm">Listing 1</a>.
<P>
If the output is wrong, you change the weights until it is correct. The neural network "learns" through this changing of weights, or "training." The summation in the lower right of <A href="fig2.htm">Figure 2</a>
shows a path where errors can come back and change the weights to produce a correct answer.<P>
I used the adaptive linear combiner to make a simple neural network &#151; the adaptive linear element or Adaline (Widrow and Lehr 1990) shown in <A href="fig3.htm">Figure 3</a>.
 The Adaline is a linear classifier. It can separate data with a single, straight line. <A href="fig4.htm">Figure 4</a>
gives an example of this type of data. Suppose you measure the height and weight of two groups of professional athletes, such as linemen in football and jockeys in horse racing, then plot them. As expected, the linemen plot out in one area of the graph (the -1 group) and the jockeys in another (the +1 group). You can draw a single straight line separating the two groups.<P>
You can feed these data points into an Adaline and it will learn how to separate them. Then you can give the Adaline new data points and it will tell us whether the points describe a lineman or a jockey. The inputs to the Adaline (the <I>x</I>'s) are the sets of points (height and weight) and the output from the Adaline is +1 (jockey) or --1 (lineman). Each input (height and weight) is an input vector. The input vector is a C array that in this case has three elements: one for height, one for weight, and one extra (all input vectors have this extra element).<P>
The Adaline contains two new items. These are the threshold device and the LMS algorithm, or learning law. The threshold device takes the sum of the products of inputs and weights and hard limits this sum using the <I>signum</I> function. If the sum is less than 1, the output is -1, else the output is +1. <A href="list2.htm">Listing 2</a>
shows a subroutine which implements the threshold device <I>signum</I> function.<P>
The second new item is the <FONT FACE="Symbol">a</FONT>-LMS (least mean square) algorithm, or learning law. This describes how to change the values of the weights until they produce correct answers. In <FONT FACE="Symbol">a</FONT>-LMS, the Adaline takes inputs, multiplies them by weights, and sums these products to yield a <I>net</I>. The binary output is +1 for <I>net</I> &gt;=0 and -1 for <I>net</I> &lt;0.<P>
If the binary output does not match the desired output, the weights must adapt. Each weight will change by a factor of <FONT FACE="Symbol">D</FONT><I>w</I> (<a href="phillip1.htm#equat3">Equation 3</a>)
. The <FONT FACE="Symbol">h</FONT> is a constant which controls the stability and speed of adapting and should be between 0.1 and 1.0. <a href="phillip1.htm#equat4">Equation 4</a>
shows the next step where the <FONT FACE="Symbol">D</FONT><I>w</I>'s change the <I>w</I>'s. <A href="list3.htm">Listing 3</a>
shows a subroutine which performs both <a href="phillip1.htm#equat3">Equation 3</a>
and <a href="phillip1.htm#equat4">Equation 4</a>.
 Notice how simple C code implements the human-like learning.<P>
Where do you get the weights? Originally, the weights can be any numbers because you will adapt them to produce correct answers. The weights make up the weight vector &#151; another C array with the same number of elements as the input vector array.<P>
The learning process consists of feeding inputs into the Adaline and computing the output using <A href="list1.htm">Listing 1</a>
and <A href="list2.htm">Listing 2</a>.
 If the output is incorrect, adapt the weights using <A href="list3.htm">Listing 3</a>
and go back to the beginning. <A href="fig5.htm">Figure 5</a>
shows this idea using pseudocode.<P>
<b><a name="equat2">Equation 2</a></b><br>
<IMG SRC="equat2.gif"><P>
<b><a name="equat3">Equation 3</a></b><br>
<pre><FONT FACE="Symbol">D</FONT>w<SUB>i</SUB> = <FONT FACE="Symbol">h</FONT>x<SUB>i</SUB> (target -net)
          for <I>i</I> = <I>o, n</I></pre>
<b><a name="equat4">Equation 4</a></b><br>
<pre><I>w</I><I><SUB>i</SUB></I> = <I>w</I><I><SUB>i</SUB></I> +<FONT FACE="Symbol">D</FONT><I>w</I><I><SUB>i</SUB></I> for <I>i = o, n</I></pre>
You can use the Adaline to make another neural network &#151; the multiple adaptive linear element or Madaline (Widrow and Lehr 1990) shown in <A href="fig6.htm">Figure 6</a>.
 The Madaline can solve problems where the data are not linearly separable such as shown in <A href="fig7.htm">Figure 7</a>.
 This is a graph of the heights and weights of professional football (+1) and basketball (--1) players. This is not as easy as linemen and jockeys, and the separating line is not straight (linear). Nevertheless, the Madaline will "learn" this crooked line when given the data.<P>
The Madaline in <A href="fig6.htm">Figure 6</a>
is a two-layer neural network. The first layer contains hard limiting (+1 or --1) Adalines and the second layer is a single, fixed-logic element. Each Adaline in the first layer uses <A href="list1.htm">Listing 1</a>
and <A href="list2.htm">Listing 2</a>
to produce a binary output. The binary output passes on to a final decision maker that makes either an <I>AND, OR</I>, or <I>MAJORITY</I> decision. <A href="list4.htm">Listing 4</a>
shows how to perform these three types of decisions.<P>
The Madaline shown uses the <I>Madaline 1</I> learning law instead of the <FONT FACE="Symbol">a</FONT>-LMS learning law. (There are three different Madaline learning laws, but we'll only discuss Madaline 1.) The Madaline 1 has two steps. First, give the Madaline data, and if the output is correct do not adapt. Second, if the output is incorrect, adapt (using <A href="list3.htm">Listing 3</a>)
 the Adaline whose +1 or -1 output disagrees with the final answer and whose <I>net</I> (<A href="phillip1.htm#equat2">Equation 2</a>)
 is closest to 0.<P>
Suppose you have a Madaline with three Adalines and a <I>MAJORITY</I> decisionmaker. If the output should have been +1 and two of the Adalines produced --1's, use the <FONT FACE="Symbol">a</FONT>-LMS law and adapt the Adaline which produced a --1 and had a <I>net</I> closest to 0. <A href="fig8.htm">Figure 8</a>
shows the idea of the Madaline 1 learning law using pseudocode.<P>
Once you have the Adaline implemented, the Madaline is easy because it uses all the Adaline computations. The only new items are the final decision maker from <A href="list4.htm">Listing 4</a>
and the Madaline 1 learning law of <A href="fig8.htm">Figure 8</a>.
<P>
<h4><FONT COLOR="#000080"><A name="01B9_00E5">Using Neural Networks<A name="01B9_00E5"></FONT></h4></P>
There are three steps to using a neural network. First, input data with known correct answers. This can be tedious so you may want to write code which reads data from your favorite spreadsheet or database. Second, train the neural network using the learning law. This executes the learning-law code and requires no user interaction. Third, use the trained neural network on new data. You can enter one data point at a time or read it from a file.<P>
<h4><FONT COLOR="#000080"><A name="01B9_00E6">The Adaline Program<A name="01B9_00E6"></FONT></h4></P>
<A href="list5.htm">Listing 5</a>,
 <A href="list6.htm">Listing 6</a>,
 <A href="list7.htm">Listing 7</a>,
 and <A href="list8.htm">Listing 8</a>
are complete programs that implement an Adaline and a Madaline neural network. I wrote these programs to be flexible enough to work many different problems, command-line driven, interactive, portable among compilers and operating systems, simple, and use a minimum amount of floating-point math.<P>
<A href="list5.htm">Listing 5</a>
shows the main routine for the Adaline neural network. The routine interprets the command line and calls the necessary Adaline functions. The command line is<P>
<pre>adaline inputs-file-name
        weights-file-name
size-of-vectors mode</pre>
The mode is either input, training, or working to correspond to the three steps to using a neural network. <I>main</I> uses the <I>malloc</I> function to allocate space for the input and weight arrays. This gives you flexibility because it allows different-sized vectors for different problems. The vectors are not <I>floats</I> so most of the math is quick-integer operations.<P>
<A href="list6.htm">Listing 6</a>
shows the functions which implement the Adaline. The first three functions obtain input vectors and targets from the user and store them to disk. These functions implement the input mode of operation. The next two functions display the input and weight vectors on the screen. These are useful for testing and understanding what is happening in the program.<P>
Next in <A href="list6.htm">Listing 6</a>
is <I>train_the_adaline</I>. This performs the training mode of operation and is the full implementation of the pseudocode in <A href="fig5.htm">Figure 5</a>.
 <I>train_the_adaline</I> calls the next four functions of <A href="list6.htm">Listing 6</a>
as it loops through the input vectors and tests for correct answers. If the answers are incorrect, it adapts the weights.<P>
The next functions in <A href="list6.htm">Listing 6</a>
resemble <A href="list1.htm">Listing 1</a>,
 <A href="list2.htm">Listing 2</a>,
 and <A href="list3.htm">Listing 3</a>.
 These calculate Adaline outputs and adapt the weight vector. There is nothing difficult in this code. The final function in <A href="list6.htm">Listing 6</a>
is <I>process_new_case</I>. This is the working mode for the Adaline. You call this when you want to process a new input vector which does not have a known answer. <I>process_new_case</I> uses the other functions in <A href="list6.htm">Listing 6</a>
to obtain the new input vector, calculate the answer, and display it.<P>
Believe it or not, this code is the mystical, human-like, neural network. It can "learn" when given data with known answers and then classify new patterns of data with uncanny ability.<P>
<h4><FONT COLOR="#000080"><A name="01B9_00E7">Adaline Example<A name="01B9_00E7"></FONT></h4></P>
<A href="fig4.htm">Figure 4</a>
showed a simple classification problem involving football linemen and horse-racing jockeys. <A href="tab1.htm">Table 1</a>
lists the height, weight, and classifications of the data. The first step is to enter the data. The command is<P>
<pre>adaline adi adw 3 i</pre>
The file names <I>adi</I> and <I>adw</I> can be anything you want. The program prompts you for data and you enter the 10 input vectors and their target answers. I entered the heights in inches and the weights in pounds divided by 10. This made the weights the same magnitudes as the heights. If your inputs are not the same magnitude, then your weights can go haywire during training.<P>
The next step is training. The command is<P>
<pre>adaline adi adw 3 t</pre>
The program loops through training and prints the results to the screen. For this case, the weight vector was (--21 --1840 --664). If you use these numbers and work through the equations and the data in <A href="tab1.htm">Table 1</a>,
 you will have the correct answer for each case. The final step is working with new data. The command is:<P>
<pre>adaline adi adw 3 w</pre>
The program prompts you for a new input vector and returns the class (+1 or --1) it calculates. If you enter a height and weight similar to those given in <A href="tab1.htm">Table 1</a>,
 the program should give a correct answer. However, with only 10 input vectors for training, it is likely that a height and weight entered will produce an incorrect answer. The more input vectors you use for training, the better trained the network. Ten input vectors is not enough for good training. Use more data for better results.<P>
<h4><FONT COLOR="#000080"><A name="01B9_00E8">The Madaline Program<A name="01B9_00E8"></FONT></h4></P>
<A href="list7.htm">Listing 7</a>
shows the <I>main</I> routine of the Madeline program. The command line is<P>
<pre>madaline inputs-file-name
        weights-file-name
size-of-vectors
        number-of-adalines
mode choice-type</pre>
The new parameters are the number of Adalines to use and the mode (<I>AND, OR</I>, or <I>MAJORITY</I>). The code resembles Adaline's <I>main</I> program. Here, the weight vector is two-dimensional because each of the multiple Adalines has its own weight vector. The remaining code matches the Adaline program as it calls a different function depending on the mode chosen.<P>
<A href="list8.htm">Listing 8</a>
shows the new functions needed for the Madaline program. The function <I>train_the_madaline</I> implements the pseudocode shown earlier in <A href="fig8.htm">Figure 8</a>.
 This function loops through the input vectors, loops through the multiple Adalines, calculates the Madaline output, and checks the output. If the output does not match the target, it trains one of the Adalines. This function is the most complex in either program, but it is only several loops which execute on conditions and call simple functions. Notice how it uses several of the Adaline functions given in <A href="list6.htm">Listing 6</a>
(<I>calculate_net, calculate_output, train_weights</I>). This reflects the flexibility of those functions and also how the Madaline uses Adalines as building blocks.<P>
The next function, <I>madaline_output</I>, resembles <A href="list4.htm">Listing 4</a>.
 It calculates the final Madaline output using either the <I>AND, OR</I>, or <I>MAJORITY</I> method. <I>which_adaline</I><I>,</I> in <A href="list8.htm">Listing 8</a>,
 chooses which Adaline to adapt. You want the Adaline that has an incorrect answer and whose <I>net</I> is closest to zero.<P>
The final function is <I>process_new_madaline</I>. This implements the working mode of the Madaline and resembles <I>process_new_case</I> in <A href="list6.htm">Listing 6</a>.
 <I>process_new_madaline</I> obtains the new input vector from the user, calculates the Adaline outputs, and calculates the Madaline output.<P>
<h4><FONT COLOR="#000080"><A name="01B9_00E9">Madaline Example<A name="01B9_00E9"></FONT></h4></P>
<A href="fig7.htm">Figure 7</a>
showed another height vs. weight graph using football and basketball players. The dividing line is crooked (non-linear). This is beyond the ability of a single Adaline. <A href="tab2.htm">Table 2</a>
shows the input vectors and their correct classifications. The first step is to input these vectors using the command line<P>
<pre>madaline bfi bfw 2 5 i m</pre>
The file names <I>bfi</I> and <I>bfw</I> are arbitrary. I chose five Adalines, which is enough for this example. You should use more Adalines for more difficult problems and greater accuracy. You will need to experiment with your problems to find the best fit. I chose the <I>MAJORITY</I> Madaline decisionmaker. Again, experiment with your own data.<P>
The program prompts you for all the input vectors and their targets. I entered the height in inches and the weight in pounds divided by ten to keep the magnitudes the same. Next is training and the command line is<P>
<pre>madaline bfi bfw 2 5 t m</pre>
The program loops through the training and produces five each of three element weight vectors.<P>
Now it is time to try new cases. The command line is<P>
<pre>madaline bfi bfw 2 5 w m</pre>
The program prompts you for a new vector and calculates an answer. This is a more difficult problem than the one from <A href="fig4.htm">Figure 4</a>.
 Therefore, it is easier to find an input vector that should work but does not, because you do not have enough training vectors. Ten or 20 more training vectors lying close to the dividing line on the graph of <A href="fig7.htm">Figure 7</a>
would be much better.<P>
<h4><FONT COLOR="#000080"><A name="01B9_00EA">Another Madaline Example<A name="01B9_00EA"></FONT></h4></P>
Recognizing a handwritten character is vastly different from the previous example, but the Madaline program will do it without changing any source code. You will train the Madaline to recognize whether or not a handwritten character is a capital <I>A</I>. <A href="fig9.htm">Figure 9</a>,
 <A href="fig10.htm">Figure 10</a>,
 and <A href="fig11.htm">Figure 11</a>
show how I created input vectors for training the Madaline. I made a 9x7 matrix, wrote an <I>A</I> by hand, and blocked off the <I>A</I> into the squares. This is one input training vector containing 63 elements (see <A href="tab3.htm">Table 3</a>)
. Each element of the vector is either a 1 or 0 and the target output is + 1 for an <I>A</I> and -1 for anything else. I sketched out six <I>A</I>'s, one <I>B</I>, one <I>E</I>, and one garbage input vector. I entered all of these long vectors by hand using the command<P>
<pre>madaline ai2 aw2 63 5 i m</pre>
I trained the Madaline using:<P>
<pre>madaline ai2 aw2 63 5 t m</pre>
I tried new cases using:<P>
<pre>madaline ai2 aw2 63 5 w m</pre>
The Madaline correctly identified several more <I>A</I>'s and non-<I>A</I>'s that I sketched and entered. This demonstrates how you could recognize handwritten characters or other symbols with a neural network . It would be nicer to have a hand scanner, scan in training characters, and read the scanned files into your neural network. That would eliminate all the hand-typing of data.<P>
These examples illustrate the types and variety of problems neural networks can solve. The heart of these programs is simple integer-array math. They execute quickly on any PC and do not require math coprocessors or high-speed 386's or 486's<P>
Do not let the simplicity of these programs mislead you. They implement powerful techniques. The splendor of these basic neural network programs is you only need to write them once. You can apply them to any problem by entering new data and training to generate new weights. Practice with the examples given here and then stretch out.<P>
<h4>References</FONT></h4></P>
Widrow, Bernard and Michael A. Lehr. September 1990. "Thirty Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation," <I>Proceedings of the IEEE.</I><P>

<h4><a href="../../../source/1992/sep92/phillip2.zip">Get Article Source Code</a></h4>

</BLOCKQUOTE>
</BODY>
</HTML>
