


<HTML>
<HEAD>

<TITLE>September 1992/Time Complexity</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocsep.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Features</FONT></H2>

<hr><h2 align="center"><font color="#800000">Time Complexity<A name="01AD_00DB"><A name="01AD_00DB"></font></h2><P>
<h3 align="center"><font color="#800000"><A name="01AD_0000"><A name="01AD_0000">Wilbon Davis</font></h3><hr><blockquote><P>
<P><i><A name="01AD_0000"><A name="01AD_0000">Wilbon Davis holds a B.S. and an M.S. in physics and mathematics from Southwest Texas State University, and is a professor in the Computer Science department of SWTSU. He also teaches a wide range of computing topics in industry. He can be reached by mail at SWTSU, 601 University Drive, San Marcos, TX 78666<I>-</I>4616, or by Internet at wd01@WTEXAS.bitnet.</i></P><P>
<h4><FONT COLOR="#000080"><A name="01AD_00DC">Introduction<A name="01AD_00DC"></FONT></h4></P>
Precise formulas for estimating run time, memory consumption, I/O traffic, and other values of interest would make choosing among algorithms simpler, but such formulas are rarely available when needed. Fortunately, there is a way to make crude but useful estimates to guide these choices. You estimate the <I>cost function</I> associated with each algorithm to see how its execution time grows with problem size.<P>
By way of example, I will present the evolution of a sorting program and the performance gains that arise from several refinements. Then I will discuss a general strategy for estimating performance differences among choices of algorithms.<P>
<h4><FONT COLOR="#000080"><A name="01AD_00DD">The Bubble Sort<A name="01AD_00DD"></FONT></h4></P>
One of my favorite instructional films, "Sorting Out Sorting" <a href="#ref4">[4]</a>, points out that bubble sort is an algorithm whose fame exceeds its virtue. However, two things have kept bubble sort from disappearing from my computing lexicon &#151; bubble sort is easily understood and provides an excellent example when discussing efficiency of algorithms.<P>
Bubble sort works simply. You repeatedly scan a list to be ordered, comparing adjacent items. When the items in a pair are out of order, swap them. When a scan results in no swaps, the list is sorted. C code for a simple version of bubble sort appears in <A href="list1.htm">Listing 1</a>.
<P>
I compiled the code in <A href="list1.htm">Listing 1</a>
without optimization and for 16-bit operation with the Zortech C compiler. The second row of <A href="tab1.htm">Table 1</a>
contains the results of timing the program as it sorted various sizes of arrays of randomly generated integers.<P>
Sorting the smaller arrays went quickly, but the array of 8,000 elements took 277.6 seconds, a bit over 4.5 minutes and a bit beyond my patience. Look at the second row of <A href="tab1.htm">Table 1</a>.
 Note that the time increased by a factor of 4.08 going from 1,000 to 2,000 items and increased by a factor of 3.99 in going from 2,000 to 4,000 items. When going from 4,000 to 8,000 items, the time increased by a factor of 277.60/67.95, or 4.09. It seems reasonable to guess that doubling the size of the array produces about a four-fold increase in run time.<P>
As a matter of fact, a good predictor of bubble sort's performance is to increase sort time by the square of the ratio of the numbers of items in the arrays. The data in the table nicely fits a quadratic, giving<P>
<pre>   <I>Bubble_time</I>(<I>n</I>) = .878 - .972<I>n</I> + 4.445<I>n</I><SUP>2</SUP></pre>
where<I> n</I> is the list size in thousands as an approximation of the sort time in seconds.<P>
This formula is an example of a cost function. That is, <I>Bubble_time</I> is a function defined on the natural numbers and is never negative. In this case, the cost of sorting was measured in seconds, but many other measures such as bytes of memory, dollars, etc., might be appropriate.<P>
If you step through the code in <A href="list1.htm">Listing 1</a>,
 you will soon see that after the first scan, the largest item in the list will be in its permanent position at the end of the list. Likewise, after the second scan, the next largest element will be in the next-to-last position, and so on.<P>
The code in <A href="list2.htm">Listing 2</a>
incorporates this idea of scans of decreasing lengths, and yields a program that is almost twice as fast as the first version. However, the results in <A href="tab1.htm">Table 1</a>
show the same quadrupling of execution time with each doubling of array size. The data movements have not changed &#151; only some unneeded comparisons are eliminated.<P>
<h4><FONT COLOR="#000080"><A name="01AD_00DE">Tuning the Bubble Sort<A name="01AD_00DE"></FONT></h4></P>
To test the effectiveness of improved compiler technology, I invoked Zortech's global optimization feature and recompiled the code in <A href="list2.htm">Listing 2</a>
to run under a 32-bit DOS extender. The resulting program is indeed faster as the fourth column of <A href="tab1.htm">Table 1</a>
indicates. The optimized version consistently runs in slightly less than half the time required by the unoptimized 16-bit version.<P>
So far, the various combinations of code and compilation strategies have yielded ratios of 1 : .49 : .20 for the run times of the three versions discussed.<P>
Other ideas come to mind. Use a faster machine, switch to assembly language, even employ a better programmer. But no matter which of these ploys is used, the formula for the time will always contain a term with a factor of <I>n</I><SUP>2</SUP>. Thus, while these measures can affect the constant factors in the formula, none will prevent a doubling of problem size from causing an approximate quadrupling of computation time.<P>
The difficulty is that without a fundamental change in the algorithm, the same items are swapped in the same sequence. Experimenting with paper and pencil shows one problem to be that items which are far out of position may require a large number of passes to finally come to rest.<P>
<h4><FONT COLOR="#000080"><A name="01AD_00DF">The Shell Sort<A name="01AD_00DF"></FONT></h4></P>
The code in <A href="list3.htm">Listing 3</a>
modifies bubble sort using Shell's idea of giving elements that are far apart special consideration. The code first rearranges the array so that items which are <I>n</I> positions apart are in the correct order. Then, items that are (<I>n</I>+1)/2 apart are considered. After each execution of the inner bubble-sort loop, the distance between items is halved. After several passes, the distance between compared elements becomes 1 and the algorithm becomes identical to the original bubble sort. However, using Shell sort, most of the large data movements already have taken place.<P>
Clearly, the code in <A href="list3.htm">Listing 3</a>
involves extra overhead. Is the overhead worthwhile? Judge for yourself by examining the results of repeating the bubble sort experiments with Shell sort in <A href="tab2.htm">Table 2</a>.
 (Using the formulas in the last row of <A href="tab1.htm">Table 1</a>,
 compute the values omitted in the table and you will understand my reluctance to fill in the blank spots experimentally.)<P>
As you can see, the speed increase is remarkable. Exchanging a pair of items which are far apart takes no longer than exchanging an adjacent pair because a single distant exchange replaces many adjacent exchanges. Thus, Shell sort has a fundamentally different response to changes in problem size than bubble sort. This change is reflected in the approximation formula for the run-time:<P>
<pre>   <I>Shell_time</I>(<I>n</I>) = .210<I>n</I><SUP>1.351</SUP></pre>
<h4><FONT COLOR="#000080"><A name="01AD_00E0">Cost Functions<A name="01AD_00E0"></FONT></h4></P>
A single term in each of the cost functions considered so far dominates the function's values for large values of <I>n</I>. In the expression<P>
<pre>   <I>Fastest_bubble_time</I>(<I>n</I>) = .941 - .672<I>n</I> + .959<I>n</I><SUP>2</SUP></pre>
for the fastest of the bubble sort versions, the term .959<I>n</I><SUP>2</SUP> is already over 100 times the size of the next largest term when <I>n </I>equals 70. So, using<P>
<pre>   <I>Fastest_bubble_time</I>(<I>n</I>) = .959<I>n</I><SUP>2</SUP></pre>
will not be terribly inaccurate for large values of <I>n</I>.<P>
Consider 4.57<I>n</I><SUP>13</SUP>, the ratio of the approximate times for the fastest bubble sort and Shell sort. Whether the coefficient is 4.57 or any other positive value, the ratio becomes greater than 1 for large enough <I>n</I>. No matter how fast your machine, no matter how bright your programmer, no matter what language you use, if you use bubble sort for large enough problems, you will lose a sorting race to someone using Shell sort programmed in interpreted BASIC on a 4.77MHz PC.<P>
Fortunately, you have ready access to information to take advantage of this lesson. Simplified cost functions can be derived from a pseudocode design as easily as from code. The cost functions for most well-known algorithms are easily found in references. These simplified cost functions can give guidance about choosing among alternative algorithms very early in the design process as well as later when code is written.<P>
The topic of estimating cost functions for algorithms is usually called the study of algorithmic complexity. (Remember that time complexity is the context of this article, but the principles apply equally well to other measures of cost.) Other phrases that are associated with the study are "asymptotic complexity," "order analysis," and "order arithmetic."<P>
<h4><FONT COLOR="#000080"><A name="01AD_00E1">Functional Dominance<A name="01AD_00E1"></FONT></h4></P>
The answer to isolating the portion of the cost function that is related to the nature of the algorithm from the other factors lies in the idea of <I>functional dominance</I>. Simply put, given a cost function <I>f</I>, by what simpler function can it be replaced while ensuring that the replacement does not under- estimate the cost?<P>
Suppose an algorithm X has as its cost function<P>
<pre>   <I>12n</I><SUP>2</SUP> + 3<I>n</I> + 5</pre>
Can this cost function be replaced by a term <I>Cn</I><SUP>2</SUP> where <I>C</I> is a constant? Simplicity is in the eye of the beholder, but assume that this single term satisfies that condition. A little algebra applied to the inequality<P>
<pre>   12<I>n</I><SUP>2</SUP> + 3<I>n</I> + 5 &lt; <I>Cn</I><SUP>2</SUP></pre>
yields<P>
<pre>   12 + 3/<I>n</I> + 5/<I>n</I><SUP>2</SUP> &lt; <I>C</I></pre>
and lets us see how C depends on n. If I substitute 1 for n, I find that choosing C = 21 will make the inequality<P>
<pre>   12<I>n</I><SUP>2</SUP> + 3<I>n</I> + 5 &lt; 21<I>n</I><SUP>2</SUP></pre>
true whenever n, the problem size, is 1 or greater.<P>
So 21<I>n</I><SUP>2</SUP> is an approximation to the cost function of <I>x</I> for any positive <I>n</I>, and the approximation never underestimates. Of course, the approximation overestimates the runtimes somewhat. Suppose we say that "large enough" starts at <I>n</I> = 100 instead of at 1. Then the inequality will be true for <I>C</I> &gt; 12.0305 provided <I>n</I> &gt; 100. For larger problems, 12.0305<I>n</I><SUP>2</SUP> is certainly a better approximation than 21<I>n</I><SUP>2</SUP>. However, even approximate values of <I>C</I> and <I>n</I> are rarely needed. For my purposes, the important portion of the simplified cost function is the <I>n</I><SUP>2</SUP> part.<P>
<h4><FONT COLOR="#000080"><A name="01AD_00E2">"Big O" Notation<A name="01AD_00E2"></FONT></h4></P>
Formally, the discovery of values for <I>C</I> and <I>n</I> in the previous paragraph has satisfied the definition of functional dominance. A cost function <I>f</I> is dominated by a cost function <I>g</I> if and only if for some positive constant <I>C</I> and for all sufficiently large <I>n, f(n) &lt; Cg(n)</I>. The notation <I>O(g)</I>, (read "big Oh of <I>g</I>") denotes the set of all functions that are dominated by the cost function <I>g</I>. So,<P>
<pre>   12<I>n</I><SUP>2</SUP> + 3<I>n</I> + 5 <FONT FACE="Symbol">e</FONT> <I>O(n</I><SUP>2</SUP>)</pre>
expresses the notion that <I>n</I><SUP>2</SUP> dominates the left hand side. Often,<P>
<pre>  <I>f</I> <FONT FACE="Symbol">e</FONT> O (<FONT FACE="Symbol">g</FONT>)</pre>
is read as "<I>f</I> is of order <I>g</I>," or "<I>f</I> is of order <I>O(g)</I>."<P>
When two cost functions <I>f</I> and <I>g</I> have the reciprocal property that <I>f</I> <FONT FACE="Symbol">e</FONT> O(<FONT FACE="Symbol">g</FONT>) and </I>g<I> <FONT FACE="Symbol">e</FONT> O(<FONT FACE="Symbol">f</FONT>), either of the functions can be used to estimate the response of the other to an increase in problem size. For example,</I><P>
<pre>   <I>n</I><SUP>2</SUP> <FONT FACE="Symbol">e</FONT> O(12<I>n</I><SUP>2</SUP> + 3<I>n</I> + 5)</pre>
and<P>
<pre>   12<I>n</I><SUP>2</SUP> + 3<I>n</I> +5 <FONT FACE="Symbol">e</FONT> <I>O</I>(<I>n</I><SUP>2</SUP>)</pre>
Thus for large <I>n</I>, the ratios of the squares of the problem sizes estimate the ratios of the costs of executing an algorithm. If <I>n</I><SUB>1</SUB> = 1000 and <I>n</I><SUB>2</SUB> = 1500, the ratios of the respective run-times should be about<P>
<pre>   15002/10002 = 2.25</pre>
The problems of the precision of the approximation and the problem sizes covered seldom cause much trouble. Furthermore, the exact value of the constant <I>C</I> is not important. In the first place, <I>C</I> is dependent on the environment rather than solely on the algorithm. And in the second place, when the approximations are used to estimate the effects on ratios of run-times for problems of various sizes, the constant <I>C</I> divides out anyway.<P>
<h4><FONT COLOR="#000080"><A name="01AD_00E3">Handy Rules<A name="01AD_00E3"></FONT></h4></P>
Several theorems, or handy rules if you prefer, make simplifying a cost function easier. For example, the order of any polynomial whose highest power is <I>k</I> is <I>O(n</I><SUP>k</SUP><I>)</I>. That is, <I>a</I><SUB>k</SUB><I>n</I><SUP>k</SUP> + <I>a</I><SUB>k-1</SUB><I>n</I><SUP>k-1</SUP> + ... + <I>a</I><SUB>1</SUB><I>n</I> + <I>a</I><SUB>o</SUB> is of order <I>n</I><SUP>k</SUP>. So, when the cost function for an algorithm is a polynomial of degree <I>k</I>, then for "large" problems, an <I>m</I>-fold increase in problem size will result in about an <I>m</I><SUP>k</SUP>-fold increase in run time.<P>
Given two cost functions <I>f</I> and <I>g</I>, if <I>f</I> dominates <I>g</I> or <I>vice versa</I>, denote the dominant function of the pair by <I>dom(f,g)</I>. Then some other helpful rules are:<P>
1. The order of <I>f</I> + <I>g</I> is <I>O(dom(f,g))</I>.<P>
2. If <I>k</I> is a constant, the order of <I>kf</I> is <I>0(f)</I>.<P>
3. The order of <I>fg</I> is <I>O(f)O(g)</I>.<P>
These theorems can be used to simplify cost functions readily.<P>
The expressions in the above theorems occur naturally in the analysis of code or pseudocode. For example, suppose two sections of code <I>S1</I> and <I>S2</I> are executed in sequence, the time to execute <I>S1</I> is <I>f(n)</I>, and the time to execute <I>S2</I> is <I>g(n)</I>. Then, the time to execute<P>
<pre>   S1;
   S2;</pre>
is simply <I>f(n)</I> + <I>g(n)</I>. By rule 1, the order of <I>f(n)</I>+<I>g(n)</I> is dom<I>(f(n),g(n))</I> so the order of the above sequence is <I>O(dom(f(n),g(n))</I>.<P>
<h4><FONT COLOR="#000080"><A name="01AD_00E4">Rules for Code Sequences<A name="01AD_00E4"></FONT></h4></P>
Other rules of deriving complexity estimates for code are illustrated in the following:<P>
<pre>Rule:
    if (cond)    /* assume time to evaluate
        cond doesn't depend on n */
      S1;    /* S1 <FONT FACE="Symbol">e</FONT> O(f) */
    else
      S2;    /* S2 <FONT FACE="Symbol">e</FONT> O(g) */</pre>
<I>O(dom(f,g))</I> is order of <I>if</I><I>-</I><I>then</I> statement.<P>
<pre>Rule:
    while(cond) /* loop is executed O(g) times */
      S1;   /* O(f) is order of body of loop */</pre>
<I>O(fg)</I> is order of entire loop.<P>
Now for a real example. Consider the following pseudocode for a selection sort, where <I>x</I> is an array of <I>n</I> values:<P>
<pre>for i = 1 to n-1</pre>
find the index j of the smallest<P>
<pre>   value in the array between</pre>
positions i and n-1;<P>
interchange the items at positions j<P>
<pre>   and i;
endfor;</pre>
Note that the loop is executed <I>n</I><I>-</I><I>1</I> times, which is <I>O(n)</I>. The section that searches the array for the smallest value will search differing size lists with each iteration. The longest segment searched is <I>(n </I><I></I>&#151;<I> 1)</I> elements long and the shortest has length 2. So, the average length of the segments searched is (<I>n</I> + 1)/2 which is of order <I>O(n)</I>.<P>
Since code of order <I>O(n)</I> is inside a loop that is executed <I>O(n)</I> times, the order of the algorithm is <I>O(n</I><I><SUP>2</SUP></I><I>)</I>.<P>
<h4><FONT COLOR="#000080"><A name="01AD_00E5">Best and Worst Case Behavior<A name="01AD_00E5"></FONT></h4></P>
The previous analyses have assumed "average" behavior. However, algorithms can also be analyzed for worst-case or best-case behavior. For example, bubble sort is superb if used to sort an already sorted array. The algorithm discovers no swaps on its first pass and exits, so its order in that case is <I>O</I>(<I>n</I>). Quicksort with naively chosen pivots is of order <I>O</I>(<I>n</I><SUP>2</SUP>) for previously sorted arrays and so can be a very poor choice in some circumstances.<P>
Some common algorithms and their average case orders are given in <A href="tab2.htm">Table 2</a>.
<P>
As a final example, look at the flowchart in <A href="fig1.htm">Figure 1</a>
of a module that contains four other modules. The numbers on the lines indicate the number of times that path is traversed. The question is, where should limited resources be concentrated to improve the performance of the code for large problems?<P>
To find the bottleneck, compute the contribution of each module to the overall order of the code. The contribution to the <I>if</I><I>-</I><I>then</I><I>-</I><I>else</I> construct by A is <I>O(nxn)</I> = <I>O(n</I><SUP>2</SUP><I>)</I>. <I>B</I> contributes <I>O(n</I><SUP>2</SUP><I>xn)</I> = <I>O(n</I><SUP>3</SUP><I>)</I> to the <I>if</I><I>-</I><I>then</I><I>-</I><I>else</I>. The dominant of the two orders is<P>
<pre>   <I>dom(n</I><SUP>2</SUP><I>n</I><SUP>3</SUP><I>) = 0(n</I><SUP>3</SUP><I>)</I></pre>
so the order of the <I>if</I><I>-</I><I>then</I><I>-</I><I>else</I> can be traced directly to <I>B. C</I> is executed <I>O(n)</I> times and contributes<I> O(n</I><SUP>1.5</SUP>) to the process. But, <I>C</I>'s contribution is dominated by <I>B. D</I> is executed <I>O(n)</I> times and has order <I>O(nlog(n))</I> so its contribution is<I> O(n</I><I><SUP>2</SUP></I><I>1og(n))</I>. So, since<P>
<pre>   <I>dom(n</I><SUP>3</SUP><I>,n</I><SUP>2</SUP><I>log(n)) = O(n</I><SUP>3</SUP><I>)</I></pre>
the culprit is <I>B</I>. To improve the efficiency of the program, work to decrease the order of <I>B</I> and to decrease the number of times <I>B</I> is executed.<P>
<h4><FONT COLOR="#000080"><A name="01AD_00E6">When to Analyze<A name="01AD_00E6"></FONT></h4></P>
You have two opportunities to influence the execution time of a software product. The first, and perhaps most important, is during the design phase when you choose the data structures and their accompanying algorithms to represent the objects in the application environment. Your primary tool at this point is complexity analysis as discussed here.<P>
Your second opportunity is during the coding phase. Wisely choosing languages, hardware, and other such factors can have a profound effect on software performance. But my best advice is to resist the temptation to write code that sacrifices simplicity for speed. Strive instead for understandable, correct code. Then, when you have working code, use a profiler to determine the sections of code that contribute most to run time and work on those sections. Numerous studies have shown that the majority of the time to execute a program is spent in a small minority of the lines of code. Clearly, time spent recoding small sections of critical code for speed brings greater rewards than time used optimizing rarely executed code.<P>
Remember that the phrase "for large problems" was used throughout the discussion. How large must a problem be before the above discussion is relevant? The answer depends on the problem, the algorithms considered, the machine, and the user of the program. The information gleaned from the order analysis of time complexity is almost always of great value, but there are occasional exceptions. For example, I suspect that if I had to maintain a small, sorted table of ten or so items in an operating system, I would find a simple insertion sort to be faster than Quicksort because of the overhead.<P>
A brief example, admittedly extreme, will illustrate the relative benefits of compiler improvements, hardware speed improvements, recoding, and other such pursuits versus finding better algorithms. Suppose a problem of size 64 could be solved in 24 hours by each of two algorithms <I>A</I> and <I>B</I> and that the order of <I>A</I> is <I>O(n</I><SUP>2</SUP><I>)</I> and that the order of <I>B</I> is <I>O</I>(2<SUP>n</SUP>). A machine is available that is 100 times faster than current equipment. How large of a problem can now be solved in 24 hours? Using algorithm <I>A</I>, a problem of size 640 can be solved in a day. Using algorithm <I>B</I>, the capacity goes from 64 to only 70.<P>
So a final lesson to be learned from this article is that improvements in hardware will never solve all computing problems. In some cases, only new algorithms will do.<P>
<h4>References</FONT></h4></P>
1.     <I>Algorithms</I>, Robert Sedgewick, Addison-Wesley, Reading, Ma., 1984<P>
2.     <I>Discrete Mathematics in Computer Science</I>, Donald Stanat and David McAllister, Prentice-Hall, Edgewood Cliffs, NJ, 1977<P>
3.     <I>Handbook of Algorithms and Data Structures</I>, G. H. Gonnet, Addison-Wesley, Reading, Ma., 1984<P>
<a name="ref4">4</a>.     <I>Sorting Out Sorting</I>, Computer Systems Research Group, Toronto, 1981<P>
5.     <I>Writing Efficient Programs</I>, Jon Bentley, Prentice-Hall, Edgewood Cliffs, NJ, 1982<P>

<h4><a href="../../../source/1992/sep92/davis.zip">Get Article Source Code</a></h4>

</BLOCKQUOTE>
</BODY>
</HTML>
