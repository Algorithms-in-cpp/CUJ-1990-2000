


<HTML>
<HEAD>

<TITLE>July 1991/Standard C</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocjul.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Columns</FONT></H2>

<hr><h2 align="center"><font color="#800000">Standard C<A name="0189_00CE"><A name="0189_00CE"></font></h2><P>
<h3 align="center"><A name="0189_0000"><A name="0189_0000">Math Functions</h3><P>
<h3 align="center"><font color="#800000">P.J. Plauger</font></h3><hr><blockquote><P>
<P><i><A name="0189_0000"><A name="0189_0000">P.J. Plauger is senior editor of The C Users Journal. He is secretary of the ANSI C standards committee, X3J11, and convenor of the ISO C standards committee, WG14. His latest book is Standard C, which he co-authored with Jim Brodie. You can reach him at pjp@plauger.uunet.</i></P><P>
<h4><FONT COLOR="#000080"><A name="0189_00CF">Introduction<A name="0189_00CF"></FONT></h4></P>
Writing good math function is hard. It is still commonplace to find professional implementations of programming languages that provide math functions with serious flaws. They may generate intermediate overflows for arguments with well-defined function values. They may lose considerable significance. Or they may generate results that are just plain wrong in certain cases.<P>
What's mildly surprising about this state of affairs is that implementors have had plenty of time to learn how to do things right. The earliest use for computers was to solve problems with a distinctive engineering or mathematical slant. The first libraries, in fact, consisted almost entirely of functions that computed the common math functions. FORTRAN, a child of the 1950s, was named for its ability to simplify FORmula TRANslation. Those formulas were larded with math functions.<P>
Over the years, implementors have become more sophisticated. The IEEE 754 Standard for floating-point is a significant milestone on the road to safer and more consistent floating-point arithmetic. (I discussed floating-point representations and the IEEE 754 Standard in <I>Standard C</I>, <I>CUJ</I> January 1991.) Yet in another sense, IEEE 754 adds to the implementor's woes. It introduces the complexity of gradual underflow, codes for infinities and not-a-numbers, and exponents of different sizes for different precisions. Small wonder that many implementors often support only parts of the IEEE 754 Standard.<P>
Regular readers of this column know by now that I have written an entire implementation of the Standard C library. I have been discussing it in installments for nearly a year now. You can soon see it all in one place. Prentice-Hall is publishing my book, <I>The Standard C Library</I>, and The C Users Group is making the machine-readable code available.<P>
I spent about as much time writing and debugging the functions declared in <I>&lt;math. h&gt;</I> as I did the rest of the library combined. That surprised me, I confess. I have written math libraries at least three times over the past twenty-odd years. You'd think that I have had plenty of time to learn how to do things right, as well. I certainly thought so.<P>
<h4><FONT COLOR="#000080"><A name="0189_00D0">Goals<A name="0189_00D0"></FONT></h4></P>
Writing <I>&lt;math. h&gt;</I> took so long because I adopted several rather ambitious goals:<P>
<UL><li>The math library should be portable over a range of popular computer architectures. All functions are designed to yield 56 bits of precision. That makes them suitable for a number of machines with 64-bit <I>double</I> representation &#151; those with IEEE 754-compatible math co-processors (53 bits of precision), the IBM System/370 family (53 to 56 bits), and the DEC VAX family (56 bits).</li>
<li>Each function should accept all argument values in its domain (the argument values for which it is mathematically defined). It should report a <I>domain error</I> for all other arguments. In this case, the function returns a special code that represents NaN for not-a-number.</li>
<li>Each function should produce a finite result if its value has a finite representation. It should report a <I>range error</I> for all values too large or too small to represent. If the value is too large in magnitude, the function returns a special code +Inf that represents plus infinity, or the negative of that code -Inf that represents minus infinity, as appropriate. If the value is too small in magnitude, the function returns zero.</li>
<li>Each function should produce the most sensible result for the argument values NaN, +Inf, and --Inf. On an implementation that supports multiple NaN codes, such as IEEE 754, the functions preserve particular NaN codes wherever possible. If a function has a single argument and the value of that argument is a NaN, for example, the function returns the value of the argument.</li>
<li>Each function should endeavor to produce a result whose precision is within <I>two bits</I> of the best-available approximation to any representable result.</li>
<li>No function should ever generate an overflow, underflow, or zero divide, regardless of its argument values and regardless of the result.</li>
<li>No function requires a floating-point representation other than <I>double</I> to perform intermediate calculations.</li></UL>
I believe I have come close to these goals, if I haven't achieved them in all cases.<P>
<h4><FONT COLOR="#000080"><A name="0189_00D1">Non-Goals<A name="0189_00D1"></FONT></h4></P>
I should also point out a number of goals I chose <I>not</I> to achieve:<P>
<UL><li>The library doesn't try to distinguish +0 from --0. IEEE 754 worries quite a bit about this distinction. All the architectures I mentioned above can represent both flavors of zero. But I have trouble accepting (or even understanding) the rationale for this extra complexity. I can sympathize with recent critiques of the IEEE 754 Standard that challenge that rationale. Most of all, I found the functions quite hard enough to write without fretting about the sign of nothing.</li>
<li>The library does nothing with various flavors of NaNs. IEEE 754 arithmetic, for example, distinguishes <I>quiet NaNs</I> from <I>signalling NaNs</I>. The latter should generate a signal or raise an exception. This implementation essentially treats all NaNs as quiet NaNs.</li>
<li>I provide low-level primitives only for the IEEE 754 representation. They happen to work rather well with the DEC VAX floating-point representation as well, but the fit isn't perfect. The VAX hardware doesn't recognize as special the code values for things like +Inf and -Inf. Such codes can disappear in expressions that perform arithmetic with them. The primitives must be altered to support System/370 floating-point.</li>
<li>I have not checked the functions on System/370. The "wobbling precision" on that architecture requires special handling. Mostly, I have tried to provide such special handling, but it may not be thorough enough.</li>
<li>Many functions are probably suboptimal for machines that retain much fewer than 53 bits of precision in type <I>double</I>. The C Standard permits a <I>double</I> to retain as few as ten decimal digits of precision &#151; about 31 bits. For such machines, you should reconsider the approximations chosen in various math functions.</li>
<li>Functions that use approximations will almost certainly fail for machines that retain more than 56 bits of precision. For such machines, you <I>must</I> reconsider the approximations chosen.</li>
<li>Floating-point representations with bases other than 2 or 16 are poorly supported by this implementation of the math library. An implementation with base-10 floating-point arithmetic, for example, would call for significant redesign.</li></UL>
Even with these constraints, you should find that this implementation of the math library is useful in a broad variety of environments.<P>
<h4><FONT COLOR="#000080"><A name="0189_00D2">Numerical Techniques<A name="0189_00D2"></FONT></h4></P>
Computing math functions safely and accurately requires a peculiar style of programming:<P>
<UL><li>The finite precision of floating-point representation is both a blessing and a curse. It lets you choose approximations of limited accuracy. But it offers only limited accuracy for intermediate calculations that may need more.</li>
<li>The finite range of floating-point representation is also both a blessing and a curse. It lets you choose safe data types to represent arbitrary exponents. But it can surprise you with overflow or underflow on intermediate calculations.</li></UL>
You learn to dismantle floating-point values by performing various <I>seminumerical operations</I> on them. The separate pieces are fractions with a narrow range of values, integer exponents, and sign bits. You can work on these pieces with greater speed, accuracy, and safety. Then you paste the final result together using other seminumerical operations.<P>
Over a sufficiently narrow range of values, most functions are easy enough to approximate. The trick is to find a form that is most economical &#151; you want to perform the minimum number of floating-point additions, multiplications, and divisions on each call to the function. For example, you can approximate essentially <I>any</I> smooth curve over a narrow range with a polynomial that has enough terms. If the curve has an asymptote, however, you may find that you need lots of terms. Such curves favor a <I>rational approximation</I> &#151; a ratio of two polynomials. Computing two smaller polynomials and dividing their values is often faster than computing one larger polynomial.<P>
One thing you do <I>not</I> do is write a loop that tests for any sort of convergence. You can compute the <I>sqrt</I> function, for example, by repeated divide-and-average starting with an initial guess. Know the quality of the guess and you can precompute how many times to iterate. You can compute the <I>sin</I> function, for another example, by truncating its Taylor series expansion. Know the range of arguments you must accept and you can precompute how many terms to keep. You want to avoid messy and time-consuming comparisons.<P>
If a polynomial can do the job, it is usually the approximation of choice. Horner's rule lets you evaluate a polynomial of order <I>N</I> with a simple multiply-add loop:<P>
<pre>double y = c[0];
   int i = 0;

   while (++i &lt;= N)
      y = y * x + c[i];</pre>
Here, the coefficients are stored in the constant array <I>c</I>.<P>
To approximate a function with a polynomial, you can often use linear least-squares. Say you want to approximate the function <I>f(x)</I> with a quadratic (second-order) polynomial over the interval [<I>a, b</I>]. Then you want to minimize the sum <I>S</I> defined as:<P>
<IMG SRC="equat1.gif"><P>
If you remember enough calculus, this approach is much easier than you might at first think. Differentiate <I>S</I> with respect to each of the <I>C</I><I>i</I> and set each result to zero. You end up with three linear equations in three unknowns. You have to know how to integrate terms such as <I>f(x)* x</I><I>n</I>, then solve the set of equations to sufficient precision. Survive all those obstacles and you get a neat approximation.<P>
One way to save some of that work is to find an infinite-series representation of the function you wish to approximate. Then all you have to do is decide where to chop it off for the precision you need. If you want to be extra tidy, you can then <I>economize</I> the result by using Chebychev polynomials. That reduces the number of terms you must compute for the same degree of precision. It also spreads the error out over the entire range instead of bunching it at one end. (For an excellent guide to understanding and using this approach, see R.W. Hamming, <I>Numerical Methods for Scientists and Engineers</I>, McGraw-Hill Book Co. Inc, New York, 1962).<P>
So another recipe for finding a polynomial approximation is:<P>
<UL><li>Find a series representation that converges.</li>
<li>Determine where it's safe to truncate the series.</li>
<li>Economize it to get a smaller polynomial with better error properties.</li></UL>
For many applications, you can obtain a satisfactory approximation with a minimum of work.<P>
You can obtain better approximations using more advanced techniques. John F. Hart, et al., <I>Computer Approximations</I>, (Robert E. Krieger Publishing Company, Malabar, Florida, 1978) shows you quite a few. This book is also chock full of carefully computed approximations to the most widely used math functions. You can often find one with just the precision you need.<P>
An excellent book on writing math libraries is William J. Cody, Jr. and William Waite, <I>Software Manual for the Elementary Functions</I>, (Prentice-Hall, Inc., Englewood Cliffs, N.J., Inc., 1980). Many of the functions I wrote for <I>&lt;math.h&gt;</I> make use of algorithms and techniques described by Cody and Waite. Quite a few use the actual approximations derived by Cody and Waite especially for their book. I confess that on a few occasions I thought I could eliminate some of the fussier steps they recommend. All too often I was proven wrong. I happily built on the work of these careful pioneers.<P>
As a final note on resources, the acid test for many of the functions declared in <I>&lt;math.h&gt;</I> was the public-domain <I>ELEFUNT</I> (for "elementary function") tests. These derive from the carefully wrought tests in Cody and Waite. To obtain a copy, mail to the Internet address <I>netlib@research. att.com</I> the request:<P>
<pre>send index from elefunt</pre>
<h4><FONT COLOR="#000080"><A name="0189_00D3">Programming Techniques<A name="0189_00D3"></FONT></h4></P>
I have said just about all I intend to say about the numerical techniques behind the math functions. Others have done a better job of explaining that topic than I can do. Go read the books mentioned above if you want to learn more. In what follows, my primary interest is what you can do programming <I>in</I> C and <I>for</I> the C programmer.<P>
The functions in <I>&lt;math.h&gt;</I> vary widely. I find it best to discuss them in several groups. I begin with a few special constants and a brief sample of the functions that follow. The sample shows some of the seminumerical functions that manipulate the components of floating-point values, such as the integer, fraction, and exponent parts. Along the way, I also present several low-level primitives. These are used by all the functions declared in <I>&lt;math.h&gt;</I> to isolate dependencies on the specific representation of floating-point values.<P>
I discussed earlier the general properties of machines covered by this particular set of primitives. I emphasize once again that the parametrization doesn't cover all floating-point representations used in modern computers. You may have to alter one or more of the primitives for certain computer architectures. In rarer cases, you may have to alter the higher-level functions as well.<P>
I'll start with the definition of the macro <I>HUGE_VAL.</I> I define it as the data object _<I>Hugeval</I> and initialize it with the IEEE 754 code for +Inf. To do so, I introduce the type _<I>Dconst</I>. It is a union that lets you initialize a data object as an array of four <I>unsigned shorts</I>, then access the data object as a <I>double</I>. The data object _<I>Hugeval</I> is one of a handful of floating-point values that are best constructed this way.<P>
<A href="list1.htm">Listing 1</a>
shows the file <I>xvalues.c</I> that defines this handful of values. It includes a definition for_<I>Inf</I> that matches _<I>Hugeval.</I> I provide both in case you choose to alter the definition of <I>HUGE_VAL.</I> The file also defines:<P>
<UL><li>_<I>Nan</I>, the code for a generated NaN that functions return when no operand is also a NaN but the result is a NaN</li>
<li>_<I>Rteps</I>, the square root of <I>DBL_EPSILON</I> (approximately), used by some functions to choose between different approximations</li>
<li>_<I>Xbig</I>, the inverse of _<I>Rteps._D</I>, used by some functions to choose between different approximations</li></UL>
The use of the last two values will become clearer when you see how functions use them.<P>
The file <I>xvalues.c</I> is essentially unreadable. It makes extensive use of system-dependent parameters defined in the internal header <I>&lt;yvals.h&gt;</I>. I use this internal header throughout the library to summarize the properties of a given system. <I>xvalues.c</I> does not directly include <I>&lt;yvals.h&gt;</I>. Instead, it includes the internal header <I>"xmath.h"</I> that includes <I>&lt;yvals.h&gt;</I> in turn. All the files that implement <I>&lt;math.h&gt;</I> include <I>"xmath.h"</I>. Since that file contains an assortment of distractions, I show it in pieces as the need arises.<P>
The macros defined in <I>"xmath.h"</I> that are relevant to <I>xvalues.c</I> are:<P>
<pre>#define _DFRAC  ((1&lt;&lt;_DOFF)-1)
#define _DMASK  (0x7fff&amp;~_DFRAC)
#define _DMAX   ((1&lt;&lt; (15-_DOFF))-1)
#define _DNAN   (0x8000|_DMAX&lt;&lt;_DOFF|1&lt;&lt;(_DOFF-1))</pre>
All these values build on the value of the macro _<I>DOFF,</I> defined in <I>&lt;yvals.h&gt;</I>. It specifies the offset in bits of the characteristic in the most-significant word of a <I>double</I>. For IEEE 754, it has the value 4. For the VAX, it has the value 7.<P>
If you can sort through this nonsense, you will observe that:<P>
<UL><li>The code for Inf has the largest-possible characteristic (_<I>DMAX</I>) with all fraction bits zero.</li>
<li>The code for generated NaN has the largest-possible characteristic with the most-significant fraction bit set.</li></UL>
In general, a NaN has at least one nonzero fraction bit. I chose this particular code for generated NaN to match the behavior of the Intel 80X87 math coprocessor.<P>
The presence of all these codes makes even the simplest functions nontrivial. For example, <A href="list2.htm">Listing 2</a>
shows the file <I>fabs.c</I>. In a simpler world, you could reduce it to the last <I>return</I> statement:<P>
<pre>return (x &lt; 0.0 ? -x : x);</pre>
Here, however, we want to handle NaN, -Inf, and +Inf properly along with zero and finite values of the argument <I>x</I>. That takes a lot more testing.<P>
<I>xdtest.c</I> in <A href="list3.htm">Listing 3</a>
defines the function _<I>Dtest</I> that categorizes a <I>double</I> value. The internal header <I>"xmath.h"</I> defines the various offsets and category values that _<I>Dtest</I> uses. The macro definitions of interest here are:<P>
<pre>    /* word offsets within double */
#if_DO==3
#define _D1    2  /* little-endian order */
#define _D2    1
#define _D3    0
#else
#define _D1    1  /* big-endian order */
#define _D2    2
#define _D3    3
#endif
    /* return values for _D functions */
#define FINITE  -1
#define INF     1
#define NAN     2</pre>
The header <I>&lt;yvals.h&gt;</I> defines the macro _<I>DO</I> as either the value 3 or zero. It is the index of the most-significant word in a <I>double</I>. Hence, it specifies the order in which words appear in memory (sometimes known as "byte sex").<P>
Note that a floating-point value with characteristic zero is not necessarily zero. IEEE 754 supports gradual underflow. The value is zero only if all bits (other than the sign) are zero.<P>
<h4><FONT COLOR="#000080"><A name="0189_00D4">In Times To Come<A name="0189_00D4"></FONT></h4></P>
This sampler should give you a feel for the basic approach. Next month, I start through the math library in earnest. Remember that my goal is not to turn all C programmers into numerical analysts. Rather, it is to give you a better idea of what goes on when you call a library function. That should teach you where to be alert for problems. It should also teach you to use, and to appreciate, a good math library when you find one.<P>

<h4><a href="../../../source/1991/jul91/plauger.zip">Get Article Source Code</a></h4>

</BLOCKQUOTE>
</BODY>
</HTML>
