


<HTML>
<HEAD>

<TITLE>January 1991/Pennies In Long Double</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocjan.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Portability</FONT></H2>

<hr><h2 align="center"><font color="#800000">Pennies In Long Double<A name="0024_0009"><A name="0024_0009"></font></h2><P>
<h3 align="center"><font color="#800000"><A name="0024_0000"><A name="0024_0000">Dr. Timothy Prince</font></h3><hr><blockquote><P>
<P><i><A name="0024_0000"><A name="0024_0000">Timothy Prince has a B.A. in physics from Harvard and<I></I> <I></I>a Ph.D. in mechanical engineering from the University of<I></I> <I></I>Cincinnati. He has 25 years of experience in aerodynamic<I></I> <I></I>design and computation. He can be contacted at 39<I></I> <I></I>Harbor Hill Dr., Grosse Pointe Farms, MI 48236.</i></P><P>
<h4><FONT COLOR="#000080"><A name="0024_000A">Coprocessor Problems<A name="0024_000A"></FONT></h4></P>
In a letter to <I>The C Users Journal</I> some time ago, Paul Wexler pointed out some confusing results that developed while dealing with dollars and cents on numeric coprocessors with extended double precision registers. He showed that the Microsoft C v5.1 and Turbo C compilers treated the code:<P>
<pre>double d = 0.99;
int i = (d *= 100.0);</pre>
as if the second line were written:<P>
<pre>register long double dtemp = (long double) d*100.0;
d = dtemp;
int i = dtemp;</pre>
with the results d==99.0 and i==98!<P>
According to K&amp;R or ANSI C, the original code should give the same results as:<P>
<pre>d *= 100.0;
int i = d;</pre>
So the most popular C compilers conform neither to K&amp;R nor to ANSI! Paul tried the same code on some other systems and got correct results.<P>
I tried it on the Sun 4.0 compiler and found that<P>
<pre>f77 -f68881</pre>
gave the same non-K&amp;R results as Paul had obtained with the Microsoft and Turbo compilers, but<P>
<pre>f77 -f68881 -02</pre>
generated a code expansion equivalent to:<P>
<pre>i = d = (long double)d*100.0;</pre>
which gives the correct results.<P>
The value of 0.99 rounded to the IEEE standard double precision format happens to be slightly less than 99/100. <I>(long double)d*100</I> is less than 99, but if rounded again to standard double precision, the result is exactly 99.<P>
<h4><FONT COLOR="#000080"><A name="0024_000B">Normal Surprises In Floating Point<A name="0024_000B"></FONT></h4></P>
Most of us at some time have written something like<P>
<pre>for(x=0.01; x!=l.0; x+=0.01)
   .....</pre>
Because there is no exact binary equivalent to 0.01, the conditional is always true. The code might work on a system without extended precision, but is likely to hang on 80X87 or 68881 processors. If the condition is changed to <I>x&lt;=1.0,</I> the number of iterations will vary by 1 among various systems. Change the counter to integral values:<P>
<pre>for(xx=1.; xxd; ++xx) {
   x=xx*0.01;
   ....}</pre>
and it will run portably on all systems with correct floating point implementation, no matter what type is given for <I>xx</I>. On many systems, it will run faster if the types of <I>x</I> and <I>xx</I> are the same.<P>
<h4><FONT COLOR="#000080"><A name="0024_000C">Try It On Your Own Machine<A name="0024_000C"></FONT></h4></P>
I carried Paul's example a bit further. In every case, the result for <I>i</I> (in his example) is either the same as the result <I>j</I> or the result <I>k</I> (when compilers take liberties with the standards,) but the <I>k</I> may be less than <I>i</I> when varying precisions of floating point arithmetic are used. In single precision or PDP-11 style double precision, the value of 0.99 is greater than 99/100, and a series of other values must be tried to verify the adherence of the compiler to the standard.<P>
C libraries should be able to convert output to the 17 or 18 digits of precision that are required to show the difference between the binary approximation and the original decimal value. Any binary floating point number may be expressed exactly in decimal, but there is no practical reason or means for going beyond the number of digits required to specify a unique binary value. Converting the other way, most fractional decimal values have no exact equivalent binary fractional value. If properly programmed, the numeric coprocessors can perform the conversions reversibly to the extent demanded by the IEEE standards. In plain double precision there will be small deviations.<P>
The inexact converson from decimal to binary is sometimes taken as an argument for using decimal arithmetic in applications such as financial calculations. However, practical applications of this kind generally have a fixed number of fractional decimal places. It should make no difference to the result whether binary or decimal arithmetic is used.<P>
You may wish to see the hexadecimal pattern of the binary floating point number. Since the <I>%x</I> conversion is not available for <I>double</I>, you must use unions or pointer casts. Converting the value byte by byte avoids some of the confusion introduced when the byte storage order is reversed for <I>long</I> or <I>double</I>. K&amp;R compilers may have no <I>unsigned char</I>, so the <I>0xff</I> mask is used to compensate. <I>%lx</I> conversion would be expected to work on <I>float</I> if <I>sizeof(float)==sizeof(long)</I> and byte orders are not of concern. When you run the code, look for the repeating patterns in the hex representation for evidence of rounding. When the pattern that is chopped off is greater than <I>0x80</I>..., there should be upward rounding.<P>
If your system does not have extended precision, double rounding (after divide and again after subtract) occasionally will give an incorrect rounding. This occurred only once in the hex output on my test using Z80 software floating point. The same effect reduces the number of different values that <I>printf()</I> can produce, making numbers appear to be exact numbers of cents when they aren't. This violates the IEEE standard but is normal without extended precision.<P>
<h4><FONT COLOR="#000080"><A name="0024_000D">Why Do The Gurus Do This To Us?<A name="0024_000D"></FONT></h4></P>
In the design of the 8087 and 68881 numeric coprocessor families, the number of instructions required was reduced by choosing one data type (<I>register long double</I>) for the results of all floating point operations. This was a logical extension of the traditional C scheme of promoting all <I>float</I> arguments and expressions to <I>double</I>. In normal operation, it often gives an extra decimal digit of accuracy and avoids most accidents caused by exceeding the range provided for <I>double</I>, which was small enough to require special precautions on older hardware.<P>
Since the coprocessors give an incentive in accuracy as well as speed for allocation of variables to registers, numerical results may be expected to differ on these machines from the values obtained on other architectures. In fact, by literal interpretation of the IEEE floating point standard, these coprocessors may give incorrect results in double precision. All operations are rounded first to 64 bits precision (plus a 15-bit exponent and a sign) and then to 53 bits precision (with 11-bit exponent and hidden bit allowing room for the sign).<P>
In effect, the coprocessor rounding threshold varies from about 0.4998 to 0.5002 of the unit in the last place (ULP) of a double. Theoretically, this could cause the same kind of non-standard behavior, but the possibility is remote and over-shadowed by effects of the magnitude of 0.25 ULP such as those which we have been discussing. Typical Z80 floating point software routines have a rounding threshold which varies from 0.49 to 0.51, and still give excellent accuracy.<P>
You'll hardly notice whether a value that is assigned and then used for subsequent calculation is rounded off, unless you push your luck. Meanwhile, with the benchmark wars continuing, the compiler writers are looking for every chance to maximize the use of register variables. Certain optimizing compilers, given Paul's example code, will perform all the calculations at compile time, leaving nothing but the <I>printf</I> calls, and no record of the sequence of operations used to obtain the constant results.<P>
<h4><FONT COLOR="#000080"><A name="0024_000E">Are The Standards Correct Or Is My Compiler Right?<A name="0024_000E"></FONT></h4></P>
There might be a case for allowing compilers to ignore casts and assignments that reduce the width of expressions, but when I write code such as<P>
<pre>int ix;
double x;
x -= (ix = ((x=0.0) ? 0.5 : -0.5) + x);</pre>
I expect <I>x</I> to come out such that <I>x</I> += <I>ix</I> would restore the original <I>x</I>. This wouldn't work if the double expression were allowed to leapfrog over the assignment to <I>ix</I>. When something from the integer family is involved, most of us would agree with the standard treatments. The Motorola coprocessors can evaluate this expression without reading data back from memory, so there is less likelihood of cheating to obtain faster code. On the Intel processors, the result that I intended could be produced by<P>
<pre>register long double xtemp = RINT(x);
/*  old Intel FORTRAN function */
ix  = xtemp;
x - = xtemp;</pre>
but the compiler can't be sure of this. <I>RINT()</I> was Intel's invention. It would round a <I>long double</I> to an integral value in accordance with IEEE standards.<P>
When various precisions of floating point are involved, opinions are not unanimous. In the matrix factorization code (3), I assigned the value that was to be used in immediately subsequent calculations to a local <I>double</I> variable, before assigning it to a <I>float</I> array element, to save the time taken by compilers that would either read it back from memory or cast it back to <I>double</I>. If I knew that the compiler would generate fast code by leapfrogging an assignment, (some do), I wouldn't write it that way.<P>
Certain compilers will ignore the evaluation implied by an intermediate assignment if the assigned value is not actually used later on. I assume that this is contrary to the ANSI standard, but I'm sure that others, whose opinions count more, will assume differently. This is a gray area, since one must be prepared for a compiler to promote a variable to <I>register long double</I>, for instance, as long as it does so with reasonable consistency. If the variable is never used, by this logic, the compiler could choose to promote it to the extent that the implied cast is eliminated. Using the same name for another purpose later on doesn't count either, since renaming is an accepted technique for optimizing compilers.<P>
On the other hand, I might want to round an expression off to a precision consistent with the numbers from which it came, to avoid problems analogous to Paul Wexler's. If the compiler were allowed to pass over assignments and casts, it couldn't be done. Besides, it might be difficult to remember if the compiler treated <I>double</I> differently from its treatment of <I>int</I>.<P>
It's easier to write code that guarantees that the rounding will be passed over than to prevent the compiler from making a surprise interpretation. The standard definitions give more flexibility, control, and self-consistency in the language definition.<P>
<h4><FONT COLOR="#000080"><A name="0024_000F">How To Pinch Pennies<A name="0024_000F"></FONT></h4></P>
Paul Wexler feels that C is inadequate to control the operations of numeric coprocessors. In particular, he suggests that he would like full control of the rounding mode. Naturally, he is dismayed by the occasional theft of 99.44% of a penny by the computational troll.<P>
Several compilers supply system-dependent functions that allow the rounding mode to be changed. I suggest that there are valid reasons for their unpopularity. First, of course, they don't exist on many compilers and there isn't any accepted syntax, so you must accept lack of portability. Secondly, the idea of having to break your source code to insert such function calls, and take the performance penalties involved in preventing compiler optimizations or code pipelining across such calls, is unappealing.<P>
The need for a way to specify rounding to an integral value has been recognized in the definition of many languages and in the IEEE floating point standard. For instance, Pascal has the <I>ROUND()</I> function, which was adopted by FORTRAN as <I>NINT()</I>. At the same time, FORTRAN adopted the even more important rounding without type casting called <I>ANINT()</I>. If it were not for the disagreement between the IEEE and the language standards committees over the need for rounding to even in the half-way case, the situation would be satisfactory in these languages.<P>
In C, the Pascal <I>ROUND()</I> can be simulated by a macro involving adding or subtracting 0.5 before applying <I>(int)</I>. Situations remain where it is preferable not to use a cast, either for performance or to avoid narrowing the range.<P>
Financial calculations are an example. The way to perform calculations down to the cent is to use integral numbers of cents and to round off to the nearest cent after each operation that could introduce a fractional value, including conversions from dollars to cents. If we always use double precision, numbers of cents up to 253-1, or nearly 16 digits (for standard IEEE double precision), may be calculated exactly.<P>
The 8087 appears to have been intended for the integral value floating point method of decimal support, as it has an instruction for storing directly from <I>register long double</I> to an 18-digit decimal integer. In fact, if the <I>atof()</I> function makes use of the 8087, it must convert the input decimal dollar figure to cents internally before converting to binary and changing it back to dollars by multiplying by 0.01. We would have been better off to convert the decimal string to cents ourselves to begin with. Then we could use the "inexact" flag to verify that <I>atof()</I> performs an exact conversion.<P>
The designers of C have been stubborn in not providing a standard floating round function. Many processors have one (e.g., the Intel <I>FRNDINT</I>), and the inability to have it compiled into the code when it is needed will hurt performance. There is no portable way to do it in C. The best that can be done is to use a macro with provisions for change to support specific machines. Rounding to an integer can be done portably, and conceivable optimizing compilers could recognize code such as the <I>ix=x</I> example above and generate more efficient code when appropriate.<P>
Rounding may be performed in IEEE double arithmetic by<P>
<pre>#include &lt;float.h&gt;
#define ROUNDER 1./DBL_EPSILON
#define FRNDINT(x) (((x) &lt;ROUNDER) &amp;&amp; ((x) &gt;-ROUNDER) \
   ? ((x)&gt;=0.0 ? ((x)+ROUNDER)-ROUNDER : ((x)-ROUNDER)+ROUNDER) : (x))</pre>
which normally would be simplified by omitting the cases for absolute value of <I>x</I> greater than <I>ROUNDER</I>. Without this precaution, larger numbers are rounded to multiples of 2. The constant <I>ROUNDER</I> has the value 252 for standard IEEE double precision.<P>
This code will take over twice as long as the hardware round instruction. It's usually faster than the FORTRAN <I>ANINT()</I> functions, which are written to overrule the IEEE style rounding of most modern hardware designs. You will need to check that your compiler treats the parentheses in accordance with the draft ANSI standard. At least one vendor (Convex) has chosen to violate this feature of the language standards in search of better benchmark speed. The whole macro could be optimized away!<P>
Lovers of the obscure will notice that a slight rearrangement permits all of the comparisons to be performed with integer instructions, if the programmer knows the formats actually used for <I>int</I> and <I>double</I> data. A compiler for a pipelined or multiple adder machine should produce code such as<P>
<pre>register double temp1,temp2;
register int tmp1,tmp2;
temp1  =  x+ROUNDER;
temp2  =  x-ROUNDER;
tmp1  =  x==0.0;
tmp2  =  x+ROUNDER;
temp1  -=  ROUNDER;
temp2  +=  ROUNDER;
tmp2  &amp;=  x-ROUNDER;
temp1  =  tmp1?temp1:temp2;
temp1  =  tmp2?temp1:x;</pre>
which keeps the total elapsed machine time under control and does wonders for the megaflops rating of the code. If the number of stages in the pipeline times the number of adders is at least four, there is no incentive to try integer comparison.<P>
<h4><FONT COLOR="#000080"><A name="0024_0010">Conclusion<A name="0024_0010"></FONT></h4></P>
Considering the amount I have had to say on a seemingly simple topic, one can sympathize with C programmers who avoid grappling with floating point. I have a little less sympathy with compiler and run time library writers who compound the programmer's problems by exceeding the legal limits of optimization or just don't think about the reliability issues.<P>
Normal precautions in the use of floating point may help avoid problems with compilers that take shortcuts and deviate from the rounding behavior required by the language and hardware standards. Most such situations are recognizable and may be taken care of by explicit specification of rounding. Similar situations exist where code cannot be expected to be reliably portable without such precautions, even among compilers that adhere to the language standard. In C, a problem is presented by the lack of a portable way to specify rounding without truncating to <I>int.</I><P>
<h4>References</FONT></h4></P>
Ritchie, Dennis M., The C Programming Language &#151; Reference Manual, Bell Laboratories, Murray Hill, N.J. 1978.<P>
Harbison, S. P., Steele, G. L. C: A Reference Manual, Second Edition, Prentice-Hall 1987.<P>
Prince, T.C., "Efficient Matrix Coding in C," The C Users Journal, May 1989, p. 59.<P>
</BLOCKQUOTE>
</BODY>
</HTML>
