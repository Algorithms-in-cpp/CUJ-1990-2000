


<HTML>
<HEAD>

<TITLE>May 1991/We Have Mail</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocmay.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Departments</FONT></H2>

<hr><h2 align="center"><font color="#800000">We Have Mail<A name="0123_009C"><A name="0123_009C"></font></h2>
<hr>
<BLOCKQUOTE>
<A name="0123_0000"><A name="0123_0000">Dear Mr. Plauger,<P>
I have just read Mr. Kolias' letter in the December issue concerning Borland's BGI stroked fonts. There is a free font editor available directly from Borland. You can ask Borland to send you the "BGI Developer's Disk" if you are a registered user of one of Borland's compilers. You need to specify the registration number. The German subsidiary of Borland, Borland GmbH, has shipped these disk free of charge. Maybe Borland America has a different policy, but I don't think so. The disk contains a font editor, some new fonts, some new drivers (including VGA 256 color and Hercules), and some example programs. If you have problems getting the disk try:<P>
Borland GmbH<br>
Lindwurmstrasse 88<br>
W - 8000 Muenchen 2<br>
GERMANY<br>
Tel. +49 89 720100<P>
I can be reached using <I>wilhelms @uniaug.de</I> or<P>
Gerhard Wilhelms<br>
Universitaet Augsburg<br>
Lehrstuhl fuer Informatik I<br>
Universitaetsstr. 2<br>
W - 8900 Augsburg<br>
GERMANY<P>
PS: Thanks for your fine magazine!<P>
<I>Thank you &#151; pjp</I><P>
Dear CUJ,<P>
I am starting to wonder if I made a mistake when I recently renewed my subscription to this magazine. It is disheartening to see how casually authors attribute errors to compiler bugs. In each issue there are at least two or three problems which are dismissed as compiler bugs which, with a minimum amount of research, can be shown to be user errors. I would have thought that the editorial staff would eliminate such unfounded remarks, or at least require the author to provide some evidence of the supposed bug. It worries me when I read an article full of phrases such as "the compiler seems to..." and "presumably."<P>
Instead, we are subjected to articles such as Stuart Baird's "Using Large Arrays in Turbo C" peppered with mistaken compiler bug claims and erroneous suppositions. I use Turbo C daily and have done some difficult UNIX-to-MS-DOS ports. My main difficulties in such ports are the UNIX assumptions about arrays and memory architectures, so I have encountered just about every segmentation related problem at least once.<P>
Given the declaration<P>
<pre>int a[20000], b[20000];</pre>
there is no memory model which will handle this as we might assume it would. In Turbo C, static data is limited to a total of 64K except in the huge memory model, where each source file may contain up to 64K of static data. Since this source file has more than 64K of static data, it cannot be compiled correctly by Turbo C. This is documented behavior, so I'd hardly call it a bug. A major inconvenience brought about by the CPU, but definitely not a compiler bug.<P>
Incidentally, this could be done by creating two source files, each containing one of the declarations above, and compiling under the huge memory model. Each source file has less than 64K of static data, but the total amount of static data is well over 64K. If the declaration above is made outside the scope of a function, or inside a function but declared as static, the compiler will generate an error message, "Too much global data defined in file..."; however, if the same declaration appears within a function and is not static, no error is generated because the data is no longer allocated in the global data area but on the stack at each function invocation. Such a declaration should generate a similar error at compile time, since stack space is always limited to 64K. It cannot truly be considered a bug as there are no guarantees about available stack space. A deeply recursive function with 1 byte of local data can run out of stack space. I think, however, Borland did overlook the cases which could be caught a compile time.<P>
Another mistaken claim is that in the following code<P>
<pre>long i;
unsigned long count = l00000ul;
char far *a;
a = (char far *) farmalloc(count);
for(i = 0L; i      count; i++) a[i];</pre>
uses <I>long</I> arithmetic in the array subscript operator. The array subscript operator takes two operands. One is of course a pointer expression; the other is an integral expression. The documentation says "When an integral type is added to or subtracted from a 'pointer to type,' the result is also of type 'pointer to type,'" It also says that the type of the integral expression is <I>unsigned int</I>, unless the pointer is a <I>huge</I> pointer, in which case the integral expression is <I>signed long</I>. Of course, if the pointer is <I>huge,</I> normalization also occurs.<P>
Since <I>a</I> is not a <I>huge</I> pointer, <I>i</I> is converted to <I>unsigned int</I>, added to the <I>far</I> pointer, and the result is a <I>far</I> pointer which points to someplace in the same 64K segment as a does. Had <I>a</I> been declared to be a <I>huge</I> pointer, the example should work as expected, as the author points out. Incidentally, unless <I>a</I> is a <I>huge</I> pointer, the code above will produce a warning message, "Conversion may lose significant digit" if warnings are enabled.<P>
Turbo C can catch many of the subtler problems in user code if all warnings are enabled. This is precisely why I always compile with warnings fully enabled, and correct and/or confirm all code which generates warnings.<P>
As for the "small bug" in <I>printf</I> in the huge model, my tests failed to produce anything but correct results. I am using Turbo C++ in the C compilation mode. Earlier versions might well produce different results, but I do not recall any such problem, and I have used Turbo C 1.5 and 2.0 before switching to Turbo C++.<P>
While the article contained useful information and caveats, these repeated casual, unsupported references to supposed compiler and library bugs soured it for me. Mr. Baird is not alone, nor can he truly be faulted. I'd reword an old saying, if I had a dime for every time someone 'discovered' a compiler bug that was actually bad user code, I would be a very rich man.<P>
Michael S. Percy<br>
420 Gallo Way<br>
Seneca, SC 29678<br>
<I>grimlok@hubcap.clemson.edu</I><br>
(803) 885-1132<P>
<I>Where were you when I was selling C compilers? If half my customers were as tolerant as you, I'd have a lot more dimes in my pocket today. I remember telling users where the manual let the compiler off the hook. It did no good. A user expecting an error message wants an error message. As far as the unhappy user is concerned, a missing error message is a compiler bug.</I><P>
<I>I too use C part of Turbo C++. It is a fine product. It also contains bugs. That inclines me to believe an author who states that bizarre behavior is "probably" a compiler bug. I too am wary of an offhand tendency to blame the tools for what may well be a program bug. That inclines me to demand supporting arguments. In the end, I simply accept the fact that one man's bug is another man's feature. &#151; pjp</I><P>
Dear Mr Plauger,<P>
I am concerned about the article "Automated Software Testing" in the February 1991 C <I>Users Journal</I>. My concern is not with Mr. McLaughlin but with the review process that the <I>C Users Journal</I> uses for its articles. Mr. McLaughlin had the difficult job of reducing many years of testing experience into a three-page article.<P>
My first concern is about the third paragraph in the section "Testing All the Code:" (p 105):<P>
"... The only way to ensure that code does not break down in an unexpected manner is to use a profiler, [...], to ensure that every line is being exercised, that all calls to all routines are used. If this is not done then a program that passes a test plan may break down in the field because a subroutine was not tested under all conditions."<P>
Exercising every line of code does not mean that the code has been tested under all conditions. Example: I can exercise every line of Listing 1
on page 107 with the letter <I>d</I>. Further, there are other ways to help ensure that the code does not break down in unexpected ways; for example: code inspections or code reviews. Both code reviews/inspections and testing need to be used to minimize errors in software.<P>
My second concern involves the last paragraph in summary (p 107):<P>
"In this sample testing program more than 1,000 tests of the program were run."<P>
The range of printing characters, defined by <I>isprint</I>, is from Ox20 to Ox7E: a total of 95 characters. Since the test cases are generated at random, I do not know if each character has been tested at least 10 times, or the letter <I>b</I> has been tested 1,000 times. Running the test several times does not allow me to know that I have covered all of the possible printing characters. Can I have confidence in the testing process if I don't know what has been tested?<P>
As <I>The C Users Journal</I> continues to publish more articles on software development practices, I feel you owe it to your readers (and their customers) to be both correct and sound in the practices that are shown in your articles.<P>
Sincerely,<P>
Calvin Hertel<br>
249 126th Ave. NW<br>
Coon Rapids, MN 55433<P>
<I>Both of your examples are valid, but I think you're being a bit harsh. It is indeed an overstatement to say that using a profiler is "the only way" to ensure code quality. It is a necessary component in adequate testing, but it is not sufficient. Still, the other methods you cited are simply alternate ways to ensure that all corners of the code get reviewed.</I><P>
<I>Testing a program by generating random test cases does raise the confidence level. You can't always sniff out those chunks that have a finite set of inputs and test them exhaustively. Even if you do, you raise the prospect that an error in these specialized tests will overlook one or more inputs. Just don't assume that random testing raises confidence in proportion to the number of inputs generated.</I><P>
<I>I am glad you appreciate the difficulty of reviewing so much material in a small amount of space. It is tough on the reviewer, the editor, and the stuff getting reviewed. Nevertheless, reviews provide an important service to readers. We can only try to keep doing this tough job as fairly as possible. In that light, I accept your remarks as constructive criticism. &#151; pjp</I><P>
Dear Sirs,<P>
I would like to make some comments concerning the questions of Mr. Ken Yerves about the location of specific data in an <I>.</I><I>EXE</I> file (<I>CUJ</I>, December 1990).<P>
Each <I>.</I><I>EXE</I> file has a specific file header with information which can help to determine the location of data in the <I>.</I><I>EXE</I> file. Part of this file header you can see in the following structure:<P>
<pre>struct EXE_HEADER
{
char MZ[2]; /* MZ-header for .EXE files */
unsigned last_sector; /*lenght of
   last used sector in file */
unsigned file_size; /* size of file,
   including header, in paragraphs */
unsigned reloc_count; /* number of
   relocation table items */
unsigned header_size; /* size of
   header in 16-byte paragraphs */
} exe_header;</pre>
After opening your <I>.</I><I>EXE</I> file in binary mode, you can read this header information:<P>
<pre>fread(&amp;exe_header, sizeof(exe_header),
   1, file_ptr);</pre>
Then you can position the file pointer. The following two macros can simplify the code (assuming the data you are looking for is a structure variable named <I>config_data)</I>:<P>
<pre>#define MK_FP(s,o) \
   ((voide far*)(((unsigned long)(s)&lt;&lt;16)
 | (o)))

#define POSITION \
   ((char huge *)&amp;config_data \
   - (char huge *)MK_FP(_psp, 0x100))

fseek(file_ptr, POSITION, SEEK_SET);</pre>
And there you are! Just another <I>fread</I> will get your data from your <I>.EXE</I> file without searching for a unique key:<P>
<pre>fread(&amp;config_data, sizeof(config_data),
   1, file_ptr);</pre>
After reconfiguring your application and before existing, you can write your <I>config_data</I> back to the <I>.EXE</I> file in the same manner.<P>
Yours sincerely,<P>
Michael Wiedmann<br>
Innsbrucker Str.<br>
35D-1000 Berlin 62<br>
Germany<P>
<I>Thanks. &#151; pjp</I><P>
Dear CUJ:<P>
I was interested in the article "A Login Shell for MS-DOS" by Leor Zolman in the February 1991 issue of <I>CUJ</I>. However, in implementing his code on my system, I ran into several problems for which I propose solutions, along with a few suggested imporvements.<P>
The first concerns his recommendation that the last line of the user's login script (i.e., batch file) specify the login command itself. The reason is to maintain security by returning the user to the LOGIN prompt instead of the DOS prompt. While the reason is sound, it's implementation is not. As given, the login script is executed using the <I>system()</I> function, which loads a copy of <I>COMMAND.COM</I> after the current invocation of <I>LOGIN</I>. When the last line of the script specifies 'login', <I>LOGIN</I> is invoked again, leaving the first copy of <I>LOGIN</I> "resident." One can easily see that each time the user executes a script file, memory will be quickly chewed up as each copy <I>LOGIN</I> loads after the previous one.<P>
One solution to this would be to execute the script file by loading <I>COMMAND.COM</I> as a child process so that it overlays the parent <I>LOGIN.EXE</I> at the same memory location. The code in <A href="list1.htm">Listing 1</a>
demonstrates.<P>
Instead of the common <I>login</I> on the last line of the script file, as Mr. Zolman suggests, I use the DOS command <I>exit</I>, which returns execution back to <I>AUTOEXEC.BAT</I> from where <I>LOGIN</I> was originally invoked on boot-up. However, instead of ending <I>AUTOEXEC.BAT</I> and returning to the DOS prompt, simply execute <I>LOGIN</I> from within an endless loop. This ensures that <I>LOGIN</I> will reload when the script file terminates. The following fragment from <I>AUTOEXEC.BAT</I> shows how:<P>
<pre>:NEXT
LOGIN
GOTO NEXT</pre>
to give yourself "superuser" status and the ability to get out of your own trap, write a special batch file (protected by a password) that loads <I>COMMAND.COM</I> again. This will put you back at the DOS prompt, which will give you full access. Typing <I>exit</I> at the DOS prompt will reload <I>LOGIN</I> again. (For a bit of additional security, change the attributes of all login script files to "read-only" and the password file to "hidden/read-only").<P>
In giving my adaptation of <I>LOGIN</I> a final polish, I deleted <I>zgetch(), zputs()</I>, and <I>zgets()</I>, and instead used the DOS Input String function as implemented by <I>cgets()</I> and <I>getpass()</I> in the Turbo C library. Using these functions lets you limit cursor travel to 8 bytes (the length of a file name) and provides "full" DOS editing capability. In addition, <I>getpass()</I> does not echo characters to the screen when a user types a password.<P>
I hope these comments will help others trying to implement <I>LOGIN</I> for themselves. My thanks for an enjoyable and useful magazine and to Mr. Zolman in particular for a great idea.<P>
Sincerely,<P>
Thomas Nelson<br>
5004 W. Mt. Hope Rd.<br>
Lansing, MI 48917<P>
Leor responds:<P>
<I>Thank you, Mr. Nelson. I feel pretty stupid to have overlooked the stacking effect of repeated login invocations! And your usage of execlp is also a clear improvement. In examining your code, I discovered another oversight on my part: when using the </I><I>cprintf()</I><I> function, a "newline" is displayed by specifying \r\n, not just \n as I had done throughout the original program.</I><P>
<I>By the way, I don't actually use the login program under DOS (since most of my programming is under Xenix these days), and I guess it shows from those goofs!</I><P>
<I>I'll now offer an improvement based on your login-loop concept. I would expand the loop at the end of </I><I>AUTOEXEC.BAT</I><I> to the following:</I><P>
<pre>:next
if not exist login.on goto :end
cls
login
goto :next
:end</pre>
<I>I would then create a dummy file in the root directory named </I><I>LOGIN.ON</I><I>. Its presence enables the login program. In order to disable login completely, I'd log in as the super-user (as per your suggestion, where the super-user batch file contains an explicit invocation of the command processor), and rename </I><I>LOGIN.ON</I><I> to </I><I>LOGIN.OFF</I><I>, then give the exit command. This will return control to the </I><I>AUTOEXEC.BAT</I><I> loop, whereupon the test condition will become true (</I><I>LOGIN.ON</I><I> not found) and </I><I>AUTOEXEC.BAT</I><I> will terminate completely, freeing up ALL available memory for the "super-user". To re-enable login, just rename the file back to </I><I>LOGIN.ON</I><I>. &#151; Leor Zolman</I><P>
Editor:<P>
The article "Complex Function Library," by Maynard A. Wright in your September 1990 issue is a prime example of misdirected programming efforts. In this article, Mr. Wright goes to great pains to find the most efficient way to program some complex transcendental functions via standard "textbook algorithms" when, in fact, his time might have been better spent in developing improved algorithms.<P>
In Table 1 of this article the author lists the sizes and run times of three functions for calculating the hyperbolic sine (<I>csinh</I>) when the argument is complex. The three functions differ in the way they handle arguments and return values. From this table we learn that, although the <I>.obj</I> and <I>.exe</I> sizes vary by only a few percent, the run times vary by ten percent, from the slowest to the fastest of the three functions.<P>
My point is that while some time can be saved through careful programming, often much more time can be saved by using a more efficient algorithm. Let me use the same function, <I>csinh,</I> as an example. Wright's method for <I>csinh</I> follows:<P>
<pre>#include &lt;math.h&gt;
. . .
double arg_real, arg_imag,
   sinh_real, sinh_imag;
. . .
sinh_real = cos(arg_imag) *
   sinh(arg_real);

sinh_imag = sin(arg_imag) *
   cosh(arg_real);</pre>
While the code above is mathematically correct, it is very inefficient. Both the trigonometric and hyperbolic functions are expensive to calculate. Essentially all of the calculation time is spent in the four library function calls. In much less time than it takes to write the code for the three variations for <I>csinh</I> in the article, the writer could have cut the execution time by 40 percent by noting that both <I>sinh</I> and <I>cosh</I> are combinations of <I>exp(arg_real)</I>. In fact, it is likely that the library routines for these functions actually call the library <I>exp</I> function. By writing the hyperbolic functions in terms of exponentials (after all, the hyperbolic functions are nothing but shorthand for combinations of exponentials) one can calculate <I>csinh</I> as follows:<P>
<pre>#include &lt;math.h&gt;
. . .
double arg_real, arg_imag, sinh_real,
   sinh_imag, a, b; ...
. . .
b = 1.0 / (a = exp(arg_real));
sinh_real = cos (arg_imag) * 0.5 *
   (a -  b);
sinh_imag = sin(arg_imag3 * 0.5 *
   (a + b);</pre>
The number of library calls is reduced from four to three, but one of the three is the exponential function, which, as was mentioned above, is probably called indirectly by both <I>sinh</I> and <I>cosh</I> in Wright's method. I coded and timed the two methods and found the time savings to be about 40% in favor of the latter. Similar savings can be had for most of the other functions in Wright's paper.<P>
Your journal serves an important function in educating C programmers, and I agree that programming style is important, especially for program maintenance. However, as the above example illustrates, It is almost always the case that a more efficient algorithm will save more computational time than the most effective programming techniques applied to an inefficient algorithm.<P>
Yours truly,<P>
Jim Sharpiro, Ph.D.<br>
Professional Software, Inc.<br>
2460 Hawthorne Ave.<br>
Boulder, CO 80302<P>
<I>Your point is well taken, but you overlook an equally important point. Math functions must return accurate results for all valid inputs. I believe that Mr. Wright's functions do so. Yours does not. For small values of </I>arg_real<I>, your computation of </I>sinh_real<I> loses most or all of its precision, thanks to the subtraction of nearly equal values. For a band of large values, your approach suffers an overflow even though the function value is defined.</I><P>
<I>Here is a case where you are better off suffering the performance penalty and letting carefully crafted library functions do the hard bits for you. &#151; pjp</I><P>

<h4><a href="../../../source/1991/may91/letters.zip">Get Article Source Code</a></h4>

</BLOCKQUOTE>
</BODY>
</HTML>
