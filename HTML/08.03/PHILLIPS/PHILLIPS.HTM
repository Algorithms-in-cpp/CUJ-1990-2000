


<HTML>
<HEAD>

<TITLE>March 1990/Belief Maintenance Using The Dempster-Shafer Theory Of Evidence</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocmar.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Artificial Intelligence</FONT></H2>

<hr><h2 align="center"><font color="#800000">Belief Maintenance Using The Dempster-Shafer Theory Of Evidence<A name="0080_0044"><A name="0080_0044"></font></h2><P>
<h3 align="center"><font color="#800000"><A name="0080_0000"><A name="0080_0000">Dwayne Phillips</font></h3><hr><blockquote><P>
<P><i><A name="0080_0000"><A name="0080_0000">The author works as a computer and electronics engineer with the U.S. Department of Defense and is a doctoral candidate in Electrical and Computer Engineering at Louisiana State University. His interests include computer vision, artificial intelligence, software engineering, and programming languages. He first used the Dempster Shafer theory of evidence in 1984 and uses it extensively in his PhD research into computer vision.</i></P><P>
An expert system makes a decision given an amount of evidence. Usually it must choose between several competing answers or hypotheses. The human expert keeps these answers in his mind while he thinks over the problem. He gathers evidence and shifts his thoughts from one answer to another. After gathering evidence, he chooses the most favorable answer. We all do this in our daily decisions, but we don't think about the process, and we certainly don't keep track of specific numbers in our head.<P>
An expert system needs a sub-system to pool evidence and reach decisions: a <I>belief maintenance system</I>. The belief maintenance system keeps track of the hypotheses and the degree of belief attributed to each hypothesis. When the expert system finishes gathering evidence, the belief maintenance system chooses the answer.<P>
In some expert systems a belief maintenance system is not necessary, because some expert systems make decisions based on a single, clear cut piece of evidence. For instance, suppose an expert system has the task of rolling up the windows in your car. The evidence is whether or not it is raining. The system would check the atmosphere and ask, "Is it raining?" If the answer were yes, it would roll up the windows.<P>
In other expert systems a belief maintenance system is essential. Suppose the expert system had to decide at 9.00 AM whether or not to roll up the windows at 3.00 PM. Now the question is tougher. Evidence would include the daily weather forecast, the wind speed and direction, the relative humidity, weather records from past years, forecasts from the Farmer's Almanac, satellite photographs, and other relevant sources. The expert system would pool all the evidence and arrive at an answer.<P>
Consider the nature of evidence. Some evidence is not reliable (the weatherman is wrong sometimes and right sometimes). Some evidence is uncertain (an intermittent atmospheric reading). Some is incomplete (the wind speed by itself does not tell us much). Some evidence is contradictory (the weatherman's forecast and the atmospheric conditions). Finally, some evidence is incorrect (a broken atmospheric sensor or a wrong weather forecast).<P>
The belief maintenance system must deal with these factors, taking the evidence, assigning a measure of belief to each hypothesis, and changing this belief as new evidence becomes available. The resulting decision must be the same regardless of the order in which the system gathers the evidence.<P>
The method of belief maintenance that most of us know is classical probability. The basic properties of this system are [Beyer]:<P>
<pre> A)     P(<FONT FACE="Symbol" SIZE=2>Æ</FONT>) = 0 (null set)</FONT></FONT>
 B)     P(<FONT FACE="Symbol" SIZE=2>Q</FONT>) = 1 (entire sample set)</FONT></FONT>
 C)     P(A) = 1 - P(A')
 D)     P(A<FONT FACE="Symbol" SIZE=2>È</FONT>B) = P(A) + P(B), if A and B are mutually exclusive</FONT></FONT>
 E)     P(A<FONT FACE="Symbol" SIZE=2>Ç</FONT>B) = P(A) * P(B), if A and B are mutually exclusive</FONT></FONT></pre>
Another belief maintenance system came from the MYCIN project (a pioneering medical expert system developed in the early seventies by Edward Shortliffe at Stanford.) MYCIN used a system of certainty factors to keep track of hypotheses. Shortliffe later dropped the certainty factor system for the Dempster-Shafer theory of evidence.<P>
The Dempster-Shafer (D-S) theory of evidence was created by Glen Shafer [Shafer, 1976] at Princeton. He built on earlier work performed by Arthur Dempster. The theory is a broad treatment of probabilities, and includes classical probability and Shortliffe's certainty factors as subsets.<P>
In the D-S theory of evidence, the set of all hypotheses that describes a situation is the frame of discernment. The letter <FONT FACE="Symbol" SIZE=2>Q</FONT> denotes the frame of discernment. The hypotheses in <FONT FACE="Symbol" SIZE=2>Q</FONT> must be mutually exclusive and exhaustive, meaning that they must cover all the possibilities and that the individual hypotheses cannot overlap.</FONT></FONT></FONT></FONT><P>
The D-S theory mirrors human resoning by narrowing its reasoning gradually as more evidence becomes available. Two properties of the D-S theory permit this process: the ability to assign belief to ignorance, and the ability to assign belief to subsets of hypotheses.<P>
An example provides the easiest way to understand these properties and how they differ from classical probability. Suppose we want to decide which of three persons in an office &#151; Adam, Bob, and Carol &#151; will come in early to turn on the lights and make coffee.<P>
In the D-S theory the set <FONT FACE="Symbol" SIZE=2>Q</FONT> = {<I>Adam or Bob or Carol</I>}. The sets {<I>Adam</I>}, {<I>Bob</I>}, and {<I>Carol</I>} are the mutually exclusive and exhaustive hypotheses. They are singletons. In the frame of discernment there are 2<SUP><FONT FACE="Symbol" SIZE=1>Q</FONT></SUP> or 8 possible interpretations. (<A href="fig1.htm">Figure 1</a>)
</FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT><P>
<A href="fig1.htm">Figure 1</a>
contains two special sets in {<I><FONT FACE="Symbol" SIZE=1>Æ</FONT></I>} and {<I>Adam, Bob, Carol</I>}. The first is the null set, which cannot hold any value. As later examples will show, the null set normalizes beliefs. The second special set is {<I>Adam or Bob or Carol</I>}, represented by <FONT FACE="Symbol" SIZE=2>Q</FONT>. Assigning belief to <FONT FACE="Symbol" SIZE=2>Q</FONT> does not help distinguish anything. Therefore, <FONT FACE="Symbol" SIZE=2>Q</FONT> represents ignorance.</FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT><P>
Representing ignorance is a key concept. Humans often give weight to the hypothesis "I don't know", which is not possible in classical probability. Assigning belief to "I don't know" allows us to delay a decision until more evidence becomes available. This mirrors the human tendency to procrastinate.<P>
Suppose that given a piece of evidence, we make the assertion shown in <A href="fig2.htm">Figure 2</a>.
 The D-S theory calls an assertion a <I>basic probability assignment</I>. The <I>M</I> in <A href="fig2.htm">Figure 2</a>
represents the <I>measure of belief</I>. The assertion of <A href="fig2.htm">Figure 2</a>
says that we believe Adam is the best choice with a weight of <I>0.6</I>. We'll give the other <I>0.4</I> of belief to <FONT FACE="Symbol" SIZE=2>Q</FONT> or "I don't know," thus allowing us to delay deciding on Adam.</FONT></FONT><P>
We cannot make this type of assertion in classical probability. The classical system's property of complements given earlier forces us to give Adam' <I>0.4</I> (the complement of Adam) if we give <I>0.6</I> to Adam. In this case <I>Adam'</I> = {<I>Bob or Carol</I>}. Notice the difference between <FONT FACE="Symbol" SIZE=2>Q</FONT> ={<I>Adam or Bob or Carol</I>} and <I>Adam'</I> ={<I>Bob or Carol</I>}. <I>Adam'</I> gives more belief to Bob and Carol than we want. <FONT FACE="Symbol" SIZE=2>Q</FONT> allows us to express a true "no comment" on the situation.</FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT><P>
Assigning belief to subsets in the D-S theory allows us to assign belief to a general concept instead of being too specific. Suppose in our example that the local police advise us that we should not have women coming to work early by themselves. We would make an assertion like that, shown in <A href="fig3.htm">Figure 3</a>.
 This assertion gives a weight of 0.7 to the subset {<I>Adam or Bob</I>} and a weight of <I>0.3</I> to ignorance or no comment.</FONT></FONT></FONT></FONT><P>
Classical probability does not permit a subset assertion. Recall that property D requires <I>P</I><I>(</I><I>Adam or Bob</I><I>)</I> = <I>P</I><I>(</I><I>Adam</I><I>)</I> + <I>P</I><I>(</I><I>Bob</I><I>)</I>. That property would force us to assign specific beliefs to <I>Adam</I> and to <I>Bob</I> individually. We do not want to be that specific. We want to procrastinate and think it over some more.</FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT><P>
Also, property C would make us assert the <I>0.3</I> to the complement of {<I>Adam or Bob</I>} which is {<I>Carol</I>}. We do not want to assert <I>0.3</I> to {<I>Carol</I>}. Assigning belief directly to {<I>Carol</I>} would contradict the evidence the police gave us.</FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT></FONT><P>
The D-S theory employs Dempster's rule of combination to combine two assertions. The mathematical formulas may be found in the references. They confuse the best of us, but they are simple when illustrated. <A href="fig4.htm">Figure 4</a>
shows how the two assertions combine.<P>
The table in <A href="fig4.htm">Figure 4</a>
is an <I>intersection tableau</I>. Which lists one assertion across the top and one down the side. Inside the tableau are the intersections of the sets in the rows and columns, with the products assigned to the intersections. The measures of belief inside the table sum to the final values given below the table.<P>
Notice how combination narrows the decision process. The single set {<I>Adam</I>} now has the highest belief. The subset {<I>Adam or Bob</I>} comes in second with no comment last.</FONT></FONT></FONT></FONT><P>
Now suppose that we require the first person in the office in the morning to bring up the computer system. Carol is an expert at this so we make the assertion shown in <A href="fig5.htm">Figure 5</a>.
 This attributes most of the belief to {<I>Carol</I>}. This new requirement or piece of evidence contradicts the previous evidence given by the police. That is the nature of evidence. Dempster's rule of combination allows us to combine the contradictory evidence and draw a logical conclusion.</FONT></FONT><P>
<A href="fig6.htm">Figure 6</a>
shows the combination of the result of <A href="fig4.htm">Figure 4</a>
and the assertion of <A href="fig5.htm">Figure 5</a>.
 Inside the intersection tableau is the null set. There is no intersection between the set {<I>Carol</I>} and the set {<I>Adam</I>} and there is also no intersection between the set {<I>Carol</I>} and the set {<I>Adam, Bob</I>}. The null set cannot hold any value. Therefore, it normalizes the beliefs of the other subsets. The sum of the beliefs of the other subsets is divided by one minus the belief in the null set. The beliefs of all the subsets sum to one. The bottom of <A href="fig6.htm">Figure 6</a>
shows this extra step.<P>
As a result, Carol is now the choice for coming in early in the morning. If she is unable to do so, then Adam is the logical replacement. If Adam is unavailable, then Bob comes in early.<P>
<h4><FONT COLOR="#000080"><A name="0080_0045">Implementation<A name="0080_0045"></FONT></h4></P>
The preceding examples show that no complex mathematics are involved in combining two assertions. Dempster's rule of combination uses simple addition, subtraction, multiplication, and division. The only tricky part is the intersections of the sets in the tableau.<P>
There are several ways to solve the intersection question. Since there are three singletons and 2<SUP>3</SUP> total interpretations, we'll represent the hypotheses with three bits as in <A href="fig7.htm">Figure 7</a>.
<P>
<A href="list2.htm">Listing 2</a>
shows the C function that combines two assertions. The inputs are two <I>belief vectors</I>, each holding an assertion. The belief vector is a one-dimensional array of floats. In our examples, the <I>LENGTH_OF_BELIEF_VECTOR</I> is eight because we have three singletons and 2<SUP>3</SUP>=8. The belief vector has a space, or slot, for each hypothesis, ordered as in <A href="fig7.htm">Figure 7</a>.
 The belief vector is awkward to initialize since we would like Adam in slot one, not slot four, and Carol in slot three, not slot one. Nevertheless, a uniform belief vector allows a very simple subroutine to combine the assertions.</FONT></FONT><P>
The first for loop initializes the sum_vector, the belief vector which holds the sums of the values found inside the intersection tableau. sum_vector holds the sums for later when normalization occurs.<P>
The <I>for</I><I> a</I> loop goes through the belief vectors, finds the intersections, and calculates the products. The two <I>if &gt; 0.0</I> statements reduce processing time by eliminating unnecessary multiplication by zero. The function uses the C bitwise AND operator <I>&amp;</I> to find the intersection of sets. Without the bitwise <I>AND</I>, the function would be much longer and much more complex.</FONT></FONT></FONT></FONT></FONT></FONT><P>
The last <I>for</I> loop performs the normalization. The values in <I>sum_vector</I> are divided by one minus the value assigned to the null set. The answer is stored in <I>vector1</I><I>.</I></FONT></FONT></FONT></FONT></FONT><P>
The <I>combine_using_dempsters_rule</I> function is the meat of the program written in Turbo C v1.5. I used this compiler because it had a few functions that made the user interface more pleasant. Except for those functions, there is nothing in the program that is machine, compiler, or operating system specific.</FONT></FONT><P>
One important note about implementing Dempster's rule of combination. The number of calculations depends on 2<SUP><FONT FACE="Symbol" SIZE=1>Q</FONT></SUP>. In our example there were eight hypotheses. Alternatively, 200 single hypotheses would produce 2<SUP>200</SUP> subsets, 2<SUP>200</SUP> slots in the belief vector, and 2<SUP>200</SUP> floating point calculations. This gets out of hand rather quickly. Several of the references [Gordon, Shortliffe 1985] [Shafer 1985] [Shafer 1987] deal exclusively with this topic. The discussion and proposed solutions are beyond the scope of this article.</FONT></FONT></FONT></FONT><P>
<h4><FONT COLOR="#000080"><A name="0080_0046">Conclusion<A name="0080_0046"></FONT></h4></P>
The Dempster-Shafer theory of evidence is one method that an expert system may use to keep score on competing hypotheses while it gathers evidence and draws a logical conclusion. It is more general and capable than the classical probability with which most of us are familiar. It is easy to implement and executes quickly as long as the number of hypotheses is manageable. I suggest you try it on your next expert system or AI-related project.<P>
<h4><FONT COLOR="#000080"><A name="0080_0047">References<A name="0080_0047"></FONT></h4></P>
Beyer, William H., <I>CRC Standard Mathematical Tables</I>, 26th edition, CRC Press, 1983, pp. 503-559.<P>
Gordon, Jean, Edward H. Shortliffe, "The Dempster-Shafer, Theory of Evidence," pp. 272-292 of Shortliffe, Edward H., Bruce G. Buchanan, eds., <I>Rule Based Expert Systems</I>, Addison Wesley Publishing Company, 1984.<P>
Gordon, Jean, Edward H. Shortliffe, "A Method for Managing Evidential Reasoning in a Hierarchical Hypothesis Space," <I>Artificial Intelligence</I>, Vol. 26, No. 3, July 1985, pp. 323-357.<P>
Shortliffe, Edward H., Bruce G. Buchanan, eds., <I>Rule Based Expert Systems</I>, Addison Wesley Publishing Company, 1984.<P>
Shafer, Glen, <I>A Mathematical Theory of Evidence</I>, Princeton University Press, 1976.<P>
Shafer, Glen, "Hierarchical Evidence," The Second Conference on Artificial Intelligence Applications, IEEE Press, December 1985, pp. 16-21.<P>
Shafer, Glen, Roger Logan, "Implementing Dempster's Rule for Hierarchical Evidence," <I>Artificial Intelligence</I>, Vol. 33, No. 3, November 1987, pp. 271-298.<P>

<h4><a href="../../../source/1990/mar90/phillips.zip">Get Article Source Code</a></h4>

</BLOCKQUOTE>
</BODY>
</HTML>
