

<HTML>
<HEAD>

<TITLE>June 1994/float-Precision Math Library</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocjun.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Scientific/Numerical Applications</FONT></H2>

<hr><h2 align="center"><font color="#800000">float-Precision Math Library<A name="0114_0088"><A name="0114_0088"></font></h2><P>
<h3 align="center"><font color="#800000"><A name="0114_0000"><A name="0114_0000">Tim Prince</font></h3><hr><blockquote><P>
<P><i><A name="0114_0000"><A name="0114_0000">Tim Prince has a B.A. in physics from Harvard and a Ph.D. in mechanical engineering from the University of Cincinnati. He has 25 years of experience in aerodynamic design and computational analysis. He can be reached at Box 16007, San Diego CA 92176.</i></P><P>
<h4><FONT COLOR="#000080"><A name="0114_0089">Introduction<A name="0114_0089"></FONT></h4></P>
C compilers traditionally have had available a library of the usual math functions, in <I>double</I> precision only, defined in the header <I>&lt;math. h&gt;</I>. One of the aims of the ANSI standardization was to make the choice between <I>float</I> (single precision) and <I>double</I> (double precision) available in a portable way.<P>
Standard C reserves a set of names for standard math library functions in <I>float</I> precision, although it doesn't require that these functions should be present in the library. (Just add <I>f </I>to the conventional name, so <I>double sqrt(double)</I> becomes <I>float sqrtf(float)</I>.) In order to use these functions in a portable way, source code should be provided for those functions which an application will use.<P>
P.J. Plauger's book, <I>The Standard C Library</I>, includes all required <I>double</I> math functions. It has raised the standard of quality for these functions. Functions which meet the accuracy standards and take care of the many possible error conditions can be difficult to understand. In <I>float</I> precision, several of the functions can be written in a more straightforward way without losing accuracy.<P>
The required <I>double</I> functions will work without any problems when invoked with <I>float</I> arguments, even when not within the scope of the prototypes declared in the Standard C headers. So the only reason for using special <I>float</I> versions is to gain speed or compactness of compiled code. Since <I>float</I> precision is marginal in many applications, these functions should retain as much accuracy as possible. Ideally they should provide correctly rounded results in <I>float</I> precision for all representable results.<P>
The <I>errno</I> values <I>ERANGE</I> and <I>EDOM</I> need not be stored except in those cases where the C Standard requires them. The following functions are written with the expectation that IEEE special codes such as Infinity and Not a Number will be propagated according to normal rules, so that time need be spent on them only in those cases where bit pattern manipulations prevent them from propagating normally.<P>
I hope that these <I>float</I> precision functions will provide an opportunity for improved understanding of math functions, although I have not sacrificed efficiency for readability. These functions have been written with a view to efficiency on typical Reduced Instruction Set (RISC) computers, particularly those with some pipelining of floating-point instructions, such as HP PA-RISC.<P>
<h4><FONT COLOR="#000080"><A name="0114_008A">Inline Functions<A name="0114_008A"></FONT></h4></P>
Several of the math functions are best expressed using inline code. In particular, <I>fabs</I> and <I>sqrt</I> are supported as single instructions on most modern computers, in both <I>float</I> and <I>double</I> versions. Many C compilers do not employ these instructions, so an inefficient function call has to be made. The Gnu gcc compiler has a satisfactory way of dealing with simple inline functions, allowing optimum code sequences for the given architecture to be prescribed in <I>&lt;math. h&gt;</I>.<P>
A version of <I>sqrtf</I> is discussed below, and shortcut macros for <I>modff</I> and <I>fmodf</I> are included for use in a modified <I>&lt;math.h&gt;</I> (<A href="list1.htm">Listing 1</a>)
. In my experience, none of these are very useful, but they are included for completeness, including fulfillment of the test suites.<P>
Rather than deal with subnormals and the like in <I>frexpf</I> and <I>ldexpf</I> functions, it is more efficient on IEEE-compliant architectures to promote to <I>double</I> as a way of dealing with numbers out side the range of magnitudes<I> FLT_MIN to FLT_MAX</I>. The portability and modularity of the primary math functions could be improved by using these functions, but I chose to improve efficiency, as the prevailing IEEE floating-point architectures provide reasonable portability anyway.<P>
<h4><FONT COLOR="#000080"><A name="0114_008B">Internal Header "<B><I>xmath.h</I></B>"<A name="0114_008B"></FONT></h4></P>
In order to simplify things a little at the expense of VAX compatibility, I have modified the internal header file <I>"xmath. h"</I> introduced by Plauger for writing all the math functions. It now uses <I>long</I> instead of <I>short int</I> variables to perform bit manipulations on floating-point numbers. The following changes are made in his "parameter" header "<I>yvals. h</I>", which is included by "<I>xmath. h</I>":<P>
<pre>#define _DO 0    /* big-endian order */
#define _DOFF 20    /* IEEE double union of longs */
#define _DSIGN 0x80000000    /* sign bit mask (avoid) */
#define _DBIAS 1022    /* exponent field of 0.5 */
#define _FOFF (FLT_MANT_DIG - 1)    /* exponent offset */</pre>
and these changes or additions to <I>"xmath. h"</I>:<P>
<pre>typedef union { /* VAX compatibility discarded */
   double _D;
    unsigned long _W[2];
   } _Dvar;
#define _DMASK (~_DFRAC)
#define _DMAX ((1&lt;&lt;(31-_DOFF))-1) /* 15 for short */
#define _DNAN (0x80000000|_DMAX&lt;&lt;_DOFF \
    |1&lt;&lt;(_DOFF-1))/* avoid this */
#define DSIGN(x) (((unsigned long *)&amp;(x)) \
   [_D0]&amp;_DSIGN) /* avoid this */
/* modified _Dtest (avoid) */
#define _Dtest(x) ((x)&gt;DBL_MAX||(x)&lt;-DBL_MAX \
   ?INF:(x)!=(x)?NAN:(x)!=0?FINITE:0)</pre>
I feel that more efficient and readable code generally results from writing out the special cases which must be treated, rather than generating Plauger's special codes <I>INF, NAN, FINITE</I>, etc. In most situations, negative values can be detected more efficiently in the obvious portable way by comparison with 0 than by testing the sign bit of a union.<P>
<h4><FONT COLOR="#000080"><A name="0114_008C"><I>atan2f<A name="0114_008C"></I></FONT></h4></P>
<A href="list2.htm">Listing 2</a>
shows the code for <I>atan2f. atanf</I> is treated as a special case of <I>atan2f</I>, as this seldom costs any time, and avoids duplication of code. The programmer can save time and accuracy by using <I>atan2f</I> rather than performing a division before calling <I>atanf</I>.<P>
In the absence of an inlined <I>fabsf</I> implementation, it is faster to copy the arguments to a union of <I>float</I> and <I>long</I> before comparing their magnitudes. Otherwise there is not sufficient reason to use such ugly code. This code works for any machine which uses ones complement floating point and has the same sign bits and bit order in <I>float</I> and <I>long</I> words.<P>
Under UNIX, <I>atan2(0.</I><I>,</I><I>0.)</I> produces zero, according to well established tradition, partly as a result of the limitations of pre-IEEE floating-point architectures. The IEEE standard clearly dictates the result NaN, which follows without any special effort from this code.<P>
The transformation<P>
<pre><I>atan2</I>(<I>x,y</I>) = <FONT FACE="Symbol" SIZE=2>p</FONT><I>/2</I> - <I>atan2</I>(y,x)</pre>
(in the first quadrant) is used to reduce the range to the first or last octant. In <I>float</I> precision, no further reductions are necessary, as it is possible to use a single Chebyshev economized polynomial over the range -<FONT FACE="Symbol" SIZE=2>p</FONT>/2 to <FONT FACE="Symbol" SIZE=2>p</FONT>/2. This is a bit remarkable, as the corresponding Taylor series diverges at the ends of this interval. The gain of simplicity more than compensates for the number of terms.<P>
When evaluating polynomials that have a leading first-order term with coefficient 1, there are two reasons for separating this term from a Horner evaluation polynomial, adding the first-order term last. First, if the terms in the series decrease in magnitude with increasing order, all the roundoff errors can be kept out beyond the least significant bit of the result. The final addition produces a result which is usually correctly rounded. Second, the last multiplication can be computed in parallel with the higher-order terms, giving the effect of one less operation on parallel or pipeline architectures.<P>
Rather than evaluating one long polynomial by Horner's method, I evaluate terms in pairs to take advantage of pipelined architectures, or those that can multiply and add in parallel. This involves one or two additional multiplications, but gains far more speed on a pipeline or parallel processor than it loses on a purely sequential processor. The additional roundoff errors are submerged into insignificance by performing the final operations in the order described in the preceding paragraph.<P>
When the operands have been inverted, the final subtraction from <FONT FACE="Symbol" SIZE=2>p</FONT>/2 should be performed in <I>double</I> precision to avoid losing a bit of accuracy. On typical Sun or HP systems, the compiler will generate efficient code for this sequence of operations only if the constant is fetched first, as shown.<P>
One of the comparisons at the end is performed by integer arithmetic, since we already made integer copies of the arguments to get around the lack of an inline <I>fabsf</I>. Integer comparison is much faster on most processors, and there are no other operations to pipeline with the comparison in this position.<P>
Rather than use entirely different approximations for <I>acosf</I> and <I>asinf</I>, these functions invoke <I>sqrt</I> and <I>atan2f</I> either indirectly or by macro expansion. If an inline <I>sqrt</I> is available, this method is as efficient as any, as well as being consistent and economizing code. The alternate choice<P>
<pre><I>acosf</I>(<I>x</I>) = <FONT FACE="Symbol" SIZE=2>p</FONT>/2 - <I>asinf</I>(<I>x</I>)</pre>
can cost some accuracy.<P>
<h4><FONT COLOR="#000080"><A name="0114_008D"><I>cosf, sinf,</I><B> and</B><I> tanf</I><A name="0114_008D"></FONT></h4></P>
All three trigonometric functions can be calculated at once by the hidden function <I>_Sinf</I>. Using the half-angle tangent formulae, most of the calculations are shared by these functions, as shown in <A href="list3.htm">Listing 3</a>.
<P>
We start by scaling the argument by 1/(2<FONT FACE="Symbol" SIZE=2>p</FONT>), after promoting to <I>double</I>. This is the first of several points where different methods would be required to preserve accuracy if we were not using extra precision. Many lines of code are required to subtract off the nearest integral part of the scaled argument, allowing for various <I>FLT_ROUNDS</I> cases. If your system conforms to IEEE standard, and doesn't allow for changing rounding mode at run time, the compiler should be able to eliminate all but the <I>FLT_ROUNDS == 1</I> case.<P>
The formulae are based on the calculation of <I>tan</I>(<I>x/2</I>) by a rational approximation. The scaling by 0.5 and the original scaling by 1/(2<FONT FACE="Symbol" SIZE=2>p</FONT>) are accounted for in the coefficients, which are scaled to make the smallest coefficient one.<P>
After elimination of dead code, there are no conditional branches other than the one used to implement a "copysign" operation, and the code pipelines well. HP PA_RISC parallel multiply and add instructions are used effectively, particularly if the order of source code operations is optimized experimentally.<P>
The three values <I>cosf</I><I>,</I><I> sinf</I><I>,</I> and <I>tanf</I> are returned in a struct. If a masking macro is used to select the desired component, an optimizing compiler should be able to eliminate redundant calls to _<I>Sinf</I><I>,</I> if you can convince the compiler that there are no side effects. Otherwise, you have to call _<I>Sinf</I> explicitly and keep the desired values to gain the potential benefit when you need two or more of the results for the same argument.<P>
With a good optimizing compiler, or a gcc-like macro facility, you might choose to return the three values from which the various trig functions are calculated, using a macro to perform the final division. This would save some time where not all the return values are used, and might allow the division to run in parallel with code from the calling function. Calculation of the functions secant, cosecant, and cotangent could be optimized.<P>
Certain compilers generate poor code when returning float values in a struct, storing first into a temporary memory location and then copying from there to the struct return location. Better results seem to be achieved by having the function return a pointer to an array of <I>float</I> values, but this clearly would require use of writable static storage in the library, an unsavory practice.<P>
This source code can be used in a multiple-copy function, typically to generate <I>sinf</I> and <I>cosf</I> of two arguments at once. A little time is saved by pipelining the first divide instruction with the calculation of the second group of data. On more deeply pipelined architectures, multiple-copy versions or inline compilation could provide a significant benefit. One of the means for facilitating in-line compilation is an extended macro facility such as gcc provides. Like the FORTRAN inline statement functions, this facility can deal with side effects and reduce the optimization workload on the compiler.<P>
<h4><FONT COLOR="#000080"><A name="0114_008E"><I>coshf<A name="0114_008E"></I></FONT></h4></P>
Hyperbolic cosine could be written as a macro in terms of <I>expf</I>, but not quite in the obvious way. The macro would read<P>
<pre>#define coshf(x) (expf(fabsf(x)-(float)M_LN2) \
   +.25f/expf(fabsf(x)- (float)M_LN2))</pre>
The reason for subtracting <I>log</I>(2) from the argument is to avoid premature overflow in <I>expf</I>. But this costs accuracy for small arguments.<P>
This range vs. accuracy problem is solved by using a hidden function _<I>Expf</I> (<A href="list4.htm">Listing 4</a>)
 to calculate <I>expf(x)</I> in <I>double</I> precision, to increase accuracy and range beyond the limits of <I>float</I> precision, but not to full <I>double</I> range or accuracy.<P>
The visible function of <A href="list5.htm">Listing 5</a>
checks for NaN arguments because we don't always want to do this test in _<I>Expf</I>. The expression <I>x != x</I> tests for a NaN on IEEE-compliant hardware. (If the compiler doesn't optimize it away, that is. If the compiler is generating code for non-IEEE hardware, it may as well consider this as dead code.) _<I>Expf</I> returns the exponent <I>ceil(log2(xx))</I>, which allows us to determine whether there will be overflow or underflow and set <I>errno</I> accordingly, without having to test the final result.<P>
The factors 0.5 are distributed so that the multiplication and division can be parallelized. We write the division first because we expect it to take longer and want it to start in the pipeline first.<P>
<h4><FONT COLOR="#000080"><A name="0114_008F"><I>expf<A name="0114_008F"></I></FONT></h4></P>
The visible <I>expf</I> function (<A href="list6.htm">Listing 6</a>)
 is consistent with <I>coshf</I>. It too could be replaced by a macro, if we were not concerned with error reporting.<P>
<I>_Expf</I> passes data through a <I>double</I> pointer, because the function result is used to report the exponent field, for use in determining whether overflow or underflow will occur. <I>double</I> data is used because the typical use of<I>_Expf</I> is within the <I>powf</I> function, where the argument is obtained by arithmetic on the result of a <I>log</I> function. As <I>exp</I> strips a number of high-order bits off the mantissa of the argument and delivers them in the exponent of the result, it would be useful to have at least 32 bits of precision in the argument, in accordance with IEEE extended single precision. The <I>_Logf</I> function supplies this precision.<P>
Since the argument datum has been forced to memory already, the sign bit can be retrieved quickly and used as an index into an array of rounding constants. The nearest integer multiple of <I>log</I>(2) is subtracted from the argument, with the code written to maximize the opportunity for a peephole optimizer to shortcut the cast from <I>double</I> to <I>int</I> to <I>double</I>. Ideally, we would do something to protect against overflow in the cast to <I>int</I> in case the argument is too large, but it is unlikely to overflow an <I>int</I> if it originated as a <I>logf</I> result, with maximum possible value <I>log(FLT_MAX)</I>.<P>
The mantissa of the result is calculated by a simple rational approximation, with no precautions needed for accuracy other than carrying out beyond <I>float</I> precision. The exponent field is limited so that it is within the range of a valid <I>double</I> but well outside the range of finite <I>float</I> values. This limiting is done by integer instructions, which can run parallel with the floating-point operations.<P>
The rational polynomial has four terms each in the numerator and denominator. Because of the symmetry<P>
<pre><I>exp</I>(<I>x</I>) = -<I>exp</I>(-<I>x</I>)</pre>
the numerator and denominator are the same except for a change of sign in the odd-order terms.<P>
Scaling the coefficients in the rational polynomial so that the smallest coefficient is one allows evaluation in the least number of operations. If the polynomial were evaluated in <I>float</I> precision, it would be necessary to scale so that the first-order coefficient becomes one, allowing a <I>+=</I><I> </I>operation to conserve accuracy. Accuracy still would be degraded by the final division. For this reason, published versions of <I>exp</I> manipulate the formula so that the final operation becomes an add of 0.5. This takes longer on a pipelined processor, eating up the time we might have saved by not casting from <I>float</I> to <I>double</I> and back.<P>
The exponent scaling in <I>_Logf</I> and <I>_Expf</I> can use shortcuts. The sign is required to be positive and there is no need to deal directly with overflow or underflow with the range of <I>double</I> being much wider than <I>float</I>.<P>
Positive and negative arguments are treated the same. When writing source code, you should try not to divide by <I>expf</I><I>,</I> as a faster and sometimes more accurate result can be obtained by changing the sign of the argument and multiplying.<P>
<h4><FONT COLOR="#000080"><A name="0114_0090"><I>logf</I><B> and </B><I>log10f</I><A name="0114_0090"></FONT></h4></P>
The functions of <A href="list7.htm">Listing 7</a>
invoke, normally by a masking macro, the function _<I>Logf</I>, which has a <I>double</I> argument and return value, as in traditional C. We can bypass the issue of subnormals by casting immediately to <I>double.</I> As I mentioned above, we need an extra three digits of precision in the result in certain contexts, just to avoid throwing away information for large or small arguments.<P>
We start out flipping back and forth between checking for error cases and working on the expected case of a finite positive argument. This is done for the benefit of architectures that have a long pipeline for floating-point comparison. No harm is done by calculating as if everything were normal while testing for exceptions.<P>
The first complicated operation extracts the exponent field, determines what adjustment needs to be made to scale the argument as close as possible to 1.0, and performs the scaling. If the fractional part of the argument is less than <I>sqrt</I>(0.5), the scale factor is incremented. By masking off the fractional part and performing an integer compare, we avoid having to generate a floating-point version of the fractional part. The integer comparison uses only the high-order 21 bits of precision, as the boundary where the scaling changes doesn't need to be exact.<P>
We hasten to get the divide into the pipeline before the final two comparisons for exceptions. Even in this situation, where it seems not to save any operations, calculating the <I>2*r</I> term separately improves parallelism.<P>
<h4><FONT COLOR="#000080"><A name="0114_0091"><I>powf<A name="0114_0091"></I></FONT></h4></P>
In <I>float</I> precision, we slip by with a relatively simple <I>powf</I> function (<A href="list8.htm">Listing 8</a>)
. Since the C language definition allows for zero and negative numbers to be raised to integral powers by <I>powf</I>, we have to separate out these cases before taking a <I>log</I>. On the assumption that integral powers will usually be small enough that raising to a power by multiplication will be faster than <I>logf</I> and <I>expf</I>, the same method is used for all integral powers. The number of operations for raising to a power this way increases only logarithmically with the exponent.<P>
If the exponent is not integral, we calculate the result in a straightforward way, using <I>_Logf</I> and <I>_Expf</I> so that accuracy is preserved by keeping all the intermediate calculations in <I>double</I> precision. <I>ERANGE</I> is reported if the result is calculated to be Infinity or zero. The possibility of gradual underflow is assumed, so that non-zero numbers may be as small as<P>
<pre><I>2^(FLT_MIN_EXP-FLT_MANT_DIG)</I></pre>
A function similar to <I>_Expf</I> using base 2 instead of base <I>e</I> may run slightly faster when used along with a <I>_Log</I> base 2.<P>
For accuracy and efficiency, it is best to avoid negative integral powers, because these require <I>powf</I> to invert the result. As with <I>expf</I>, for non-integral powers, division by <I>powf</I> should be avoided by changing the sign of the second argument. While it may seem annoying to have the optimum usage of <I>powf</I> change between integral and non-integral negative powers, <I>powf</I> should be avoided entirely for the usual small integral powers which are known at compile time.<P>
<h4><FONT COLOR="#000080"><A name="0114_0092"><I>sinhf<A name="0114_0092"></I></FONT></h4></P>
<I>sinhf</I> (<A href="list9.htm">Listing 9</a>)
 is much like <I>coshf</I>, with one additional hurdle. If we try the obvious<P>
<pre>0.5 * (exp(x) - exp(-x))</pre>
even in full <I>double</I> precision, accuracy is poor for small arguments because the leading terms cancel out after swamping the important ones.<P>
One solution found in the literature and even in computer architectures such as the Intel 8087 is a function which calculates <I>(exp(x) - 1).</I> The applications for such a function can be done faster or better some other way. Satisfactory rational polynomials for <I>sinh</I> exist, but there is no compensation for the extra time taken by division. A perfectly good Chebyshev economized polynomial can be found for the range of arguments where the <I>exp</I> approach doesn't work.<P>
<h4><FONT COLOR="#000080"><A name="0114_0093"><I>sqrtf<A name="0114_0093"></I></FONT></h4></P>
I might not even have written <A href="list10.htm">Listing 10</a>
if my C compiler supported inline SQRT instructions. There is little reason to have <I>sqrtf</I> in addition to <I>sqrt</I>, and this function is an exception to the rule of <I>float</I> functions being more straightforward than the standard <I>double</I> ones. The greatest economization is in not needing to take care of arguments between zero and <I>DBL_MIN</I>. In order to satisfy all the Paranoia tests, the <I>sqrt</I> function has to be carried out nearly to full <I>double</I> precision before rounding to <I>float.</I><P>
The special cases to be handled are negative arguments, which are invalid, and zero arguments, where the IEEE standard requires the result to be the identical flavor of zero. In the error case, the result should be a NaN. If the compiler generates correct code, one way to get a NaN is by casting a large <I>double</I> constant to <I>float</I><I>,</I> generating Infinity, and multiplying by zero. Optimizing compilers should generate the NaN constant at compiler time, possibly with a warning message.<P>
If the argument is finite, we go through some gyrations to obtain an initial approximation by making linear curve fits over the intervals 0.5 to 1 and 1 to 2. The low-order bit of the original exponent selects the interval. The code evidently depends on the floating point representation being <I>FLT_RADIX == 2</I>.<P>
With an initial approximation good to two significant figures, the accuracy can be improved by using Heron's approximation to double the number of significant figures as many times as needed. Since the basic algorithm gets hidden in the final code, I show it here as<P>
<pre><I>sqrt = (operand/sqrt + sqrt) </I>* 0.5</pre>
This formula goes back to antiquity, being credited to Heron. It can be derived using the Newton-Raphson method, an implementation of basic calculus.<P>
Software <I>sqrt</I> functions intended for machines with floating point arithmetic hardware always use an algorithm based on Heron's formula. Hardware implementations could use the longdivision method to produce a correctly rounded result in less time than a plain divide.<P>
The initial approximation is fixed up with the correct exponent, and iterations are performed to get the final result. We save a little time by combining three iterations into one, rationalizing to trade two divides for seven multiplications. After taking parallelism into account, we find a net gain if divide takes three or more times as long as multiply. On typical RISC systems, divide takes at least five times as long as multiply, so it is worth avoiding.<P>
The next to last approximation has to be within <I>0.5*FLT_EPSILON</I> in order to obtain the correctly rounded result for <I>sqrtf(1+FLT_EPSILON)</I>, as required by the IEEE Standard. In effect, this means that the final result has to be accurate to <I>2*FLT_DIG</I>, or nearly full <I>double</I> precision, before casting to <I>float</I>. There is just enough extra range in IEEE <I>double</I> to prevent overflow in this formula, and enough extra precision that roundoff errors of several times <I>DBL_EPSILON</I> don't affect the result.<P>
With one less iteration, the accuracy is within <I>FLT_EPSILON</I>, but the result occasionally rounds up when the IEEE standard requires it to round down. If the iterations are not widened to <I>double</I>, the result still rounds to one of the two nearest numbers representable in <I>float</I> precision, but frequently comes to the less accurate of the two.<P>
Since a hardware SQRT should be simpler than a divide, it seems a pity that C tradition assumes that we don't need it, even when most modern architectures have it. Certain run time libraries have shortcut handcoded functions for <I>fabs</I> and <I>sqrt</I>, so that the time spent is not too great. These handcoded functions will not appear in profiling data collection.<P>
<h4><FONT COLOR="#000080"><A name="0114_0094"><I>tanhf<A name="0114_0094"></I></FONT></h4></P>
<I>tanh</I>, like <I>sinh</I>, has an accuracy cancellation problem for small arguments. Because the function is by nature a ratio of two functions which can be approximated best in terms of rational approximations, no simple Chebyshev series is adequate. The approximation in <A href="list11.htm">Listing 11</a>
for small arguments is found by plugging the approximation used in <I>_Expf</I> into the formula for <I>tanh</I>.<P>
In two places we have a choice between an addition and a multiplication, and choose the addition. In both places, <I>(xx+xx)</I> yields the same result as <I>2*xx</I>; but the addition is faster on a RISC machine because the need for the constant 2.0 is eliminated. Also, the first add might be performed in parallel with a multiply on machines like the HP. In the second place, we could multiply the result of _<I>Expf</I> by itself if we hadn't doubled the argument. Since we are using <I>double</I> precision, to be rounded to <I>float</I> later on, we don't care about differences in range or roundoff.<P>
Certain compilers for RISC computers automatically change <I>(x+x) to 2*x</I> or vice versa, depending on the decision of the compiler writer. This is done because older systems generally performed better with a different choice than the newer architectures favor, and it is more convenient to write <I>*2 </I>without having to think about optimization.<P>
If the result of <I>_Expf</I> is large, we return the result 1.0. We could have saved some time by not invoking <I>_Expf</I>, but we assume that exceptional cases will be so infrequent that the time which they take can be ignored.<P>
<h4><FONT COLOR="#000080"><A name="0114_0095">Testing<A name="0114_0095"></FONT></h4></P>
Plauger's <I>tmath2.c</I> and <I>tmath3.c</I> test drivers can be adapted by adding <I>f</I> to their names, making <I>tmath2f.c</I> and <I>tmath3f.c.</I> The  tolerance, which is set in the existing code at <I>4*DBL_EPSILON</I>, must be changed to one based on <I>FLT_EPSILON</I>. All tests should pass at <I>1*FLT_EPSILON</I>. Likewise, the error of these approximations relative to the standard <I>double</I> functions should be less than <I>FLT_EPSILON</I>.<P>
You can also make <I>tmathf1.c</I> from <I>tmat1.c</I>, but using macro expansions results in many of the tests being essentially done at compile time. If they are done symbolically or in different precision, trying to use addition and subtraction of <I>1.0f/FLT_EPSILON</I> to compute <I>ceilf</I> or <I>floorf</I> will not work.<P>
The final group of macros in <I>&lt;math.h&gt;</I> (<A href="list1.htm">Listing 1</a>)
 is unsafe against side effects. If you have a compiler such as gcc which provides means for making them safe, you will want to have special versions which are selected automatically for that compiler. The relevant gcc extension consists of the ability to have a new block which returns a value from within the macro, so that new local variables can be defined.<P>
The results of these functions can be compared with the standard <I>double</I> functions, and should have a relative error less that <I>FLT_EPSILON</I>. They will not necessarily duplicate the results obtained by casting the <I>double</I> result to <I>float</I>; this could easily be off by <I>2*FLT_EPSILON</I>.<P>
The Paranoia series of tests for compliance with the IEEE floating-point Standard is particularly severe for <I>sqrtf</I>, as well as testing some extreme cases of <I>powf</I>. I used a variation on the original Paranoia FORTRAN, with the f2c translator, modified to use include files in normal C style, so that the <I>float</I> precision C functions are invoked.<P>
<h4><FONT COLOR="#000080"><A name="0114_0096">Conclusion<A name="0114_0096"></FONT></h4></P>
These <I>float</I> math functions provide maximum possible accuracy in <I>float</I> precision with a significant improvement in speed and reduction in code complexity compared to the standard <I>double</I> versions. They should work interchangeably with equivalent functions which might be provided by other libraries. If questions of accuracy or speed arise with the library provided with a compiler, these functions can be tested to determine their effect on an application.<P>
<h4>References</FONT></h4></P>
Stevenson, D., et al. <I>A Proposed Standard for Binary FloatingPoint Arithmetic</I>, IEEE Computer, 1981.<P>
Harbison, S.P. and Steele, G.L., Jr. C, <I>A Reference Manual</I>, 3rd edition, Prentice-Hall, 1991.<P>
Plauger, P.J. <I>The Standard C Library</I>, Prentice-Hall, 1992.<P>
<h4>Sidebar: <a href="sidebar1.htm">"Why So Many Math Functions?"</a></h4>

<h4><a href="../../../source/1994/jun94/prince.zip">Get Article Source Code</a></h4>

</BLOCKQUOTE>
</BODY>
</HTML>
