

<HTML>
<HEAD>

<TITLE>February 1994/Two Fast Pattern-Matching Algorithms</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocfeb.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Algorithms</FONT></H2>

<hr><h2 align="center"><font color="#800000">Two Fast Pattern-Matching Algorithms<A name="004C_001C"><A name="004C_001C"></font></h2><P>
<h3 align="center"><font color="#800000"><A name="004C_0000"><A name="004C_0000">Erick Otto</font></h3><hr><blockquote><P>
<P><i><A name="004C_0000"><A name="004C_0000">Erick Otto has a degree in computer science and has been working in the customer support group for the 5ESS telephone switch since 1985. He has been programming in C since 1984 and in the last four years has been involved in programming a Customer Complaints Database for AT&amp;T switching products. His particular interests are String/Pattern searching and techniques to make C programs run as fast as possible. He is currently working as supervisor of AT&amp;T-NS local field support in Indonesia. He may be reached at eotto@lfsin.idgtw.attibr.att.com or com!att!attibr!idgtw!lfsin!eotto.</i></P><P>
<h4><FONT COLOR="#000080"><A name="004C_001D">Introduction<A name="004C_001D"></FONT></h4></P>
This article introduces two algorithms that are very fast in finding strings in large text files. I have implemented them to perform string searching in a text database of over 120 Megabytes. I needed algorithms that were very fast and flexible, because the normal &#151; and still very much used &#151; one-by-one character comparison method was too slow to be practical. The algorithms I discuss here are about ten times faster on average than the "normal" UNIX grep.<P>
The first algorithm is actually the faster. It is based on a tuned Boyer and Moore algorithm <a href="#2">[2]</a> further developed by Hume and Sunday <a href="#3">[3]</a>. The second algorithm, called "shift-OR," is based on algorithms found by Beaza-Yates and Gonnet <a href="#1">[1]</a> and further developed by Wu and Manber. It provides more flexibility, including regular-expression searching without much loss of speed.<P>
<h4><FONT COLOR="#000080"><A name="004C_001E">The Boyer and Moore Algorithm<A name="004C_001E"></FONT></h4></P>
The basics of the first algorithm were first found by Boyer and Moore in 1977 <a href="#2">[2]</a>. The original goal was to make as few character comparisons as possible in order to speed up the search. They found that a low number of character comparisons usually means that the program is faster. By using efficient programming, and testing various constructions of algorithms, a higher execution speed can be obtained.<P>
One of the differences from grep is that the algorithm makes use of information retrieved from the pattern itself. Knowing that a certain character does not occur in the pattern is useful information. One other difference from grep is that the characters are compared fight to left.<P>
The character comparison starts at the last character of the pattern and the related position of that character in the text. (See <A href="fig1.htm">Figure 1a</a>.
) In other words, if we are positioned in the text at the character <I>x</I>, and the pattern we are looking for is <I>today</I>, there is no possibility that we can have a match at this position since <I>x</I> does not occur in the pattern.<P>
In order to know which characters occur in the pattern and which do not, we build a table <I>dist,</I> which contains the distances of every character in the pattern calculated from the end. The characters that do not occur in the pattern are set to the length of the pattern. (See <A href="fig2.htm">Figure 2</a>.
) This is another difference from grep. We have some overhead in pre-processing the pattern.<P>
Now we can look up the character <I>x</I> in the table and find that the distance is 5. This means we can advance our text pointer by 5. (See <A href="fig1.htm">Figure 1b</a>.
) The pointer into the pattern is untouched. We start the comparison again with the new pointer into the text.<P>
Now consider another situation, where the text character is an <I>a.</I> We can advance only <I>dist[a]</I>, or one character in the text. That way, the second character on the left of both the text and the pattern would be the character <I>a</I>. (See <A href="fig1.htm">Figure 1c</a>.
)<P>
Using this algorithm speeds up the search considerably because not every character in the text is compared with the pattern. It will at first only look for a starting position in the text to start the full pattern comparison.<P>
Another nifty trick that is used at this point is to see if the character that occurs the least frequently in the text (the lfc, for short) is present where we expect it. If we assume that <I>m</I> is the lfc in the pattern <I>memorandum,</I> we can do a pre-check on the leftmost occurrence of the lfc in the text. See <A href="fig3.htm">Figure 3</a>.
 This figure also shows at what positions which elements of the algorithms are used. If the pre-test is satisfied, we perform a normal match, comparing every character in the pattern with every character in the text.<P>
Finally the last basic step is to go forward in the text after we tested for a possible match. After extensive research performed by Hume and Sunday <a href="#3">[3]</a>, the shift used is based on the first leftward re-occurrence of the last character of the pattern. If there is no such re-occurrence we forward by the pattern length.<P>
This algorithm is very fast, but depends on the length of the pattern and the frequency of the characters in the pattern. <A href="list1.htm">Listing 1</a>
shows my implementation of the Boyer and Moore search algorithm. <A href="list3.htm">Listing 3</a>
shows a small program that will generate a frequency distribution derived from a file.<P>
<h4><FONT COLOR="#000080"><A name="004C_001F">The Shift-OR Algorithm<A name="004C_001F"></FONT></h4></P>
The other algorithm that I present is based on binary operations instead of character comparisons. This is faster than grep because mostly binary operations are performed.<P>
In this method we track the state of the search by using binary operations. The state variable will tell us how many characters have matched to the left of the current position in the text. To do this we need to maintain a table that contains information obtained from the pattern. The position of every character that occurs in the pattern gets a zero set in the table entry for that character. (See <A href="fig4.htm">Figure 4</a>.
)<P>
The characters are counted from left to right and the bits are counted from right to left. Since the character <I>e</I> occurs twice in the pattern <I>stereo,</I> it gets two bits set to zero. Characters that do not occur in the pattern get all ones set in the table. The initial state gets set to all ones. To perform the comparison we first shift the state left one bit. (In C, this mean insert a zero.) We then bitwise-OR the table entry for that character in the text. It can be envisioned as a window, opening (zero) when comparing a certain position.<P>
Suppose that the current character in the text is <I>s</I>. We first shift the state left one bit and bitwise-OR in <I>T[s]</I>. The result is the new state of the search, which has bit zero set to zero. This means that at the current position in the text there is one match of length one. (See <A href="fig5.htm">Figure 5a</a>.
)<P>
If the word <I>stereo</I> is in the text and we are positioned in the text at the second <I>e</I>, the state will be <I>1...101111.</I> This means that there is one match of length five left of the current position. (See <a href="fig5.htm">Figure 5b</a>.
)<P>
The way to decide if we have found a match is to set another variable to the length of the pattern (<I>stereo</I> has length 6) in the appropriate way &#151; in this case to <I>1...000000</I>. Now, if the state is less than the stop variable, we have found a match (<I>1...0111111..1000000</I>).<P>
The flexibility of this algorithm comes from the fact that we can easily manipulate the character table <I>T</I>. If we set the second bit from the right to a zero for every entry, this means we don't care what the second character of the pattern is. This is the so-called wildcard search. And the nice thing about this is that it doesn't cost a penny more to perform.<P>
We can apply the same logic to ranges of characters. We could for example set the second bit in table <I>T</I> for the characters <I>t, p</I>, and <I>f</I> to a zero. This would match the words <I>stereo, spereo</I>, and <I>sfereo</I> in the text. You could even make use of the information in the state to report partial matches.<P>
This algorithm is slower than the previous one but does not depend on the length of the pattern or the frequency of the characters. It can also be used in a much more flexible way. <A href="list2.htm">Listing 2</a>
shows my implementation of the shift-OR search algorithm.<P>
<h4><FONT COLOR="#000080"><A name="004C_0020">Implementations<A name="004C_0020"></FONT></h4></P>
The code presented hides a few tricks I would like to mention. I have put a lot of comments in the code to make clear what is happening. First the speed of a program like this also depends on the I/O mechanism that is used. In line 20 in the first program (<A href="list1.htm">Listing 1</a>)
 I therefore use the system-defined buffer size. This will reduce the number of I/O accesses. Usually, using buffers less than the system buffer size will cause unnecessary I/O.<P>
Further, in line 74 you will see a technique used to keep the number of tests inside a loop as small as possible. It usually saves a lot of execution time setting sentinels at the end or beginning of a buffer when searching for a character, instead of testing inside your loop for reaching the beginning or the end of your buffer.<P>
In line 80 we make sure that we end the text search on a line boundary, to prevent the match routine from getting half words. The <I>build</I> routine does the preparation work for the pattern search. It initializes the distance table, finds the lowest frequency character, and sets the shift. In line 176 and 177 again you see the sentinel technique, although a little different than before. We need to put as many characters at the end of the buffer as the text pointer can be forwarded. (See <A href="fig3.htm">Figure 3</a>.
)<P>
The other two more interesting points are the code lines at line 191-193 and line 234. At line 191 a technique called loop unrolling is used. Loop unrolling prevents a lot of branching (for iterations through the loop). On modern pipelined machines, the code is usually faster since most probably all the instructions can be contained in the instruction cache. Note that when on line 191 <I>k</I> is zero, lines 192-193 will also return <I>k</I> as zero, which is a prerequisite of using loop unrolling in this case. On different machine architectures, unrolling less or more times can be benificial. You can play around with it.<P>
Last, on line 234 we forward the text to the end of the line if we reported a match. Since we already printed the full line, why should we search for more occurences of the pattern? If you want to count the number of matches, this line should be removed!<P>
This program can also be easily adapted to do case-insensitive searches. The pattern will have to be converted to lower case. All table entries for characters occuring in the pattern will have to be set to their distances, but also their upper case counterparts will have to be set. Besides that, lines 205 and 209 will have to be changed so that references to the text are translated to lower case. The speed of the program is hardly lowered if this is done with a translation table using something like:<P>
<pre>char TR[MAXSYM];
/* build the case TRanslation table
*/
for(i=0; i &lt; MAXSYM; i++)
    TR[i] = i;
for(i='A'; i &lt;= 'Z'; i++)
    TR[i] = i + 'a' - 'A';</pre>
The shift-OR program (<A href="list2.htm">Listing 2</a>)
 uses the same efficient programming techniques. Starting at line 13, the size of a "word" (<I>unsigned int</I>) is defined. This is needed since the number of bits in a word is also the limit for the maximum pattern length that can be used. Of course you can use two words (this will slow the algorithm down), but the function <I>matchpat</I> will have to be altered.<P>
The build function in line 80, does the pre-processing of the pattern. I have included a small subset of the normal regular-expression pattern matching. The <I>build</I> function can handle a wildcard (dot or .) and ranges such as <I>[a-z]</I>. I am sure more inventive ways can be found to implement this, but this one works fine. In line 242, you can see one limitation that this implementation has. The first character needs to be normal, not a regular expression metacharacter. This actually speeds things up. Finally, on line 250, you see the actual binary operations being performed.<P>
<h4>References</FONT></h4></P>
<a name="1">[1]</a>     R. Baeza-Yates and G. H. Gonnet. "A New Approach to Text Searching," <I>Communications of the ACM</I>, Oct. 1992, Vol 35, No 10.<P>
<a name="2">[2]</a>     R.S. Boyer and J.S. Moore. "A Fast String Searching Algorithm," <I>Communications of the ACM</I>, Oct. 1977, Vol 20, No 10.<P>
<a name="3">[3]</a>     A. Hume and D. Sunday. "Fast String Searching," <I>Software, Practice and Experience</I>, Fall 1991.<P>

<h4><a href="../../../source/1994/feb94/otto.zip">Get Article Source Code</a></h4>

</BLOCKQUOTE>
</BODY>
</HTML>
