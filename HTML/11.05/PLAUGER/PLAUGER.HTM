

<HTML>
<HEAD>

<TITLE>May 1993/Standard C</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocmay.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Columns</FONT></H2>

<hr><h2 align="center"><font color="#800000">Standard C<A name="011A_007D"><A name="011A_007D"></font></h2><P>
<h3 align="center"><A name="011A_0000"><A name="011A_0000">Large Character Set Support</h3><P>
<h3 align="center"><font color="#800000">P.J. Plauger</font></h3><hr><blockquote><P>
<P><i><A name="011A_0000"><A name="011A_0000">P.J. Plauger is senior editor of The C Users Journal. He is convenor of the ISO C standards committee, WG14, and active on the C++ committee, WG21. His latest books are The Standard C Library, published by Prentice-Hall, and ANSI and ISO Standard C(with Jim Brodie), published by Microsoft Press. You can reach him at pjp@plauger.com.</i></P><P>
<h4><FONT COLOR="#000080"><A name="011A_007E">Introduction<A name="011A_007E"></FONT></h4></P>
Last month, I described the new machinery for processing Defect Reports &#151; to interpret or correct the C Standard. (See "State of the Art: Formal Changes to C," <I>C Users Journal</I>, April 1993.) I also outlined the "normative addendum" that will add a number of features to Standard C. Those changes are now being balloted for approval. Some are designed to help programmers write C source code that is more readable. I described them last month. But most of the changes are designed to ease writing programs that manipulate large character sets, a topic of growing interest.<P>
My goal this month is to start showing you the other, more extensive group of changes in the works for Standard C. Understand, they are still subject to modification. Member nations of ISO, and other interested parties, get several rounds of comments before we in WG14 freeze the normative addendum. I like to think, however, that this particular material is reasonably stable. It should give you a portent of how Standard C will look in another year or so.<P>
Equally important, my goal is to suggest some ways you might actually <I>use</I> these new capabilities. To some extent, the conversion to using a large character set is simple. Just look for all the places where you manipulate text as type <I>char</I><I>.</I> Replace them with objects and values of type <I>wchar_t.</I> Whatever the implementation chooses for a large character set, its codes must be representable as that type. But all sorts of subtle changes must occur as well. I'll endeavor to show you some of them.<P>
<h4><FONT COLOR="#000080"><A name="011A_007F">The Lightweight Approach<A name="011A_007F"></FONT></h4></P>
I begin by emphasizing that many programs can already speak Japanese, as it were. Programs often just read and write text without interpretation. You can feed them strings containing single-character and multibyte sequences intermixed, and they don't mess them up. (If they do, you can often fix the programs with little effort. Typically, you just stop presuming that you can break a string of text arbitrarily between any two characters.)<P>
A display or printer that does the right thing with these multibyte sequences may well show Kanji characters, just the way the user desires. Your program neither knows nor cares what happens to them outside its boundaries. Multibyte is usually the representation of choice outside programs. If you plan to do little or no manipulation of such text, it can also be the representation of choice <I>inside</I> a program as well.<P>
But lets say you have occasion to manipulate small amounts of text from a large character set within your program. You need to distinguish character boundaries so you can rearrange text or insert other characters. In that case, wide characters are the representation of choice inside the program. You want to deal, at least sometimes, in elements and arrays of type <I>wchar_t.</I><P>
Standard C already provides a minimal set of functions for converting back and forth between multibyte and wide character. You can also write wide-character constants, as <I>L'x'</I>, and wide-character strings, as <I>L"abc"</I>. Several people have demonstrated that you can do quite a bit of work with just what's already standardized. (See, for example, my book, <I>The Standard C Library</I>, Prentice Hall, 1992.)<P>
One thing we chose not to add to the original C Standard, however, was the ability to read and write wide characters directly. A format string can contain multibyte characters as part of its literal text, but you cannot easily read and write data of type <I>wchar_t</I>. You have to convert wide characters to a multibyte string in a buffer before you print it out. Or you have to read a multibyte string into a buffer before you convert it to wide characters. That can be a nuisance.<P>
<h4><FONT COLOR="#000080"><A name="011A_0080">Print and Scan Additions<A name="011A_0080"></FONT></h4></P>
So one of the changes in the normative addendum is to add capabilities to the existing print and scan functions. These are the functions declared in <I>&lt;stdio.h&gt;</I> with <I>print</I> or <I>scan</I> as part of their names. Each family of functions now recognizes two new conversion specifiers, <I>%C</I> and <I>%S</I>. For the print functions:<P>
<I>%C</I> writes to the output the multibyte sequence corresponding to the <I>wchar_t</I> argument. A shift-dependent encoding begins and ends in the initial shift sequence. That can result in redundant shift codes in the output, but the print functions make no attempt to remove them.<P>
<I>%S</I> writes to the output the multibyte sequence corresponding to the null-terminated wide-character string pointed to by the pointer to <I>wchar_t</I> argument. The same rules apply for shift codes as for <I>%C</I>. You can use a precision, as in <I>%.5S</I>, to limit the number of array elements converted.<P>
In no event will either of these conversion specifiers produce a partial multibyte sequence. They may, however, report an error. A wide-character encoding may consider certain values of type <I>wchar_t</I> invalid. Try to convert one and the print functions will store the value of the macro <I>EILSEQ</I> in <I>errno</I>. That macro is now added to the header <I>&lt;errno.h&gt;</I>.<P>
As usual, the behavior of the scan functions for the same conversion specifiers is similar, but different from that for the print functions:<P>
<I>%C</I> reads from the input a multibyte sequence and converts it to a sequence of wide characters. It stores these characters in the array designated by the pointer to <I>wchar_t</I> argument. You can use the field width to specify a character count other than one, as in <I>%5C</I>. The input sequence must begin and end in the initial shift state (if states matter).<P>
<I>%S</I> does the same job, but with two critical differences. The multibyte sequence to be converted is determined by first skipping leading whitespace, then consuming all input up to but not including the next whitespace. Here, whitespace has its old definition of being any <I>single character</I> for which <I>isspace</I> returns true in the current locale. (Yes, that can cause trouble with some multibyte encodings.) The resultant sequence must begin and end in the initial shift state. The other difference is that the scan functions store a terminating null after the sequence of converted wide characters.<P>
A multibyte encoding may consider certain character sequences invalid. Try to convert one and the scan functions will store the value of the macro <I>EILSEQ</I> in <I>errno</I>. The print functions may generate no incomplete multibyte sequences, but the scan functions aren't nearly as tidy. They can leave the input partway through a multibyte character, or in an uncertain shift state, for all sorts of reasons. Remember, both families of functions are still essentially byte-at-a-time processors.<P>
<h4><FONT COLOR="#000080"><A name="011A_0081">Staying Light<A name="011A_0081"></FONT></h4></P>
These additions to the print and scan functions were discussed before the C Standard was frozen, as I indicated earlier. We rejected them at the time because they offered only partial solutions to several problems:<P>
<UL><li>eliminating redundant shift states when generating a stream of multibyte characters</li>
<li>keeping track of shift states across function calls when scanning a stream of multibyte characters</li>
<li>producing and consuming whole multibyte characters even in the absence of shift codes.</li></UL>
The new "wide-character streams" solve such problems, but at a higher cost in machinery. I'll discuss them in a later installment.<P>
So why did WG14 agree to add machinery which is known to be inadequate? Because, for many applications, it is <I>good enough</I>, thank you. People writing "internationalized" applications have discovered a whole spectrum of needs. Earlier, I described a class of programs that need essensially no special support for large character sets. Others can use what's already in the C Standard. The biggest nuisance that many practicing programmers keep reporting is this omitted ability to read and write wide characters directly. For a small cost in code added to the print and scan functions, you get enough code to help many programmers.<P>
<h4><FONT COLOR="#000080"><A name="011A_0082">The Header <B><I>&lt;wchar.h&gt;</I><A name="011A_0082"></B></FONT></h4></P>
But let's say you need to get more serious about large character sets. In that case, you need more machinery. The normative addendum provides <I>lots</I> more machinery when you include the new header <I>&lt;wchar.h&gt;</I>. <A href="list1.htm">Listing 1</a>
shows a <I>representative</I> version of this header. The actual types used in the type definitions may vary among implementations, as can the value of the macro <I>WEOF</I>. I have simply made common choices.<P>
As you can see, this header declares quite a few new functions. Most of the new names are weird, but they can conceivably clash with those in existing C programs. That bodes ill for backward compatibility, a prime virtue in any update to a programming language standard.<P>
WG14 ameliorated the problem by deviating slightly from past practice. The name of every <I>current</I> library function is reserved in the external name space whether or not you include the header that declares it. That lets an optimizing compiler look for function names such as <I>sqrt</I> with fewer fears that it's guessing wrong in treating it special. But the names of functions declared in <I>&lt;wchar.h&gt;</I> are <I>not</I> similarly reserved. If you include the new header in any translation unit of a program, you have to look for conflicts. Otherwise, you can ignore the new stuff without fear.<P>
You'll notice a handful of type definitions in the new header. The first three are old friends. <I>size_t</I> and <I>wchar_t</I> are, in fact, already declared in multiple headers. <I>struct tm</I>, up to now, has been declared only in the header <I>&lt;time.h&gt;</I>. It is needed here to declare the function <I>wcsftime</I>. Note, however, that in &lt;<I>wchar.h</I>&gt; it is declared as an <I>incomplete structure type</I>. You still have to include <I>&lt;time.h&gt;</I> if you want to complete the type declaration, so you can poke at its innards.<P>
<h4><FONT COLOR="#000080"><A name="011A_0083">Wide Meta-Characters<A name="011A_0083"></FONT></h4></P>
Books on C seldom emphasize the point, but C has long supported a "meta-character" data type. It is implemented as type <I>int</I> with additional semantic constraints. Look, for example, at what the function <I>fgetc</I> returns. We're promised that the <I>int</I> return value is one of two things:<P>
<UL><li>a character code that can be represented as type <I>unsigned char</I>, or</li>
<li>the value of the macro <I>EOF</I>, declared in <I>&lt;stdio.h&gt;</I></li></UL>
Further, we are assured that <I>EOF</I> is distinguishable from all valid character codes. The typical implementation, in fact, has eight-bit characters and sets <I>EOF</I> to &#151;1. So valid values for a meta-character are in the closed interval [&#151;1, 255]. We use meta-characters all the time. We read them with <I>fgetc</I> or one of its sisters, <I>getc</I> or <I>getchar</I>. We test them with the functions declared in <I>&lt;ctype.h&gt;</I>, such as <I>isdigit</I> or <I>isalnum</I>. And, provided we filter out stray <I>EOF</I>s, we can even write them with <I>fputc</I> or one of its sisters, <I>putc</I> or <I>putchar</I>. Pretty handy.<P>
It is only natural that the world of wide characters should demand analogous machinery. Thus <I>&lt;wchar.h&gt;</I> defines a macro and a type that support programming with wide meta-characters:<P>
<UL><li>The type <I>wint_t</I> can represent any value representable as type <I>wchar_t</I>.</li>
<li>The macro <I>WEOF</I> can be represented as type <I>wint_t</I> and is distinguishable from all valid wide-character codes.</li></UL>
Now you should be able to understand why so many of the functions are declared the way they are in <I>&lt;wchar.h&gt;</I>. They simply carry on the old, and demonstrably useful, C tradition of trafficking in meta-characters. Only now they're <I>wide</I> meta-characters.<P>
There's one small subtlety I hate to let slide by without notice. The type <I>wint_t</I> is often represented with more bits than the type <I>wchar_t</I>. That gives the implementor lots of choices for the value of <I>WEOF</I>. But that need not be the case. It's perfectly valid for <I>wint_t</I> to be <I>the same size</I> as <I>wchar_t</I>. Then, the value of <I>WEOF</I> must be chosen from among the invalid wide-character codes. (And there better be at least one to choose from.)<P>
So don't fall into the habit of thinking of <I>wchar_t</I> as some unsigned integer type. And don't assume that <I>WEOF</I> is &#151;1, or any other negative value. You may one day be surprised. (If you want to write highly portable code, in fact, prepare for the possibility that types <I>char</I> and <I>int</I> are the same size. But that's a whole 'nother sermon.)<P>
<h4><FONT COLOR="#000080"><A name="011A_0084">State Memories<A name="011A_0084"></FONT></h4></P>
One of the notorious shortcomings of the Standard C library is its dependence on private memory. A number of functions maintain static storage to remember various values between calls. Some storage is even shared <I>among</I> different functions. The drawbacks of private memory in library functions are well known:<P>
<UL><li>What a function does each time you call it depends on more than just the value of its arguments. That makes its interface harder to learn and to use properly. <I>strtok</I>, for example, is notoriously hard to master.</li>
<li>You can't share the use of a function with memory among different tasks at the same time. Or you have to be very careful to save and restore context, as with <I>setlocale</I> and <I>localeconv</I>.</li>
<li>A stored pointer from one call can be sabotaged by a later call. <I>tmpnam(NULL</I>), for example, returns a pointer to a filename that changes with each call. Even worse, a call to <I>gmtime</I> changes the data pointed at by an earlier return from <I>localtime</I>.</li></UL>
We knew these shortcomings when we developed the C Standard. Much as we wanted to fix many of the offending functions, we decided it was too troublesome to change them. Too much existing code would have to be rewritten.<P>
Worse, we perpetuated this practice when we added several functions that manipulate large character sets. As they parse or generate multibyte strings, they may have to keep track of the current shift state. So the functions <I>mblen, mbtowc</I>, and <I>wctomb</I> have private memory. You initialize the memory to the initial shift state with one kind of call. You then progress through a multibyte string with other kinds of calls, and the memory tracks the shift state.<P>
Our major reason for using private memory was to avoid adding yet another argument to each of these functions. That argument also needs a special type definition as well. We didn't want all that semantic detail if people weren't going to parse multibyte strings all that much. After all, we've gotten by with <I>strtok</I> all these years, haven't we?<P>
Well, it turns out we were wrong. Seems <I>lots</I> of people are manipulating multibyte strings these days. And they chafe at the shortcomings I outlined above. Thus the new type defintion <I>mbstate_t</I>, and the new functions that use it. I'll describe them all next month.<P>
<h4><FONT COLOR="#000080"><A name="011A_0085">Conclusion<A name="011A_0085"></FONT></h4></P>
You'll find one more new type definition in <I>&lt;wchar.h&gt;</I>. The type <I>wctype_t</I> serves a limited role. It describes the "handle" returned from the function <I>wctype</I>. You use the handle only in the wide-character testing function <I>iswctype</I>. The handle corresponds to one of an open-ended set of character classifications. What are the classifications and how can you make new ones? The answer is sufficiently complex that I must also defer it to next month.<P>
It has taken me a whole installment just to lay the groundwork for understanding the large character set support being added to Standard C. A glance at <A href="list1.htm">Listing 1</a>
shows that <I>&lt;wchar.h&gt;</I> declares a mess of functions. By now, you can probably guess what many of them do. Your guesses are probably even right in most cases. My next task is to reinforce your prejudices where you're right and show you the surprises elsewhere. Tune in next month.<P>

<h4><a href="../../../source/1993/may93/plauger.zip">Get Article Source Code</a></h4>

</BLOCKQUOTE>
</BODY>
</HTML>
