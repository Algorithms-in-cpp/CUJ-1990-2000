

<HTML>
<HEAD>

<TITLE>July 1993/Standard C</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocjul.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Columns</FONT></H2>

<hr><h2 align="center"><font color="#800000">Standard C<A name="0223_00AF"><A name="0223_00AF"></font></h2><P>
<h3 align="center"><A name="0223_0000"><A name="0223_0000">Wide Character Streams</h3><P>
<h3 align="center"><font color="#800000">P.J. Plauger</font></h3><hr><blockquote><P>
<P><i><A name="0223_0000"><A name="0223_0000">P.J. Plauger is senior editor of The C Users Journal. He is convenor of the ISO C standards committee, WG14, and active on the C++ committee, WG21. His latest books are The Standard C Library, published by Prentice-Hall, and ANSI and ISO Standard C (with Jim Brodie), published by Microsoft Press. You can reach him at pjp@plauger.com.</i></P><P>
<h4><FONT COLOR="#000080"><A name="0223_00B0">Introduction<A name="0223_00B0"></FONT></h4></P>
This is the third (and last) in a series of columns on the facilities being added to Standard C. (See "State of the Art: Large Character Set Support," <I>C Users Journal</I>, May 1993 and "State of the Art: Large Character Set Functions," <I>C Users Journal</I>, June 1993.) By the time you read this, the first round of balloting should be complete on the "normative addendum" that adds these facilities. At least two more rounds must occur before they become a part of the C Standard.<P>
I have so far described:<P>
<UL><li>why Standard C needs more support for large character sets</li>
<li>how existing functions change in the Standard C library</li>
<li>what's in the new header <I>&lt;wchar.h&gt;</I></li>
<li>how many of the added functions work</li></UL>
I conclude this month by describing the largest single group of those functions, the ones that support wide-character input and output.<P>
I remind you again that what I'm describing here can still change in some details, as a result of comments during balloting. I expect, however, that those changes should be small.<P>
<h4><FONT COLOR="#000080"><A name="0223_00B1">Problems with Large Character Sets<A name="0223_00B1"></FONT></h4></P>
Remember that the overall goal of these additions is to ease the handling of text expressed with a very large character set. Japanese, Koreans, Chinese, Arabs, and typesetters all face a similar problem. They regularly work with "alphabets" containing thousands, or even tens of thousands, of characters. And that presents a host of problems:<P>
<UL><li>how to key in text from a huge character set</li>
<li>how to display a huge character set on a screen</li>
<li>how best to represent text inside a program</li>
<li>how best to represent text in external data streams and files</li></UL>
We learned a long time ago to divorce the first two issues from programs and programming languages. Operating systems change keystrokes into a stream of bytes for us. Similarly, they convert an output stream into suitable dots on a screen or printer. Yes, the separation breaks down from time to time, but it is an important ideal to keep striving toward. Put another way, MS-DOS and UNIX may have to worry about how to display Kanji or tenpoint Times Roman &#151; C and C++ should not.<P>
We have learned more recently to represent characters inside a program as fixed-size integers. A 16-bit integer can support distinct codes for up to 65,535 characters (setting zero aside as a null terminator). A 32-bit integer can support billions of distinct codes. Such characters in C are called <I>wide characters</I>, as opposed to the traditional one-byte versions of type <I>char</I> (and its signed and unsigned varieties).<P>
That's why the C Standard introduced the defined type <I>wchar_t</I> as a synonym for some integer type. Wide-character constants such as <I>L</I>'' have this type, and wide-character strings are arrays of this type. C++ has gone further, making <I>wchar_t</I> a distinct type, so you can overload functions on one-byte and wide-character arguments.<P>
That's also why the normative addendum adds so many new functions to manipulate wide characters. As I discussed last month, it is relatively easy to convert programs that manipulate <I>char</I> data to deal with <I>wchar_t</I> data instead.<P>
Having a fairly complete set of analogous functions can only help.<P>
A number of code sets exist for wide characters, by the way. Even Kanji has more than one in popular use within Japan. True, the new ISO 10646 standard promises to unite all the character sets of the world. But the jury is still out on just how that "universal" code set will best be used as a wide-character representation inside computer programs.<P>
<h4><FONT COLOR="#000080"><A name="0223_00B2">Multibyte Encoding<A name="0223_00B2"></FONT></h4></P>
Wide characters are not nearly as much at home in external data streams. It's an eight-bit-byte world out there. Chop a wide character into a sequence of bytes and you immediately face problems:<P>
<UL><li>Byte order differs among computers for multibyte integers of the same size.</li>
<li>You need some way to distinguish wide-character streams from conventional streams of one-byte characters.</li>
<li>Raw binary data offers few if any clues for resynchronizing the parse of a corrupted data stream.</li></UL>
For all these reasons, we have learned over the years to encode large character sets differently for data streams.<P>
This alternative form is called a <I>multibyte</I> encoding. No, there is no one universal multibyte code (just as there is still no universal wide-character code). But the rules for making multibyte codes follow a few basic patterns.<P>
In all cases, the number of bytes used to represent a character varies for different members of the set. Codes based on ASCII, for example, represent the most used ASCII characters unchanged, as one-byte sequences. (Files containing mostly ASCII code are hence economical of disk space.) Longer codes are signaled one of two ways, either by prefix codes or by shift encoding.<P>
With a prefix code, you can tell just by looking at a byte whether it stands alone as a single-byte code or whether it is the first byte of a longer sequence. You may also have to look at bytes that follow to decide when the sequence stops. But the major virtue is that each code sequence defines itself in all contexts. At worst, you may have to know that a given byte marks the start of a new code sequence.<P>
A popular Kanji encoding is Shift JIS. Any code in the interval [0x81, 0x9F] or [0xE0, 0xFC] is the first byte of a two-byte code. Any other first byte is a single-byte code. The second byte of a two-byte code must be in the interval [0x40, 0xFC].<P>
With shift encoding, you have more flexibility. A <I>shift code</I> (sometimes called an <I>escape sequence)</I> signals a change of rules. Once the byte sequence enters an alternate shift sequence, it might, for example, group two bytes at a time to define each subsequent character. The first byte might look like an ASCII <I>A</I>, but in this context it is not treated as such. Only another shift code behaves differently. It can put the byte stream back in the <I>initial shift state</I>, where <I>A</I> is <I>A</I> once again.<P>
Another popular Kanji encoding is JIS. The three-byte shift code <I>"\33$B"</I> shifts to two-byte mode. In that mode, both first and second bytes must be in the interval [0x21, 0x7E]. The three-byte shift code <I>\33(B"</I> returns to the initial shift state. Still a better example is a typical word-processor file format. One code shifts text to italic (or bold), another shifts it back to normal.<P>
Shift encoding exacts a price for its flexibility. To parse a multibyte string, you need even more context than for prefix codes. You have to know where you stand within a given character, as before. You also have to know the current shift state. The parsing job is that much harder. The prospects for getting out of sync are that much greater.<P>
<h4><FONT COLOR="#000080"><A name="0223_00B3">Multibyte Streams<A name="0223_00B3"></FONT></h4></P>
So here is the quandary. You want to work with wide characters inside a program. You need to convert them to and from multibyte characters as you read and write I/O streams. Yet opportunities abound for mucking up the parse as you read streams. And opportunies abound for generating flawed sequences, or redundant shift codes at least, as you write streams.<P>
Now consider the way a typical program reads and writes data. Occasionally, all reads occur in one place and/or all writes in another. More likely, reads and writes are sprinkled throughout the code. That makes it hard for a program to coordinate parsing a stream as multibyte characters, or generating characters in the proper shift state. The only thing in common to all reads or writes is the object of type <I>FILE</I> that controls the stream.<P>
But operations on C streams are defined in terms of byte-at-a-time input or output. (The defining primitives are <I>fgetc</I> and <I>fputc.</I>) Streams know nothing about possible shift codes or any other multibyte structure. It is altogether too easy to create mayhem using the conventional C facilities to read and write multibyte streams.<P>
The Japanese tried several ways to "fix" the existing I/O functions. They failed repeatedly. <I>scanf</I>, for example, is troublesome enough reading streams a byte at a time. Any attempt to make the function aware of multibyte syntax leads to parsing rules that are impossibly complex. <I>printf</I> is less troublesome, but still a nuisance. It has no obligation to produce shift codes and multibyte codes that are optimal, or even correct.<P>
One approach (as always) is to add another layer of code. Define a "wide input stream" that reads bytes from another stream and assembles them into a stream of wide characters. Also define a "wide output stream" that accepts wide characters and turns them into multibyte sequences that it writes to another stream. Using them gives you more of the pure wide-character environment we now know is desirable within a program.<P>
Sequential I/O works fine this way, but random access is rather more of a problem. You think you're positioning a stream at a given wide character. The actual underlying stream has to be positioned at the start of a given multibyte character. Not only that, you may have to restore the remembered shift state at the start of that multibyte character. The job is not impossible, but it is a nuisance.<P>
One way to make the job of file positioning easier is to constrain it somewhat. The C Standard already lets an implementation do this with text files. You can find out where you currently are in a text file (<I>ftell</I> or <I>fgetpos</I>). If that query succeeds, you can later return the stream to that position (<I>fseek</I> or <I>fsetpos</I>). But you can't just position the file to read byte 17,239 next, as you can with a binary file.<P>
So you impose a similar constraint on positioning within a wide-character stream. You can find out where you currently are in the wide-character stream, memorizing the position as for a text file. The only additional problem is that you may need additional bits to memorize a shift state. That's a real nuisance for <I>ftell</I>, which returns a <I>long</I>. Take one bit for the shift state and you can only memorize positions half as far into files. Need more bits for the shift state? The capability of <I>ftell</I> is more drastically reduced. You have no additional problems with <I>fgetpos</I>, however. It returns a value of type <I>fpos_t</I>, which an implementation can define as a structure containing all the fields, of whatever size, it might need.<P>
Given a file position, you can later find your way back to it by calling <I>fseek</I> or <I>fsetpos</I>, as appropriate. You can also position a file at its start with equal ease. (The implementation may have to decree that all streams start in the initial shift state, a not unreasonable constraint.) To position a file at its end can be tougher, if it uses a state-dependent encoding. The only sure way to get the shift state right is to read the file from a known position to the end. (The implementation can always end files in a known shift state, but that makes it hard to avoid writing files containing redundant shift codes, a desirable if not mandatory property.)<P>
Of course, you have to impose similar constraints on rewritting a file. You can't just drop a few wide characters in the middle of nowhere. It's too hard for an implementation to splice in an arbitrary multibyte sequence. But the same problem already exists for text files on many operating systems. They freeze record boundaries when you first write them. Rewriting the file cannot alter those boundaries. So as a general rule, once you rewrite part of a file, its contents from the end of what you just wrote to the old end of file are now in an undefined state.<P>
Even with all these constraints, however, you can get a lot of work done. People have been writing C code for years that runs with highly structured text files. Imposing multibyte syntax is just more of the same, not an entirely new problem.<P>
<h4><FONT COLOR="#000080"><A name="0223_00B4">Wide-Character Streams<A name="0223_00B4"></FONT></h4></P>
The proposed addendum to the C Standard introduces the concept of <I>wide-character streams</I>. These behave much like the extra layer of code I hypothesized above, with one important difference. The new functionality is essentially stuffed into existing <I>FILE</I> objects. You don't interpose a second stream to convert to and from streams of wide characters. Instead, you convince a conventional <I>FILE</I> object that it is mediating a wide-character stream instead of a conventional unstructured byte stream.<P>
How do you convince a <I>FILE</I> that it is mediating a wide-character stream? By the operations you perform on it. The header <I>&lt;wchar.h&gt;</I> declares analogs for many of the functions declared in <I>&lt;stdio.h&gt;</I>. (See <A href="list1.htm">Listing 1</a>.
) Like their older cousins, each of these new functions operates on a stream, either implied or explicitly named as an argument. But the new functions read and write wide characters instead of one-byte characters.<P>
Note that <I>fopen</I> does not change. The difference is that now a stream does not know, when first opened, what its <I>orientation</I> will be, whether it will become <I>wide-oriented</I> or <I>byte-oriented</I>. As soon as you call any of the functions in <A href="list1.htm">Listing 1</a>
for that stream, it becomes wide-oriented. All subsequent reads, writes, and positionings will be in terms of wide characters. If you instead call any of the older analogs to these functions, the stream becomes byte oriented. All subsequent operations will be in terms of single bytes, just like in the C library you know and love.<P>
A few functions, are ecumenical. You can, for example, call <I>setvbuf</I> or <I>fgetpos</I> for a stream and still make no commitment. You can also call these (and their related) functions for a stream of either orientation, once it is established. The one thing you cannot do is try to have it both ways. Mix calls of different orientations for the same open stream and you get undefined behavior. (That means that an implementation can do something sensible if the mood strikes it, but it doesn't have to.) The only way to alter the orientation of a stream, in fact, is to call <I>freopen</I> on the <I>FILE</I> object that mediates it.<P>
WG14 chose this approach over adding a mode qualifier to <I>fopen</I> (and <I>freopen</I>, of course). One reason for doing so was simplicity. The other was to simplify the use of <I>stdin</I> and the other standard streams with wide orientation. If the first operation determines orientation, a standard stream can go either way with no additional considerations.<P>
<h4><FONT COLOR="#000080"><A name="0223_00B5">Whither C++<A name="0223_00B5"></FONT></h4></P>
An important open issue is how best to include this new capability into C++. X3J16/WG21, the C++ standards committee, has already committed to tracking the C Standard as it evolves. You can be sure that all these new functions will become a part of the C++ library, just as the entire Standard C library already is.<P>
But is that the best way to manipulate large character sets in C++? Probably not. Serious C++ programmers already disdain the use of the C I/O machinery. They favor the classes declared in <I>&lt;iostreams.h&gt;</I>. (The continued presence of a trailing <I>.h</I> is currently being debated.) They like to use operator <I>&gt;&gt;</I> and operator <I>&lt;&lt;</I> to do the actual input and output. Every new class they write also overloads these operators, if it makes sense at all to read or write objects of that class.<P>
So can we expect <I>iostreams</I> to be further overloaded on wide characters? The decisions have not yet been made, but I'd be astonished if that didn't happen. I just worry a bit about complexity overload. C streams were already pretty complex. C++ <I>iostreams</I> add considerable complexity atop C streams. Wide streams add even more complexity <I>within</I> C streams. How well it all hangs together will be interesting to observe.<P>
As always, I have to warn that all this stuff is new. We have very little experience with wide character manipulation in general. We have even less with the newly proposed wide-character functions I've described in the previous two columns. And we have next to no experience with the wide-character streams described here. We can only hope that it all will indeed simplify our lives as programmers in the years to come.<P>

<h4><a href="../../../source/1993/jul93/plauger.zip">Get Article Source Code</a></h4>

</BLOCKQUOTE>
</BODY>
</HTML>
