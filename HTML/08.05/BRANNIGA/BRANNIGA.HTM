


<HTML>
<HEAD>

<TITLE>May 1990/An Adaptive Data Analyzer</TITLE></HEAD>
<body bgcolor="#ffffff">
<H2><A HREF="../tocmay.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   Number Crunching</FONT></H2>

<hr><h2 align="center"><font color="#800000">An Adaptive Data Analyzer<A name="00FB_008C"><A name="00FB_008C"></font></h2><P>
<h3 align="center"><font color="#800000"><A name="00FB_0000"><A name="00FB_0000">Michael Brannigan</font></h3><hr><blockquote><P>
<P><i><A name="00FB_0000"><A name="00FB_0000">Michael Brannigan is president of Information and Graphic System, IGS, 15 Normandy Court, Atlanta, GA 30324 (404) 231-9582. IGS is involved in consulting and writing software in computer graphics, computational mathematics, and data base design. He is presently writing a book on computer graphics algorithms. He is also the author of The C Math Library EMCL.</i></P><P>
This article extends the methods discussed in "Fitting Curves To Data" (<I>The C Users Journal,</I> Jan. 1990). That article presented an algorithm which, given a set of data points, produced a piecewise cubic approximation to solve the data fitting problem. However, that algorithm required the user to supply the number and position of the knots. This article describes an algorithm that chooses, adaptively, the number of knots and their position.<P>
Like the earlier algorithm, this adaptive approach will require some linear algebra. However, the adaptive algorithm will also draw upon statistical theory (to analyze errors), information theory (for the adaptation mechanism), and non-linear optimization methods (to compute knot positions).<P>
<h4><FONT COLOR="#000080"><A name="00FB_008D">The Problem<A name="00FB_008D"></FONT></h4></P>
Consider the basic problem. Given data points <I>(xi, yi) i=1,2,...,n</I> we suppose there exists a relation</FONT></FONT><P>
<pre>y<SUB>i</SUB> = F(x<SUB>i</SUB>) + e<SUB>i</SUB>, i=1,2,...,n</pre>
where <I>F(x)</I> is an unknown underlying function and <I>e</I><I><SUB>i</SUB></I> represents the unknown errors in the measurements <I>y</I><SUB>i</SUB>. The goal is to find a function <I>f(<FONT FACE="Symbol" SIZE=1>a</FONT><SUB>k</SUB>,x)</I> with parameter vector <I><FONT FACE="Symbol" SIZE=1>a</FONT><SUB>k</SUB></I>, of length <I>k,</I> which is a good approximation to <I>F(x)</I>.<P>
The earlier non-adaptive algorithm solved this problem for a fixed value of <I>k</I> (set by the user). Treating <I>k</I> as a variable creates a more complex problem. If we go even further and ask the computer to choose the best value for <I>k</I> and <I><FONT FACE="Symbol" SIZE=1>a</FONT><SUB>k</SUB></I>, then the algorithm bocomes adaptive.<P>
In curve-fitting applications, the underlying function <I>F(x),</I> often represents some (often disjointed) relationship in the physical world. Splines, or some other piecewise function, have the advantage of matching this disjointed behavior, while retaining global approximation properties. Piecewise functions also exhibit an advantageous flexibility &#151; behavior in one region doesn't significantly effect their behavior in another.</FONT></FONT><P>
Piecewise functions are also important to the analysis of time-dependent data, or time series analysis, where the periods determine the pieces of the piecewise function.<P>
Piecewise functions depend on certain points called knots. Between each knot we compute some function, usually a cubic polynomial, with parameters depending on the data. For time series the knots occur at the end of the periods, and the piecewise function is a Fourier series.<P>
No matter how you choose the piecewise function <I>f( ),</I> the unknown parameter <I>k</I> depends on these knots, how many knots and where the knots are placed. This article considers the problem of how to automatically find the optimum number of knots and the optimum placement of the knots, that is, to find <I>k.</I></FONT></FONT></FONT></FONT></FONT><P>
<h4><FONT COLOR="#000080"><A name="00FB_008E">Statistics<A name="00FB_008E"></FONT></h4></P>
An "optimum" approximation must fit the experimental data with a minimum of error. But whether a given approximation's errors are minimum, compared to a competing approximation's, depends on how we measure the error and what assumptions we make about how "minimum" errors are distributed. In every fitting operation, there are potentially two independent sources of error: poor matching between the approximating function and the real-world phenomenon, and measurement errors in the experimental data. Analytically it is convenient to contribute all errors to the experimental data.<P>
If we assume the errors in the data are uniformly distributed, with distribution function:<P>
<IMG SRC="eq2.gif"><p>
then we need only find the value of the parameter <I><FONT FACE="Symbol" SIZE=2>s</FONT></I>. Given the<I> n</I> errors <I>e</I><I><SUB>i</I></SUB><I>,i=1,2,...,n</I> from an approximation, then the maximum absolute error is called the <I>maximum likelihood estimation</I> of the value of <I><FONT FACE="Symbol" SIZE=2>s</FONT></I>. Approximations generated with the minimax norm from the non-adaptive program <a href="#note1">[1]</a> will minimize the maximum likelihood estimate of <I><FONT FACE="Symbol" SIZE=1>s</FONT></I>. Unfortunately, errors in data are uniformly distributed only when all errors can be attributed to errors of computation (that is, the data is exact up to the last few bits), thus one can seldom use the minimax norm.<P>
The most popular alternative is to assume that the errors have a normal distribution given by:<P>
<pre>f(e;<FONT FACE="Symbol" SIZE=3>m</FONT>,<FONT FACE="Symbol" SIZE=3>s</FONT>) = exp(-(e-<FONT FACE="Symbol" SIZE=3>m</FONT>)<SUP>2</SUP>/2<FONT FACE="Symbol" SIZE=3>s</FONT><SUP>2</SUP>)/((2<FONT FACE="Symbol" SIZE=3>p</FONT>)<SUP>1/2</SUP><FONT FACE="Symbol" SIZE=3>s</FONT>)</pre>
(recent research, however, does not uphold this assumption). Given the errors <I>e<SUB>i</SUB>, i=1,2,...,n</I> the maximum likelihood estimates of <I><FONT FACE="Symbol" SIZE=2>m</FONT></I> and <I><FONT FACE="Symbol" SIZE=2>s</FONT></I> are respectively the mean and standard deviation. If the mean is zero, then the standard deviation <I><FONT FACE="Symbol" SIZE=2>s</FONT></I> is given by<P>
<IMG SRC="eq4.gif"><p>
This standard deviation can be minimized by approximation using the L2 or least squares norm approach from <a href="#note1">[1]</a>.<P>
However, modern statistical research indicates that the best approximations are produced by assuming that errors are described by the less frequently used Cauchy distribution, given by:<P>
<pre>f(e;<I><FONT FACE="Symbol" SIZE=3>s</FONT></I>) = <I><FONT FACE="Symbol" SIZE=3>s</FONT></I>/(<I><FONT FACE="Symbol" SIZE=3>p</FONT></I>(<I><FONT FACE="Symbol" SIZE=3>s</FONT></I><SUP>2</SUP>+e<SUP>2</SUP>))</pre>
Statisticians call approximation driven by this assumption about errors "robust" estimation, because the estimates are not influenced by an occasional large error, or "outlier".<P>
For this distribution, the maximum likelihood estimate <I><FONT FACE="Symbol" SIZE=3>s</FONT></I><SUP>*</SUP> of the parameter <I><FONT FACE="Symbol" SIZE=3>s</FONT></I>, given the errors <I>ei,i=1,2,...,n</I> is<P>
<IMG SRC="eq6.gif"><p>
Which is a definition of the L1-norm, another approach supported in the earlier non-adaptive program <a href="#note1">[1]</a>.<P>
<h4><FONT COLOR="#000080"><A name="00FB_008F">Information Theory (The AIC)<A name="00FB_008F"></FONT></h4></P>
The minimum amount of information which one can prescribe to a set of errors is the form of the distribution function. Thus, the user, having chosen a suitable set of approximation functions, decides which distribution function to expect from the actual errors. An adaptive algorithm needs only the form of the distribution and not the distribution parameters; these parameters become an integral part of the output for subsequent analysis.<P>
To select the distribution form, we need some way to compare the errors of a single approximation with the unknown, underlying errors of the data. The 'Mean Information for Discrimination', an information theoretic concept related to the 'Maximum Entropy Principle', forms the basis for such a comparison.<P>
Using these concepts, theoreticians have shown that<P>
<pre>-2 <FONT FACE="Symbol" SIZE=3>å</FONT> ln f(e<SUB>i</SUB>;<FONT FACE="Symbol" SIZE=3>s</FONT><SUP>*</SUP>) + 2k</pre>
(where <I>f(e;<FONT FACE="Symbol" SIZE=2>s</FONT>)</I> is a selected distribution function, <FONT FACE="Symbol" SIZE=2>s</FONT><SUP>*</SUP> is the maximum likelihood estimate of the distribution parameters given the <I>n</I> errors (observations) <I>e<SUB>i</SUB></I>, and <I>k</I> is the number of unknown parameters.) Thus, by the maximum entropy principle, we seek a value of <I>k</I> and a set of errors <I>e<SUB>i</SUB></I> that minimizes this expression.<P>
This information theoretic criterion was first used in the analysis of time series and is known in the statistics literature as the AIC. If you assume that the errors are normally distributed, then this method is equivalent to the method known as 'Generalized Cross Validation'. The method described here is superior because it relaxes the restrictive assumption about normally distributed data errors. The mathematically inclined reader can find all the relevant proofs in <a href="#note2">[2]</a>.<P>
For each of the distributions considered above, we get the AIC from the maximum likelihood estimates of the parameters. Thus:<P>
<IMG SRC="eq8.gif"><p>
The goal of the algorithm is to minimize the appropriate AIC with respect to <I>k.</I></FONT><P>
<h4><FONT COLOR="#000080"><A name="00FB_0090">Optimization<A name="00FB_0090"></FONT></h4></P>
To optimize the knot positions for each value of <I>k,</I> the algorithm must determine the best position for the knots by optimizing the AIC function. This is a highly non-linear function of these knot positions, which are constrained to lie within the bounds of the knots. In other words, we have a constrained non-linear optimization problem, which is at best a difficult computational problem. The problem becomes more tractable if we get rid of the constraints and transform the problem to an unconstrained one as follows.</FONT></FONT><P>
Given the knots <I>X</I><I><SUB>o</I></SUB><I>,...,X k</I> define</FONT></FONT><P>
<pre>h<SUB>i</SUB> = X<SUB>i</SUB> - X<SUB>i-1</SUB></pre>
and<P>
<pre>s<SUB>i</SUB> = ln(h<SUB>i+1</SUB>/h<SUB>i</SUB>)</pre>
This gives <I>k-1</I> unconstrained variables and an unconstrained non-linear minimization problem. I strongly suggest you use a Quasi-Newton algorithm to solve this optimization problem. Also, make sure that the computer program that performs the optimization uses a rank-one update to keep the matrices symmetric positive definite, otherwise convergence is most unlikely.</FONT></FONT><P>
<h4><FONT COLOR="#000080"><A name="00FB_0091">Algorithm<A name="00FB_0091"></FONT></h4></P>
The flow chart in <A href="fig1.htm">Figure 1</a>
describes a general algorithm, useful for any fitting function.<P>
<h4><FONT COLOR="#000080"><A name="00FB_0092">Figure 1<A name="00FB_0092"></FONT></h4></P>
In this flow chart the knots represent unknown parameters. For a time series adaptive fit, the knots are the new periods. For piecewise polynomial fits, the knots are the points where the pieces fit together. As the flow chart shows, the algorithm converges when the value of the AIC stops decreasing. In each of the AIC functions, the first part is strictly decreasing, while the second part is strictly increasing. The minimum occurs when the increasing part, which begins at zero, takes over the decreasing part.<P>
The program in <A href="list1.htm">Listing 1</a>
utilizes the piecewise cubic approximation described in my earlier non-adaptive solution <a href="#note1">[1]</a> to create an adaptive solution to the data fitting problem.<P>
<h4><FONT COLOR="#000080"><A name="00FB_0093">Listing 1<A name="00FB_0093"></FONT></h4></P>
The main routine <I>adapt( )</I> follows the algorithm using the approximation routine <I>Hermite( )</I> from <a href="#note1">[1]</a>. First we fit the data to two, then three knots to give the first values of AIC for comparison. If a minimum AIC is not found, we consider each interval to see if it is a candidate for the inclusion of an extra knot.</FONT></FONT></FONT></FONT><P>
This step involves some heuristic reasoning. We say that an interval is a candidate for a new knot if the second moment of the errors in that interval is larger than the global second moment. If the interval is a candidate for a new knot, then we find the best position for the knot using the routine <I>point( ).</I> If the new point is close to an old knot, we exchange the knots and do not add another. If, on the other hand, the new point is not close to an old knot, then we add the new point as a new knot. This testing (to see whether the new point is close to a knot or not) is performed in the routine <I>test_point( ).</I></FONT></FONT></FONT><P>
The routine <I>AIC( )</I> computes the value of the AIC for different distributions. The utility routine <I>numpoints( )</I> computes the number of data points in a given interval. After the final fit is achieved, we then optimize the position of the knots using the routine <I>QN_BFGS_F( )</I>, which is a Quasi-Newton algorithm using a Broyden, Fletcher, Golfarb, Shanno formula. This algorithm is numerically stable if rank-one updates of the Cholesky factors are implemented.</FONT></FONT></FONT></FONT></FONT></FONT><P>
<A href="fig2.htm">Figure 2</a>
and <A href="fig3.htm">Figure 3</a>
show the results of a demonstration run in which some error was added to 200 points generated by the function <I>x</I><I><SUP>2/3</I></SUP><I>.</I></FONT><P>
<A href="fig2.htm">Figure 2</a>
shows the result of the program run with this data with a normal distribution assumption. <A href="fig3.htm">Figure 3</a>
shows the result when some spurious points from a straight line are added. A Cauchy distribution was assumed for this run so that the "outliers" would not influence the result.<P>
Additionally, the knots and their positions in the figures from <a href="#note1">[1]</a> were found using the program listed here.<P>
<h4>Bibliography</FONT></h4></P>
<a name="note1">1.</a>     Brannigan, M. "Fitting Curves to Data," <I>The C Users Journal</I>, January 1990.<P>
<a name="note2">2.</a>     Brannigan, M. "A Multivariate Adaptive Data Fitting Algorithm" in <I> Numerische Methoden der Approximation Theorie</I>, vol. 5, ISNM Birkhauser 1982.<P>
3.     Akaike, H. "<I>Information Theory and the Extension of the Maximum Likelihood Principle</I>," Proc. 2nd Symp. Inf. Th. Akademie Kiado, Budapest 1973.<P>

<h4><a href="../../../source/1990/may90/branniga.zip">Get Article Source Code</a></h4>


</BLOCKQUOTE>
</BODY>
</HTML>
