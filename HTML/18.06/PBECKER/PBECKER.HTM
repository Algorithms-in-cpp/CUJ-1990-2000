<HTML>   
     <HEAD>
<TITLE>June 2000/The Journeyman's Shop</TITLE></HEAD>
<BODY BACKGROUND="" BGCOLOR="#FFFFFF" TEXT="#000000">
<H2><A HREF="../tocjun.htm"><IMG SRC="../../toc.gif" ALT="{back to toc}" WIDTH="54" HEIGHT="54"></A><FONT COLOR="#FF0000">   C/C++ Contributing Editors</FONT></H2>

<HR>

<H2 ALIGN="center"><FONT COLOR="#800000">The Journeyman's Shop: Floating-Point Basics</FONT></H2>
<H3 ALIGN="center"><FONT COLOR="#800000">Pete Becker</FONT></H3>

<BLOCKQUOTE>
<p>You can't overcome a fear of floating-point arithmetic &#151; or complacency about it &#151; until you understand what's going on.</p>
</BLOCKQUOTE>

<HR>
<BLOCKQUOTE>

<p>Programmers mostly fall into one of three categories in their understanding of floating-point math: there are some who don't know enough about it to recognize that its results are not completely reliable; there are some who know just enough about it to think that its results are never reliable; and there are a few who understand it thoroughly and know exactly how reliable it is. As Donald Knuth puts it <a href="#1">[1]</a>:</p>

<BLOCKQUOTE>
<p>There's a credibility gap: We don't know how much of the computer's answers to believe. Novice computer users solve this problem by implicitly trusting in the computer as an infallible authority; they tend to believe that all digits of a printed answer are significant. Disillusioned computer users have just the opposite approach; they are constantly afraid that their answers are almost meaningless.</p>
</BLOCKQUOTE>

<p>Here in The Journeyman's Shop we try to fit ourselves into yet another category: those who know enough about floating-point math to understand and avoid the most common pitfalls and to recognize the times when our knowledge isn't sufficient to produce reliable results &#151; the times when we need help from an expert.</p>
<p>I'm not an expert on floating-point math. On a good day, I can tell you what "ulp" means <a href="#2">[2]</a>, but if someone starts explaining how to get an additional half an ulp in the computation of <B>atanh(x)</B> when <B>x</B> is close to 1, I'll nod and try to keep my eyes open while imagining I'm on a beach soaking up the sunshine. Don't misunderstand me: I admire people who can do this sort of analysis. But the intimate details of producing maximally accurate results are not something I need to know for the work that I do. Such details are complex enough that I don't want to dig into them for recreation either <a href="#3">[3]</a>. For most of us, it simply isn't worthwhile to spend the time needed to truly understand floating-point math. Nevertheless, we need to know something about it to use it with reasonable likelihood of success. From Knuth, once again <a href="#4">[4]</a>:</p>

<BLOCKQUOTE>
<p>[E] very well-rounded programmer ought to have a knowledge of what goes on during the elementary steps of floating-point arithmetic. This subject is not at all as trivial as most people think, and it involves a surprising amount of interesting information.</p>
</BLOCKQUOTE>

<p>This month we'll begin by looking at some of the ways that floating-point math can go astray; and then, in the interest of well-roundedness, we'll look at the implementation of a class that supports floating-point math. From time to time in future columns, we'll come back to that class to improve our understanding of the subtleties of floating-point math.</p>

<H4><FONT COLOR="#000080">Floating-Point Values Are Often Inexact</FONT></H4>

<p>Beginning programmers are often surprised by the behavior of code like this:</p>

<pre>
for (float index = 0.0; index != 1.0;
     index += 0.1)
   printf("%f\n", index);
</pre>

<p>They expect it to loop ten times and then stop. It doesn't, of course; it loops forever, or until they run out of patience and kill the program. And if these programmers think to capture the output in a file, they see that it displays exactly the values they expect:</p>

<pre>
0.000000 
0.100000 
0.200000 
0.300000 
0.400000 
0.500000 
0.600000
0.700000 
0.800000 
0.900000 
1.000000 
1.100000 
...
</pre>

<p>Naturally, they want to know why the loop didn't stop when <B>index</B> reached 1.0. Most of us know the answer: the increment value, 0.1, cannot be represented exactly in a binary floating-point value, so each time through the loop the value of <B>index</B> increases by an amount that's close to but not equal to 0.1. On the tenth time through the loop, the value of <B>index</B> isn't exactly 1.0, so the test for equality fails. The way to "fix" this is to change the loop condition from <B>index != 1.0</B> to <B>index &lt; 1.0</B>. Of course, that's only half the answer, as indicated by another problem: <B>printf</B> seems to think that the value of <B>index</B> the tenth time through the loop is exactly 1.0.</p>

<H4><FONT COLOR="#000080">Floating-Point Values Are Often Rounded for Display</FONT></H4>

<p>We all know that when we divide two positive integers the result is truncated: the result of the division is the largest integer that's less than or equal to the result that we'd get if we used real numbers instead of integers. So nobody except a rank beginner is surprised that one divided by three is zero. Integers represent only the integral part of a value, and any fractional part is simply discarded.</p>
<p>The same sort of thing happens in floating-point math. The discarding of information is more subtle, so we often don't notice that it is happening. But consider the same division performed with floating-point math: one divided by three is 0.333333, with however many threes are supported by the floating-point package being used. We're used to thinking of this sort of number as shorthand for an infinitely repeating decimal, so it doesn't surprise us when we multiply that result by three and get back our original value of one. Try it on your calculator.</p>
<p>Despite what the calculator shows, the result of the multiplication is not one. The calculator is rounding the result to the number of digits being displayed, and when you round 0.999999 to a shorter representation, it comes out as 1.0. If that's what your calculator is showing you, try increasing the number of digits that it displays to the right of the decimal point. If I set my HP-48GX to display 10 digits to the right of the decimal point, it shows the result as 1.0000000000E0. If I increase it to 11 digits, it shows as 9.99999999999E-1. If your calculator won't display enough digits to show that the result really isn't one, try subtracting one from the result.</p>
<p>Once you get the rounding out of the way, it's clear that the result is actually 0.999999, with however many nines are supported by the floating point package in the calculator. And if we remember that the result of the original division is not an infinite sequence of threes, but a truncated sequence of threes, limited by the resolution of the calculator's floating-point math, then the result makes sense: (1.0/3.0)*3.0 is 0.999999, not 1.0. When you're looking at the results of a floating-point computation, don't let rounding in the output fool you. The actual answer may be somewhat different from what is being displayed <a href="#5">[5]</a>.</p>

<H4><FONT COLOR="#000080">Representing Floating-Point Values</FONT></H4>

<p>To understand how floating-point values are represented internally, it helps to think about what's commonly known as "scientific notation." Very large and very small values are represented by showing the digits of their values, and a power of 10 to multiply the digits by. For example, Avogadro's number is 6.022*1023. The convention here is to reduce the digits to a value that's greater than or equal to one and less than 10 and to adjust the exponent accordingly. We could, if we wanted to, use 60.22*1022 for this value, but that would raise eyebrows because it is not in the usual form.</p>
<p>In a floating-point math package the same basic convention applies. A number is represented as a set of digits and an exponent. The tricky part in understanding floating-point representations is that the base for the digits and the exponent varies from system to system. Calculators typically use base 10, so conversion problems, such as the internal representation of 0.1 that we looked at earlier, rarely come up. The internal representation of the value has the same form as the external one. On computers, however, most floating-point packages use some form of binary base, most often base 2. That is, the digits in the value must be converted to base 2, and the exponent must be converted from a power of 10 to the appropriate power of 2. Further, the usual convention for a floating-point math package is that the digits are <I>normalized</I> so that their value is greater than or equal to one-half and less than one <a href="#6">[6]</a>. So the value 1.0*101 translated to a binary floating-point value would be 0.101*24.</p>
<p>In order to be able to manipulate floating-point values efficiently, a floating-point package packs the parts of the value into just a few bytes of memory. Intel chips, for example, support 32-bit, 64-bit, and 80-bit floating-point values. These values are stored in 4, 8, and 10 bytes, respectively, which makes them easy for the hardware to manipulate. For example, assignment between 8-byte floating-point values requires no knowledge of how the parts of the value are laid out in the memory block. Copying the block is all that's needed. Arithmetic, on the other hand, requires pulling out the various parts of the value so that they can be manipulated separately. The details of the internal representation are implementation-specific &#151; the C and C++ standards do not impose requirements on how the parts of a floating-point value should be laid out. You can get at those parts through the function <B>frexp</B>, but what you get from <B>frexp</B> does not necessarily have exactly the same bit patterns as the internal representation of the parts.</p>
<p>For the 32-bit representation that Intel uses, the high bit contains the sign of the value. A zero means that the value is positive, and a one means that the value is negative. The low 23 bits contain the fractional part, that is, what I've been calling the "digits" in the previous paragraphs. There's a quirk here, though, that can fool you. Remember that the digits are normalized so that their value is always between one-half and one. That is, the high bit in the digits will always be one. In Intel's internal representation, this bit is not stored. The floating-point package puts it back in when it's needed in a computation. So the 23-bit fractional part actually gives 24 bits of precision. One more thing to keep in mind: the bits in the fractional part are the same for a positive value and its negative counterpart. Only the sign bit changes <a href="#7">[7]</a>.</p>
<p>Finally, the exponent is represented in the remaining eight bits in biased form: 127 is added to the actual value. So an exponent of 0 is represented in the 32-bit format as the value 127, an exponent of 1 is 128, and an exponent of -1 is 126. To convert an internal value back to an actual exponent, subtract 127 from the internal value. Since the largest value that can be stored in an 8-bit field is 255, the largest possible exponent in this 32-bit representation is 128. The smallest possible exponent is -127. These exponents will only rarely show up in normal computations, however. An exponent of 128 is used to represent infinite values (both positive and negative), and invalid values, known as NaNs, which stands for Not a Number. An exponent of -127 with a fraction of 0 represents the value 0. All other values with an exponent of -127 are <I>denormalized</I> values. We'll talk about infinities, NaNs, and denormalized values some other time. For now, the possible values of the exponent range from -126 to 127, and an exponent of -127 for the values 0.0 and -0.0.</p>

<H4><FONT COLOR="#000080">A Class to Represent Floating-Point Values</FONT></H4>

<p><a href="list1.htm">Listing 1</a> contains the class definition for the class <B>fp</B>, which provides basic floating-point operations. It separates the three components of a floating-point value in order to make it easier to see how these parts are manipulated. The <B>bool</B> member <B>is_neg</B> indicates whether the value is negative. The <B>unsigned char</B> member <B>exp</B> holds the exponent, stored in biased form with a bias value of 127. The <B>int</B> member <B>frac</B> holds the fractional part containing 15 bits. It does not drop the high-order bit, in order to simplify the code a bit.</p>
<p>I've chosen these particular types to produce a floating-point implementation that resembles Intel's, but can be written in portable C++. For example, the language definition guarantees that an <B>unsigned char</B> can hold values from 0 to 255, so the values of exponents for the class <B>fp</B> are limited to this range, even though some C++ implementations might support a larger range of values in an unsigned char. Similarly, the fraction is limited to 15 bits in order to guarantee that the result of multiplying two fractions will fit in a <B>long</B>. The goal here is not efficient use of space, but clarity. Please don't write to me about ways of packing more bits into this class. That's not its purpose.</p>

<H4><FONT COLOR="#000080">Multiplying Two Floating-Point Values</FONT></H4>

<p>Multiplying two floating-point values is fairly easy, although there are a couple of edge conditions that we have to be careful of. To see how multiplication is done, let's go back to scientific notation and look at how to compute the square of Avogadro's number:</p>

<pre>
 6.022*10<SUP>23</SUP>
 6.022*10<SUP>23</SUP>
--------------
36.264484*10<SUP>46</SUP>
</pre>

<p>To get this raw result, we multiplied the two digit parts and added the two exponents. To put it back into the usual form, though, we have to do two things: we have to adjust the digit part so that its value is less than 10, and we have to get rid of the extra digits that we got on the end of the value. We only had four significant figures in our original value, and we should have four significant figures in the result. We begin by normalizing the result: we replace 36.264484*1046 with 3.6264484*1047. Then we round the resulting value to four significant digits, producing 3.626*1047. We usually do these latter two steps as part of the multiplication itself, so instead of writing out the multiplication as we did above we'd write it like this:</p>

<pre>
6.022*10<SUP>23</SUP>
6.022*10<SUP>23</SUP>
----------
3.626*10<SUP>47</SUP>
</pre>

<p>If you look at the implementation of <B>operator *=</B> in <a href="list2.htm">Listing 2</a>, you can see how this technique is applied in the class <B>fp</B>. The code begins by checking for zeros. If either value in the multiplication is zero, the result is set to zero, with the appropriate sign. Otherwise the code computes the exponent of the result by adding the exponents of the two factors and subtracting the bias; it computes the fractional part of the result by multiplying the fractional parts of the two factors. When the exponent is being computed, the bias must be subtracted to compensate for the fact that both exponents are biased. This could also be done by subtracting the bias from each exponent, adding the results, and adding the bias back in, but subtracting the bias is simpler. To compute the fractional part of the result, the code uses the member function <B>lf</B>, which combines the fraction with the sign and converts the result to a <B>long</B>. Multiplying the two resulting signed fractions produces a result that fits in a <B>long</B>. The results of this multiplication and the computation of the new exponent are passed to the function <B>normalize</B>, which takes the raw result and converts it into a properly normalized form, just as we did above in scientific notation.</p>
<p>The function <B>normalize</B> is where we deal with various edge conditions. The first two <B>if</B> statements recognize zero values and overflowed fractions. Those don't occur in multiplication or division. We'll look at them when we talk about addition. The <B>while</B> loop that follows the <B>if</B> statements normalizes fraction values whose high bit is zero. Again, think about scientific notation: .1 times .1 is .01, which has a leading zero digit. The same thing occurs in binary multiplication. We require that the high bit of the fraction be 1, so we multiply the fraction by 2 and reduce the exponent by 1 as many times as needed. The next <B>if</B> statement checks whether the highest bit in the part of the fractional value that we're going to discard is a 1. If so, the value is rounded up or down, according to its sign.</p>
<p>Rounding can turn a normalized value into a value that is not normalized. Looking again at scientific notation, consider 9.9999*100. If we round it to two figures to the right of the decimal point, the result is 10.00*100. To make the digits part of this value less than 10, we need to move the decimal point to the left by one position and increase the exponent by 1, producing the value 1.00*101. The floating-point code does the same thing: if bit 31 is 1, the fractional part is too large to represent in our internal notation, so the code divides it by 2 and increases the exponent by 1. Now that the code has done all the adjustments needed to get the result into normalized form, it checks whether the resulting exponent is too large or too small. If so, it calls the member function <B>exception</B>, which handles the problem by displaying an error message and exiting from the program. In a later column, we'll look at less drastic (but not necessarily better) ways of handling these problems.</p>
<p>Finally, after all these adjustments and checks, the code is ready to store the new exponent and fractional part into the object. There's one last adjustment needed to the fractional part, though. We started out with two 15-bit fractions and multiplied them to produce a 30-bit value. All of the normalization code has dealt with this 30-bit value. Before we can store this value into an <B>fp</B> object, we have to convert it back into a 15-bit fraction. That's what the right shift in the next-to-last line of <B>normalize</B> does.</p>

<H4><FONT COLOR="#000080">Dividing Floating-Point Values</FONT></H4>

<p>With the code for normalizing results in place, division is quite easy. It's done with <B>operator /=</B> shown in <a href="list2.htm">Listing 2</a>. Just like multiplication, the code begins by checking for zeros. A zero numerator produces a result of zero <a href="#8">[8]</a>. If the numerator is not zero but the denominator is, the code calls <B>exception</B> to report division by zero. If neither of the values is zero, the code computes the new exponent and the new fractional part and passes them to <B>normalize</B> for all the necessary adjustments. The new exponent is the difference between the two exponents, again adjusted for the bias. The computation of the fractional part looks tricky, but it isn't really. It shifts the numerator left by 15 bits to produce a 30-bit value, then divides that value by the denominator producing a 15-bit quotient, and then shifts the quotient left by 15 bits to produce a 30-bit value to pass to <B>normalize</B>.</p>

<H4><FONT COLOR="#000080">Adding Two Floating-Point Values</FONT></H4>

<p>To add two floating-point values, we have to adjust the one that's closer to zero so that they both have the same exponent. This means shifting the fractional part of the smaller value to the right and increasing its exponent appropriately. We do the same thing when we use scientific notation. To compute 1.0*101 + 1.0*100 (that is, 10 + 1), we adjust 1.0*100 by dividing its digits by 10 and increasing its exponent by 1 to produce 0.1*101. This is not in the usual form, but we're willing to tolerate that discrepancy during the addition, so long as the result we get is properly adjusted. Now we finish the addition by adding 1.0*101 and 0.1*101, producing the result 1.1*101.</p>
<p>The code in <a href="list2.htm">Listing 2</a> goes through two layers of functions to perform this additional adjustment. The code in <B>operator +=</B> compares the exponents of the two values being added to decide which should be adjusted. It calls the function <B>add</B> with the value that has the larger exponent as the first argument and the value that has the smaller exponent as the second argument.</p>
<p>The function <B>add</B> begins by converting the fraction from the first argument into a 30-bit value. It then looks at the difference between the exponents of the two arguments. If the second argument's exponent is so small that the second argument won't affect the result, it ignores the second argument. Otherwise it gets the fraction from the second argument, shifts it left to produce a 30-bit value, shifts that result right by the difference between the two exponents, and adds the resulting value to the 30-bit value that it got from the first argument. The resulting value is passed to <B>normalize</B>, along with the exponent for the result.</p>
<p>Since the sum of two non-zero numbers can be zero, <B>normalize</B> checks whether the fraction that was passed to it is zero. If so, it stores a zero fraction and a zero exponent in the <B>fp</B> object in order to properly represent a value of zero. Otherwise, it checks whether bit 31 in the fraction is on. If so, it divides the fraction by 2 and increases the exponent by 1 to remove the overflow. Overflow occurs when two values that are fairly close together are added. In scientific notation, consider 9.0*101 + 8.0*101. The result is 17.0*101, which must be adjusted to 1.7*102. The same thing can occur with the fraction part of a floating-point value: adding two 15-bit values can produce a 31-bit value, which must be adjusted back to 30 bits in order for subsequent code to work correctly.</p>
<p>After these two adjustments, <B>normalize</B> goes through all of the steps that we looked at earlier. Some of those steps aren't actually needed for addition, but checking for them doesn't do any harm <a href="#9">[9]</a>.</p>

<H4><FONT COLOR="#000080">Subtracting Floating-Point Values</FONT></H4>

<p>With the code for adding two floating-point values in place, subtraction is easy. The code in <B>operator -=</B> uses the identity <a href="#10">[10]</a> <B>a - b == a + (-b)</B>; it negates the second argument and adds the result to the first argument.</p>

<H4><FONT COLOR="#000080">Experimenting</FONT></H4>

<p>If you want to understand how floating-point math works, you need to experiment with it. The exploration that we've gone through in this column provides a sound basis for analyzing why floating-point math produces the results that it does, but until you've seen the details in actual computations, it's hard to have a sound grasp of how it works. You can download the code for the class <B>fp</B> from the <I>CUJ</I> website (<B>www.cuj.com/code</B>). The archive also includes a four-function command-line calculator, which you can use to explore floating-point computations in more detail than we've gone through here. Here are a few things that you might want to look into:</p>

<UL><LI>   What is the smallest positive value that can be added to 1.0 to produce a value that is different from 1.0?</LI>
<LI>   When you evaluate the expression ((1-x2)/(1-x)) for values of x that are very close to 1, why do the results differ from the value of the expression (1+x) for the same values of x?</LI>
<LI>   We looked at the expression (1.0/3.0)*3.0 and concluded that its value should be slightly less than 1.0. If you use the class <B>fp</B> to evaluate this expression, you get exactly 1.0. Why?</LI></UL>

<H4><FONT COLOR="#000080">Notes and References</FONT></H4>
<p><a name="1"></a>[1]  Donald E. Knuth "Seminumerical Algorithms," <I>The Art of Computer Programming, Volumn 2,</I> Third Edition (Addison-Wesley, 1998). ISBN 0-201-89683-4. p. 229.</p>
<p><a name="2"></a>[2]  No, it's not the sound of the loud swallow that you make when you suddenly realize that you have no idea what's going on. It stands for "unit in the last place," and it's used to describe the accuracy of a floating-point result.</p>
<p><a name="3"></a>[3]  I'm also sure that their eyes would glaze over if they had to listen to an explanation of some of the things that I think are fascinating.</p>
<p><a name="4"></a>[4]  Knuth. <I>The Art of Computer Programming,</I> Third Edition. p. 214.</p>
<p><a name="5"></a>[5]  Use the C function <B>frexp</B> to see what's really going on.</p>
<p><a name="6"></a>[6]  More generally, the fractional part must have a value that is less than 1 and greater than or equal to 1/b, where b is the base of the floating-point package. For a binary package, 1/b becomes 1/2. For a decimal package, it would be 1/10.</p>
<p><a name="7"></a>[7]  Yes, this means that 0.0 and -0.0 have different representations. We'll look at the implications of negative zeros in a later column.</p>
<p><a name="8"></a>[8]  This code ought to check for a zero denominator before deciding that the result is actually zero. We'll look at how to handle 0.0/0.0 in a later column, when we talk about NaN values.</p>
<p><a name="9"></a>[9]  Except, of course, for speed.</p>
<p><a name="10"></a>[10]  Be careful about assuming that common identities apply to floating-point math. For example, if <B>a+b==a+c</B>, it does not follow that <B>b==c</B>. The identity that we're using for subtraction does work for floating-point values, however.</p>

<p><i><B>Pete Becker</B> is Technical Project Leader for Dinkumware, Ltd. He spent eight years in the C++ group at Borland International, both as a developer and manager. He is a member of the ANSI/ISO C++ standardization committee. He can be reached by email at <B>petebecker@acm.org</B>.</i></p>

<h4><a href="../../../source/2000/jun00/pbecker.zip">Get Article Source Code</a></h4>

</blockquote></body></html>
